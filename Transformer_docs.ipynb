{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the official transformers library\n",
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformers library from Hugging Face is a open source AI library containing various attributes that have different functionalities.The library is designed with modularity and flexibility as its primary focus.\n",
    "\n",
    "It contains functions, utilies and subpackages.\n",
    "\n",
    "‣Main Package: transformers\n",
    "\n",
    "This is the root of the library.\n",
    "\n",
    "▪Major Sub-packages and Submodules:\n",
    "\n",
    "* For e.g.\n",
    "models: Contains model-specific implementations.\n",
    "\n",
    "tokenizers: Houses various tokenizer classes.\n",
    "\n",
    "pipelines: Includes ready-to-use NLP pipelines.\n",
    "\n",
    "activations: Contains activation functions. \n",
    "\n",
    "configuration_utils: Utilities for model configurations.\n",
    "\n",
    "modeling_utils: General utilities for modeling.\n",
    "\n",
    "trainer: Training utilities and classes.\n",
    "\n",
    "feature_extraction_utils: Utilities for feature extraction.\n",
    "\n",
    "-----\n",
    "\n",
    "•Under models:\n",
    "\n",
    "* For e.g.\n",
    "bert, gpt2, t5, roberta, etc.: \n",
    "\n",
    "Each is a submodule for a specific model architecture.\n",
    "\n",
    "•Under each model (e.g. bert):\n",
    "\n",
    "There are models for specific tasks or the base models themselves.\n",
    "\n",
    "BertModel: The base BERT model.\n",
    "BertForSequenceClassification: BERT model for sequence classification tasks.\n",
    "\n",
    "•Under tokenizers:\n",
    "\n",
    "Tokenizer classes for different models.\n",
    "\n",
    "* For e.g.\n",
    "BertTokenizer, GPT2Tokenizer, T5Tokenizer, etc.: \n",
    "\n",
    "•Under pipelines:\n",
    "\n",
    "Tokenizer classes for different models.\n",
    "\n",
    "* For e.g.\n",
    "pipeline: \n",
    "text-classification, question-answering, etc.: Specific pipeline tasks.\n",
    "\n",
    "•Under activations:\n",
    "\n",
    "Various activation functions.\n",
    "\n",
    "* For e.g.\n",
    "gelu, swish, mish, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASTConfig\n",
      "ASTFeatureExtractor\n",
      "ASTForAudioClassification\n",
      "ASTModel\n",
      "ASTPreTrainedModel\n",
      "Adafactor\n",
      "AdamW\n",
      "AdamWeightDecay\n",
      "AdaptiveEmbedding\n",
      "AddedToken\n",
      "Agent\n",
      "AlbertConfig\n",
      "AlbertForMaskedLM\n",
      "AlbertForMultipleChoice\n",
      "AlbertForPreTraining\n",
      "AlbertForQuestionAnswering\n",
      "AlbertForSequenceClassification\n",
      "AlbertForTokenClassification\n",
      "AlbertModel\n",
      "AlbertPreTrainedModel\n",
      "AlbertTokenizer\n",
      "AlbertTokenizerFast\n",
      "AlignConfig\n",
      "AlignModel\n",
      "AlignPreTrainedModel\n",
      "AlignProcessor\n",
      "AlignTextConfig\n",
      "AlignTextModel\n",
      "AlignVisionConfig\n",
      "AlignVisionModel\n",
      "AltCLIPConfig\n",
      "AltCLIPModel\n",
      "AltCLIPPreTrainedModel\n",
      "AltCLIPProcessor\n",
      "AltCLIPTextConfig\n",
      "AltCLIPTextModel\n",
      "AltCLIPVisionConfig\n",
      "AltCLIPVisionModel\n",
      "AlternatingCodebooksLogitsProcessor\n",
      "AqlmConfig\n",
      "AudioClassificationPipeline\n",
      "AutoBackbone\n",
      "AutoConfig\n",
      "AutoFeatureExtractor\n",
      "AutoImageProcessor\n",
      "AutoModel\n",
      "AutoModelForAudioClassification\n",
      "AutoModelForAudioFrameClassification\n",
      "AutoModelForAudioXVector\n",
      "AutoModelForCTC\n",
      "AutoModelForCausalLM\n",
      "AutoModelForDepthEstimation\n",
      "AutoModelForDocumentQuestionAnswering\n",
      "AutoModelForImageClassification\n",
      "AutoModelForImageSegmentation\n",
      "AutoModelForImageToImage\n",
      "AutoModelForInstanceSegmentation\n",
      "AutoModelForKeypointDetection\n",
      "AutoModelForMaskGeneration\n",
      "AutoModelForMaskedImageModeling\n",
      "AutoModelForMaskedLM\n",
      "AutoModelForMultipleChoice\n",
      "AutoModelForNextSentencePrediction\n",
      "AutoModelForObjectDetection\n",
      "AutoModelForPreTraining\n",
      "AutoModelForQuestionAnswering\n",
      "AutoModelForSemanticSegmentation\n",
      "AutoModelForSeq2SeqLM\n",
      "AutoModelForSequenceClassification\n",
      "AutoModelForSpeechSeq2Seq\n",
      "AutoModelForTableQuestionAnswering\n",
      "AutoModelForTextEncoding\n",
      "AutoModelForTextToSpectrogram\n",
      "AutoModelForTextToWaveform\n",
      "AutoModelForTokenClassification\n",
      "AutoModelForUniversalSegmentation\n",
      "AutoModelForVideoClassification\n",
      "AutoModelForVision2Seq\n",
      "AutoModelForVisualQuestionAnswering\n",
      "AutoModelForZeroShotImageClassification\n",
      "AutoModelForZeroShotObjectDetection\n",
      "AutoModelWithLMHead\n",
      "AutoProcessor\n",
      "AutoTokenizer\n",
      "AutoformerConfig\n",
      "AutoformerForPrediction\n",
      "AutoformerModel\n",
      "AutoformerPreTrainedModel\n",
      "AutomaticSpeechRecognitionPipeline\n",
      "AwqConfig\n",
      "BarkCausalModel\n",
      "BarkCoarseConfig\n",
      "BarkCoarseModel\n",
      "BarkConfig\n",
      "BarkFineConfig\n",
      "BarkFineModel\n",
      "BarkModel\n",
      "BarkPreTrainedModel\n",
      "BarkProcessor\n",
      "BarkSemanticConfig\n",
      "BarkSemanticModel\n",
      "BartConfig\n",
      "BartForCausalLM\n",
      "BartForConditionalGeneration\n",
      "BartForQuestionAnswering\n",
      "BartForSequenceClassification\n",
      "BartModel\n",
      "BartPreTrainedModel\n",
      "BartPretrainedModel\n",
      "BartTokenizer\n",
      "BartTokenizerFast\n",
      "BarthezTokenizer\n",
      "BarthezTokenizerFast\n",
      "BartphoTokenizer\n",
      "BaseImageProcessor\n",
      "BaseImageProcessorFast\n",
      "BasicTokenizer\n",
      "BatchEncoding\n",
      "BatchFeature\n",
      "BeamScorer\n",
      "BeamSearchScorer\n",
      "BeitBackbone\n",
      "BeitConfig\n",
      "BeitFeatureExtractor\n",
      "BeitForImageClassification\n",
      "BeitForMaskedImageModeling\n",
      "BeitForSemanticSegmentation\n",
      "BeitImageProcessor\n",
      "BeitModel\n",
      "BeitPreTrainedModel\n",
      "BertConfig\n",
      "BertForMaskedLM\n",
      "BertForMultipleChoice\n",
      "BertForNextSentencePrediction\n",
      "BertForPreTraining\n",
      "BertForQuestionAnswering\n",
      "BertForSequenceClassification\n",
      "BertForTokenClassification\n",
      "BertGenerationConfig\n",
      "BertGenerationDecoder\n",
      "BertGenerationEncoder\n",
      "BertGenerationPreTrainedModel\n",
      "BertGenerationTokenizer\n",
      "BertJapaneseTokenizer\n",
      "BertLMHeadModel\n",
      "BertLayer\n",
      "BertModel\n",
      "BertPreTrainedModel\n",
      "BertTokenizer\n",
      "BertTokenizerFast\n",
      "BertweetTokenizer\n",
      "BigBirdConfig\n",
      "BigBirdForCausalLM\n",
      "BigBirdForMaskedLM\n",
      "BigBirdForMultipleChoice\n",
      "BigBirdForPreTraining\n",
      "BigBirdForQuestionAnswering\n",
      "BigBirdForSequenceClassification\n",
      "BigBirdForTokenClassification\n",
      "BigBirdLayer\n",
      "BigBirdModel\n",
      "BigBirdPegasusConfig\n",
      "BigBirdPegasusForCausalLM\n",
      "BigBirdPegasusForConditionalGeneration\n",
      "BigBirdPegasusForQuestionAnswering\n",
      "BigBirdPegasusForSequenceClassification\n",
      "BigBirdPegasusModel\n",
      "BigBirdPegasusPreTrainedModel\n",
      "BigBirdPreTrainedModel\n",
      "BigBirdTokenizer\n",
      "BigBirdTokenizerFast\n",
      "BioGptConfig\n",
      "BioGptForCausalLM\n",
      "BioGptForSequenceClassification\n",
      "BioGptForTokenClassification\n",
      "BioGptModel\n",
      "BioGptPreTrainedModel\n",
      "BioGptTokenizer\n",
      "BitBackbone\n",
      "BitConfig\n",
      "BitForImageClassification\n",
      "BitImageProcessor\n",
      "BitModel\n",
      "BitPreTrainedModel\n",
      "BitsAndBytesConfig\n",
      "BlenderbotConfig\n",
      "BlenderbotForCausalLM\n",
      "BlenderbotForConditionalGeneration\n",
      "BlenderbotModel\n",
      "BlenderbotPreTrainedModel\n",
      "BlenderbotSmallConfig\n",
      "BlenderbotSmallForCausalLM\n",
      "BlenderbotSmallForConditionalGeneration\n",
      "BlenderbotSmallModel\n",
      "BlenderbotSmallPreTrainedModel\n",
      "BlenderbotSmallTokenizer\n",
      "BlenderbotSmallTokenizerFast\n",
      "BlenderbotTokenizer\n",
      "BlenderbotTokenizerFast\n",
      "Blip2Config\n",
      "Blip2ForConditionalGeneration\n",
      "Blip2Model\n",
      "Blip2PreTrainedModel\n",
      "Blip2Processor\n",
      "Blip2QFormerConfig\n",
      "Blip2QFormerModel\n",
      "Blip2VisionConfig\n",
      "Blip2VisionModel\n",
      "BlipConfig\n",
      "BlipForConditionalGeneration\n",
      "BlipForImageTextRetrieval\n",
      "BlipForQuestionAnswering\n",
      "BlipImageProcessor\n",
      "BlipModel\n",
      "BlipPreTrainedModel\n",
      "BlipProcessor\n",
      "BlipTextConfig\n",
      "BlipTextModel\n",
      "BlipVisionConfig\n",
      "BlipVisionModel\n",
      "BloomConfig\n",
      "BloomForCausalLM\n",
      "BloomForQuestionAnswering\n",
      "BloomForSequenceClassification\n",
      "BloomForTokenClassification\n",
      "BloomModel\n",
      "BloomPreTrainedModel\n",
      "BloomTokenizerFast\n",
      "BridgeTowerConfig\n",
      "BridgeTowerForContrastiveLearning\n",
      "BridgeTowerForImageAndTextRetrieval\n",
      "BridgeTowerForMaskedLM\n",
      "BridgeTowerImageProcessor\n",
      "BridgeTowerModel\n",
      "BridgeTowerPreTrainedModel\n",
      "BridgeTowerProcessor\n",
      "BridgeTowerTextConfig\n",
      "BridgeTowerVisionConfig\n",
      "BrosConfig\n",
      "BrosForTokenClassification\n",
      "BrosModel\n",
      "BrosPreTrainedModel\n",
      "BrosProcessor\n",
      "BrosSpadeEEForTokenClassification\n",
      "BrosSpadeELForTokenClassification\n",
      "ByT5Tokenizer\n",
      "CLIPConfig\n",
      "CLIPFeatureExtractor\n",
      "CLIPForImageClassification\n",
      "CLIPImageProcessor\n",
      "CLIPModel\n",
      "CLIPPreTrainedModel\n",
      "CLIPProcessor\n",
      "CLIPSegConfig\n",
      "CLIPSegForImageSegmentation\n",
      "CLIPSegModel\n",
      "CLIPSegPreTrainedModel\n",
      "CLIPSegProcessor\n",
      "CLIPSegTextConfig\n",
      "CLIPSegTextModel\n",
      "CLIPSegVisionConfig\n",
      "CLIPSegVisionModel\n",
      "CLIPTextConfig\n",
      "CLIPTextModel\n",
      "CLIPTextModelWithProjection\n",
      "CLIPTokenizer\n",
      "CLIPTokenizerFast\n",
      "CLIPVisionConfig\n",
      "CLIPVisionModel\n",
      "CLIPVisionModelWithProjection\n",
      "CONFIG_MAPPING\n",
      "CONFIG_NAME\n",
      "CTRLConfig\n",
      "CTRLForSequenceClassification\n",
      "CTRLLMHeadModel\n",
      "CTRLModel\n",
      "CTRLPreTrainedModel\n",
      "CTRLTokenizer\n",
      "Cache\n",
      "CacheConfig\n",
      "CamembertConfig\n",
      "CamembertForCausalLM\n",
      "CamembertForMaskedLM\n",
      "CamembertForMultipleChoice\n",
      "CamembertForQuestionAnswering\n",
      "CamembertForSequenceClassification\n",
      "CamembertForTokenClassification\n",
      "CamembertModel\n",
      "CamembertPreTrainedModel\n",
      "CamembertTokenizer\n",
      "CamembertTokenizerFast\n",
      "CanineConfig\n",
      "CanineForMultipleChoice\n",
      "CanineForQuestionAnswering\n",
      "CanineForSequenceClassification\n",
      "CanineForTokenClassification\n",
      "CanineLayer\n",
      "CanineModel\n",
      "CaninePreTrainedModel\n",
      "CanineTokenizer\n",
      "CharSpan\n",
      "CharacterTokenizer\n",
      "ChineseCLIPConfig\n",
      "ChineseCLIPFeatureExtractor\n",
      "ChineseCLIPImageProcessor\n",
      "ChineseCLIPModel\n",
      "ChineseCLIPPreTrainedModel\n",
      "ChineseCLIPProcessor\n",
      "ChineseCLIPTextConfig\n",
      "ChineseCLIPTextModel\n",
      "ChineseCLIPVisionConfig\n",
      "ChineseCLIPVisionModel\n",
      "ClapAudioConfig\n",
      "ClapAudioModel\n",
      "ClapAudioModelWithProjection\n",
      "ClapConfig\n",
      "ClapFeatureExtractor\n",
      "ClapModel\n",
      "ClapPreTrainedModel\n",
      "ClapProcessor\n",
      "ClapTextConfig\n",
      "ClapTextModel\n",
      "ClapTextModelWithProjection\n",
      "ClassifierFreeGuidanceLogitsProcessor\n",
      "ClvpConfig\n",
      "ClvpDecoder\n",
      "ClvpDecoderConfig\n",
      "ClvpEncoder\n",
      "ClvpEncoderConfig\n",
      "ClvpFeatureExtractor\n",
      "ClvpForCausalLM\n",
      "ClvpModel\n",
      "ClvpModelForConditionalGeneration\n",
      "ClvpPreTrainedModel\n",
      "ClvpProcessor\n",
      "ClvpTokenizer\n",
      "CodeAgent\n",
      "CodeGenConfig\n",
      "CodeGenForCausalLM\n",
      "CodeGenModel\n",
      "CodeGenPreTrainedModel\n",
      "CodeGenTokenizer\n",
      "CodeGenTokenizerFast\n",
      "CodeLlamaTokenizer\n",
      "CodeLlamaTokenizerFast\n",
      "CohereConfig\n",
      "CohereForCausalLM\n",
      "CohereModel\n",
      "CoherePreTrainedModel\n",
      "CohereTokenizerFast\n",
      "ConditionalDetrConfig\n",
      "ConditionalDetrFeatureExtractor\n",
      "ConditionalDetrForObjectDetection\n",
      "ConditionalDetrForSegmentation\n",
      "ConditionalDetrImageProcessor\n",
      "ConditionalDetrModel\n",
      "ConditionalDetrPreTrainedModel\n",
      "ConstrainedBeamSearchScorer\n",
      "Constraint\n",
      "ConstraintListState\n",
      "Conv1D\n",
      "ConvBertConfig\n",
      "ConvBertForMaskedLM\n",
      "ConvBertForMultipleChoice\n",
      "ConvBertForQuestionAnswering\n",
      "ConvBertForSequenceClassification\n",
      "ConvBertForTokenClassification\n",
      "ConvBertLayer\n",
      "ConvBertModel\n",
      "ConvBertPreTrainedModel\n",
      "ConvBertTokenizer\n",
      "ConvBertTokenizerFast\n",
      "ConvNextBackbone\n",
      "ConvNextConfig\n",
      "ConvNextFeatureExtractor\n",
      "ConvNextForImageClassification\n",
      "ConvNextImageProcessor\n",
      "ConvNextModel\n",
      "ConvNextPreTrainedModel\n",
      "ConvNextV2Backbone\n",
      "ConvNextV2Config\n",
      "ConvNextV2ForImageClassification\n",
      "ConvNextV2Model\n",
      "ConvNextV2PreTrainedModel\n",
      "CpmAntConfig\n",
      "CpmAntForCausalLM\n",
      "CpmAntModel\n",
      "CpmAntPreTrainedModel\n",
      "CpmAntTokenizer\n",
      "CpmTokenizer\n",
      "CpmTokenizerFast\n",
      "CsvPipelineDataFormat\n",
      "CvtConfig\n",
      "CvtForImageClassification\n",
      "CvtModel\n",
      "CvtPreTrainedModel\n",
      "DPRConfig\n",
      "DPRContextEncoder\n",
      "DPRContextEncoderTokenizer\n",
      "DPRContextEncoderTokenizerFast\n",
      "DPRPreTrainedModel\n",
      "DPRPretrainedContextEncoder\n",
      "DPRPretrainedQuestionEncoder\n",
      "DPRPretrainedReader\n",
      "DPRQuestionEncoder\n",
      "DPRQuestionEncoderTokenizer\n",
      "DPRQuestionEncoderTokenizerFast\n",
      "DPRReader\n",
      "DPRReaderOutput\n",
      "DPRReaderTokenizer\n",
      "DPRReaderTokenizerFast\n",
      "DPTConfig\n",
      "DPTFeatureExtractor\n",
      "DPTForDepthEstimation\n",
      "DPTForSemanticSegmentation\n",
      "DPTImageProcessor\n",
      "DPTModel\n",
      "DPTPreTrainedModel\n",
      "Data2VecAudioConfig\n",
      "Data2VecAudioForAudioFrameClassification\n",
      "Data2VecAudioForCTC\n",
      "Data2VecAudioForSequenceClassification\n",
      "Data2VecAudioForXVector\n",
      "Data2VecAudioModel\n",
      "Data2VecAudioPreTrainedModel\n",
      "Data2VecTextConfig\n",
      "Data2VecTextForCausalLM\n",
      "Data2VecTextForMaskedLM\n",
      "Data2VecTextForMultipleChoice\n",
      "Data2VecTextForQuestionAnswering\n",
      "Data2VecTextForSequenceClassification\n",
      "Data2VecTextForTokenClassification\n",
      "Data2VecTextModel\n",
      "Data2VecTextPreTrainedModel\n",
      "Data2VecVisionConfig\n",
      "Data2VecVisionForImageClassification\n",
      "Data2VecVisionForSemanticSegmentation\n",
      "Data2VecVisionModel\n",
      "Data2VecVisionPreTrainedModel\n",
      "DataCollator\n",
      "DataCollatorForLanguageModeling\n",
      "DataCollatorForPermutationLanguageModeling\n",
      "DataCollatorForSOP\n",
      "DataCollatorForSeq2Seq\n",
      "DataCollatorForTokenClassification\n",
      "DataCollatorForWholeWordMask\n",
      "DataCollatorWithPadding\n",
      "DataProcessor\n",
      "DbrxConfig\n",
      "DbrxForCausalLM\n",
      "DbrxModel\n",
      "DbrxPreTrainedModel\n",
      "DebertaConfig\n",
      "DebertaForMaskedLM\n",
      "DebertaForQuestionAnswering\n",
      "DebertaForSequenceClassification\n",
      "DebertaForTokenClassification\n",
      "DebertaModel\n",
      "DebertaPreTrainedModel\n",
      "DebertaTokenizer\n",
      "DebertaTokenizerFast\n",
      "DebertaV2Config\n",
      "DebertaV2ForMaskedLM\n",
      "DebertaV2ForMultipleChoice\n",
      "DebertaV2ForQuestionAnswering\n",
      "DebertaV2ForSequenceClassification\n",
      "DebertaV2ForTokenClassification\n",
      "DebertaV2Model\n",
      "DebertaV2PreTrainedModel\n",
      "DebertaV2Tokenizer\n",
      "DebertaV2TokenizerFast\n",
      "DecisionTransformerConfig\n",
      "DecisionTransformerGPT2Model\n",
      "DecisionTransformerGPT2PreTrainedModel\n",
      "DecisionTransformerModel\n",
      "DecisionTransformerPreTrainedModel\n",
      "DefaultDataCollator\n",
      "DefaultFlowCallback\n",
      "DeformableDetrConfig\n",
      "DeformableDetrFeatureExtractor\n",
      "DeformableDetrForObjectDetection\n",
      "DeformableDetrImageProcessor\n",
      "DeformableDetrModel\n",
      "DeformableDetrPreTrainedModel\n",
      "DeiTConfig\n",
      "DeiTFeatureExtractor\n",
      "DeiTForImageClassification\n",
      "DeiTForImageClassificationWithTeacher\n",
      "DeiTForMaskedImageModeling\n",
      "DeiTImageProcessor\n",
      "DeiTModel\n",
      "DeiTPreTrainedModel\n",
      "DepthAnythingConfig\n",
      "DepthAnythingForDepthEstimation\n",
      "DepthAnythingPreTrainedModel\n",
      "DepthEstimationPipeline\n",
      "DetaConfig\n",
      "DetaForObjectDetection\n",
      "DetaImageProcessor\n",
      "DetaModel\n",
      "DetaPreTrainedModel\n",
      "DetrConfig\n",
      "DetrFeatureExtractor\n",
      "DetrForObjectDetection\n",
      "DetrForSegmentation\n",
      "DetrImageProcessor\n",
      "DetrModel\n",
      "DetrPreTrainedModel\n",
      "DinatBackbone\n",
      "DinatConfig\n",
      "DinatForImageClassification\n",
      "DinatModel\n",
      "DinatPreTrainedModel\n",
      "Dinov2Backbone\n",
      "Dinov2Config\n",
      "Dinov2ForImageClassification\n",
      "Dinov2Model\n",
      "Dinov2PreTrainedModel\n",
      "DisjunctiveConstraint\n",
      "DistilBertConfig\n",
      "DistilBertForMaskedLM\n",
      "DistilBertForMultipleChoice\n",
      "DistilBertForQuestionAnswering\n",
      "DistilBertForSequenceClassification\n",
      "DistilBertForTokenClassification\n",
      "DistilBertModel\n",
      "DistilBertPreTrainedModel\n",
      "DistilBertTokenizer\n",
      "DistilBertTokenizerFast\n",
      "DocumentQuestionAnsweringPipeline\n",
      "DonutFeatureExtractor\n",
      "DonutImageProcessor\n",
      "DonutProcessor\n",
      "DonutSwinConfig\n",
      "DonutSwinModel\n",
      "DonutSwinPreTrainedModel\n",
      "DummyObject\n",
      "DynamicCache\n",
      "EarlyStoppingCallback\n",
      "EetqConfig\n",
      "EfficientFormerConfig\n",
      "EfficientFormerForImageClassification\n",
      "EfficientFormerForImageClassificationWithTeacher\n",
      "EfficientFormerImageProcessor\n",
      "EfficientFormerModel\n",
      "EfficientFormerPreTrainedModel\n",
      "EfficientNetConfig\n",
      "EfficientNetForImageClassification\n",
      "EfficientNetImageProcessor\n",
      "EfficientNetModel\n",
      "EfficientNetPreTrainedModel\n",
      "ElectraConfig\n",
      "ElectraForCausalLM\n",
      "ElectraForMaskedLM\n",
      "ElectraForMultipleChoice\n",
      "ElectraForPreTraining\n",
      "ElectraForQuestionAnswering\n",
      "ElectraForSequenceClassification\n",
      "ElectraForTokenClassification\n",
      "ElectraModel\n",
      "ElectraPreTrainedModel\n",
      "ElectraTokenizer\n",
      "ElectraTokenizerFast\n",
      "EncodecConfig\n",
      "EncodecFeatureExtractor\n",
      "EncodecModel\n",
      "EncodecPreTrainedModel\n",
      "EncoderDecoderConfig\n",
      "EncoderDecoderModel\n",
      "EncoderNoRepeatNGramLogitsProcessor\n",
      "EncoderRepetitionPenaltyLogitsProcessor\n",
      "EosTokenCriteria\n",
      "EpsilonLogitsWarper\n",
      "ErnieConfig\n",
      "ErnieForCausalLM\n",
      "ErnieForMaskedLM\n",
      "ErnieForMultipleChoice\n",
      "ErnieForNextSentencePrediction\n",
      "ErnieForPreTraining\n",
      "ErnieForQuestionAnswering\n",
      "ErnieForSequenceClassification\n",
      "ErnieForTokenClassification\n",
      "ErnieMConfig\n",
      "ErnieMForInformationExtraction\n",
      "ErnieMForMultipleChoice\n",
      "ErnieMForQuestionAnswering\n",
      "ErnieMForSequenceClassification\n",
      "ErnieMForTokenClassification\n",
      "ErnieMModel\n",
      "ErnieMPreTrainedModel\n",
      "ErnieMTokenizer\n",
      "ErnieModel\n",
      "ErniePreTrainedModel\n",
      "EsmConfig\n",
      "EsmFoldPreTrainedModel\n",
      "EsmForMaskedLM\n",
      "EsmForProteinFolding\n",
      "EsmForSequenceClassification\n",
      "EsmForTokenClassification\n",
      "EsmModel\n",
      "EsmPreTrainedModel\n",
      "EsmTokenizer\n",
      "EtaLogitsWarper\n",
      "EvalPrediction\n",
      "ExponentialDecayLengthPenalty\n",
      "FEATURE_EXTRACTOR_MAPPING\n",
      "FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n",
      "FLAX_MODEL_FOR_CAUSAL_LM_MAPPING\n",
      "FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n",
      "FLAX_MODEL_FOR_MASKED_LM_MAPPING\n",
      "FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\n",
      "FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\n",
      "FLAX_MODEL_FOR_PRETRAINING_MAPPING\n",
      "FLAX_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
      "FLAX_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
      "FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n",
      "FLAX_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\n",
      "FLAX_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
      "FLAX_MODEL_FOR_VISION_2_SEQ_MAPPING\n",
      "FLAX_MODEL_MAPPING\n",
      "FNetConfig\n",
      "FNetForMaskedLM\n",
      "FNetForMultipleChoice\n",
      "FNetForNextSentencePrediction\n",
      "FNetForPreTraining\n",
      "FNetForQuestionAnswering\n",
      "FNetForSequenceClassification\n",
      "FNetForTokenClassification\n",
      "FNetLayer\n",
      "FNetModel\n",
      "FNetPreTrainedModel\n",
      "FNetTokenizer\n",
      "FNetTokenizerFast\n",
      "FSMTConfig\n",
      "FSMTForConditionalGeneration\n",
      "FSMTModel\n",
      "FSMTTokenizer\n",
      "FalconConfig\n",
      "FalconForCausalLM\n",
      "FalconForQuestionAnswering\n",
      "FalconForSequenceClassification\n",
      "FalconForTokenClassification\n",
      "FalconModel\n",
      "FalconPreTrainedModel\n",
      "FastSpeech2ConformerConfig\n",
      "FastSpeech2ConformerHifiGan\n",
      "FastSpeech2ConformerHifiGanConfig\n",
      "FastSpeech2ConformerModel\n",
      "FastSpeech2ConformerPreTrainedModel\n",
      "FastSpeech2ConformerTokenizer\n",
      "FastSpeech2ConformerWithHifiGan\n",
      "FastSpeech2ConformerWithHifiGanConfig\n",
      "FeatureExtractionMixin\n",
      "FeatureExtractionPipeline\n",
      "FillMaskPipeline\n",
      "FlaubertConfig\n",
      "FlaubertForMultipleChoice\n",
      "FlaubertForQuestionAnswering\n",
      "FlaubertForQuestionAnsweringSimple\n",
      "FlaubertForSequenceClassification\n",
      "FlaubertForTokenClassification\n",
      "FlaubertModel\n",
      "FlaubertPreTrainedModel\n",
      "FlaubertTokenizer\n",
      "FlaubertWithLMHeadModel\n",
      "FlavaConfig\n",
      "FlavaFeatureExtractor\n",
      "FlavaForPreTraining\n",
      "FlavaImageCodebook\n",
      "FlavaImageCodebookConfig\n",
      "FlavaImageConfig\n",
      "FlavaImageModel\n",
      "FlavaImageProcessor\n",
      "FlavaModel\n",
      "FlavaMultimodalConfig\n",
      "FlavaMultimodalModel\n",
      "FlavaPreTrainedModel\n",
      "FlavaProcessor\n",
      "FlavaTextConfig\n",
      "FlavaTextModel\n",
      "FlaxAlbertForMaskedLM\n",
      "FlaxAlbertForMultipleChoice\n",
      "FlaxAlbertForPreTraining\n",
      "FlaxAlbertForQuestionAnswering\n",
      "FlaxAlbertForSequenceClassification\n",
      "FlaxAlbertForTokenClassification\n",
      "FlaxAlbertModel\n",
      "FlaxAlbertPreTrainedModel\n",
      "FlaxAutoModel\n",
      "FlaxAutoModelForCausalLM\n",
      "FlaxAutoModelForImageClassification\n",
      "FlaxAutoModelForMaskedLM\n",
      "FlaxAutoModelForMultipleChoice\n",
      "FlaxAutoModelForNextSentencePrediction\n",
      "FlaxAutoModelForPreTraining\n",
      "FlaxAutoModelForQuestionAnswering\n",
      "FlaxAutoModelForSeq2SeqLM\n",
      "FlaxAutoModelForSequenceClassification\n",
      "FlaxAutoModelForSpeechSeq2Seq\n",
      "FlaxAutoModelForTokenClassification\n",
      "FlaxAutoModelForVision2Seq\n",
      "FlaxBartDecoderPreTrainedModel\n",
      "FlaxBartForCausalLM\n",
      "FlaxBartForConditionalGeneration\n",
      "FlaxBartForQuestionAnswering\n",
      "FlaxBartForSequenceClassification\n",
      "FlaxBartModel\n",
      "FlaxBartPreTrainedModel\n",
      "FlaxBeitForImageClassification\n",
      "FlaxBeitForMaskedImageModeling\n",
      "FlaxBeitModel\n",
      "FlaxBeitPreTrainedModel\n",
      "FlaxBertForCausalLM\n",
      "FlaxBertForMaskedLM\n",
      "FlaxBertForMultipleChoice\n",
      "FlaxBertForNextSentencePrediction\n",
      "FlaxBertForPreTraining\n",
      "FlaxBertForQuestionAnswering\n",
      "FlaxBertForSequenceClassification\n",
      "FlaxBertForTokenClassification\n",
      "FlaxBertModel\n",
      "FlaxBertPreTrainedModel\n",
      "FlaxBigBirdForCausalLM\n",
      "FlaxBigBirdForMaskedLM\n",
      "FlaxBigBirdForMultipleChoice\n",
      "FlaxBigBirdForPreTraining\n",
      "FlaxBigBirdForQuestionAnswering\n",
      "FlaxBigBirdForSequenceClassification\n",
      "FlaxBigBirdForTokenClassification\n",
      "FlaxBigBirdModel\n",
      "FlaxBigBirdPreTrainedModel\n",
      "FlaxBlenderbotForConditionalGeneration\n",
      "FlaxBlenderbotModel\n",
      "FlaxBlenderbotPreTrainedModel\n",
      "FlaxBlenderbotSmallForConditionalGeneration\n",
      "FlaxBlenderbotSmallModel\n",
      "FlaxBlenderbotSmallPreTrainedModel\n",
      "FlaxBloomForCausalLM\n",
      "FlaxBloomModel\n",
      "FlaxBloomPreTrainedModel\n",
      "FlaxCLIPModel\n",
      "FlaxCLIPPreTrainedModel\n",
      "FlaxCLIPTextModel\n",
      "FlaxCLIPTextModelWithProjection\n",
      "FlaxCLIPTextPreTrainedModel\n",
      "FlaxCLIPVisionModel\n",
      "FlaxCLIPVisionPreTrainedModel\n",
      "FlaxDistilBertForMaskedLM\n",
      "FlaxDistilBertForMultipleChoice\n",
      "FlaxDistilBertForQuestionAnswering\n",
      "FlaxDistilBertForSequenceClassification\n",
      "FlaxDistilBertForTokenClassification\n",
      "FlaxDistilBertModel\n",
      "FlaxDistilBertPreTrainedModel\n",
      "FlaxElectraForCausalLM\n",
      "FlaxElectraForMaskedLM\n",
      "FlaxElectraForMultipleChoice\n",
      "FlaxElectraForPreTraining\n",
      "FlaxElectraForQuestionAnswering\n",
      "FlaxElectraForSequenceClassification\n",
      "FlaxElectraForTokenClassification\n",
      "FlaxElectraModel\n",
      "FlaxElectraPreTrainedModel\n",
      "FlaxEncoderDecoderModel\n",
      "FlaxForceTokensLogitsProcessor\n",
      "FlaxForcedBOSTokenLogitsProcessor\n",
      "FlaxForcedEOSTokenLogitsProcessor\n",
      "FlaxGPT2LMHeadModel\n",
      "FlaxGPT2Model\n",
      "FlaxGPT2PreTrainedModel\n",
      "FlaxGPTJForCausalLM\n",
      "FlaxGPTJModel\n",
      "FlaxGPTJPreTrainedModel\n",
      "FlaxGPTNeoForCausalLM\n",
      "FlaxGPTNeoModel\n",
      "FlaxGPTNeoPreTrainedModel\n",
      "FlaxGemmaForCausalLM\n",
      "FlaxGemmaModel\n",
      "FlaxGemmaPreTrainedModel\n",
      "FlaxGenerationMixin\n",
      "FlaxLlamaForCausalLM\n",
      "FlaxLlamaModel\n",
      "FlaxLlamaPreTrainedModel\n",
      "FlaxLogitsProcessor\n",
      "FlaxLogitsProcessorList\n",
      "FlaxLogitsWarper\n",
      "FlaxLongT5ForConditionalGeneration\n",
      "FlaxLongT5Model\n",
      "FlaxLongT5PreTrainedModel\n",
      "FlaxMBartForConditionalGeneration\n",
      "FlaxMBartForQuestionAnswering\n",
      "FlaxMBartForSequenceClassification\n",
      "FlaxMBartModel\n",
      "FlaxMBartPreTrainedModel\n",
      "FlaxMT5EncoderModel\n",
      "FlaxMT5ForConditionalGeneration\n",
      "FlaxMT5Model\n",
      "FlaxMarianMTModel\n",
      "FlaxMarianModel\n",
      "FlaxMarianPreTrainedModel\n",
      "FlaxMinLengthLogitsProcessor\n",
      "FlaxMistralForCausalLM\n",
      "FlaxMistralModel\n",
      "FlaxMistralPreTrainedModel\n",
      "FlaxOPTForCausalLM\n",
      "FlaxOPTModel\n",
      "FlaxOPTPreTrainedModel\n",
      "FlaxPegasusForConditionalGeneration\n",
      "FlaxPegasusModel\n",
      "FlaxPegasusPreTrainedModel\n",
      "FlaxPreTrainedModel\n",
      "FlaxRegNetForImageClassification\n",
      "FlaxRegNetModel\n",
      "FlaxRegNetPreTrainedModel\n",
      "FlaxResNetForImageClassification\n",
      "FlaxResNetModel\n",
      "FlaxResNetPreTrainedModel\n",
      "FlaxRoFormerForMaskedLM\n",
      "FlaxRoFormerForMultipleChoice\n",
      "FlaxRoFormerForQuestionAnswering\n",
      "FlaxRoFormerForSequenceClassification\n",
      "FlaxRoFormerForTokenClassification\n",
      "FlaxRoFormerModel\n",
      "FlaxRoFormerPreTrainedModel\n",
      "FlaxRobertaForCausalLM\n",
      "FlaxRobertaForMaskedLM\n",
      "FlaxRobertaForMultipleChoice\n",
      "FlaxRobertaForQuestionAnswering\n",
      "FlaxRobertaForSequenceClassification\n",
      "FlaxRobertaForTokenClassification\n",
      "FlaxRobertaModel\n",
      "FlaxRobertaPreLayerNormForCausalLM\n",
      "FlaxRobertaPreLayerNormForMaskedLM\n",
      "FlaxRobertaPreLayerNormForMultipleChoice\n",
      "FlaxRobertaPreLayerNormForQuestionAnswering\n",
      "FlaxRobertaPreLayerNormForSequenceClassification\n",
      "FlaxRobertaPreLayerNormForTokenClassification\n",
      "FlaxRobertaPreLayerNormModel\n",
      "FlaxRobertaPreLayerNormPreTrainedModel\n",
      "FlaxRobertaPreTrainedModel\n",
      "FlaxSpeechEncoderDecoderModel\n",
      "FlaxSuppressTokensAtBeginLogitsProcessor\n",
      "FlaxSuppressTokensLogitsProcessor\n",
      "FlaxT5EncoderModel\n",
      "FlaxT5ForConditionalGeneration\n",
      "FlaxT5Model\n",
      "FlaxT5PreTrainedModel\n",
      "FlaxTemperatureLogitsWarper\n",
      "FlaxTopKLogitsWarper\n",
      "FlaxTopPLogitsWarper\n",
      "FlaxViTForImageClassification\n",
      "FlaxViTModel\n",
      "FlaxViTPreTrainedModel\n",
      "FlaxVisionEncoderDecoderModel\n",
      "FlaxVisionTextDualEncoderModel\n",
      "FlaxWav2Vec2ForCTC\n",
      "FlaxWav2Vec2ForPreTraining\n",
      "FlaxWav2Vec2Model\n",
      "FlaxWav2Vec2PreTrainedModel\n",
      "FlaxWhisperForAudioClassification\n",
      "FlaxWhisperForConditionalGeneration\n",
      "FlaxWhisperModel\n",
      "FlaxWhisperPreTrainedModel\n",
      "FlaxWhisperTimeStampLogitsProcessor\n",
      "FlaxXGLMForCausalLM\n",
      "FlaxXGLMModel\n",
      "FlaxXGLMPreTrainedModel\n",
      "FlaxXLMRobertaForCausalLM\n",
      "FlaxXLMRobertaForMaskedLM\n",
      "FlaxXLMRobertaForMultipleChoice\n",
      "FlaxXLMRobertaForQuestionAnswering\n",
      "FlaxXLMRobertaForSequenceClassification\n",
      "FlaxXLMRobertaForTokenClassification\n",
      "FlaxXLMRobertaModel\n",
      "FlaxXLMRobertaPreTrainedModel\n",
      "FocalNetBackbone\n",
      "FocalNetConfig\n",
      "FocalNetForImageClassification\n",
      "FocalNetForMaskedImageModeling\n",
      "FocalNetModel\n",
      "FocalNetPreTrainedModel\n",
      "ForceTokensLogitsProcessor\n",
      "ForcedBOSTokenLogitsProcessor\n",
      "ForcedEOSTokenLogitsProcessor\n",
      "FunnelBaseModel\n",
      "FunnelConfig\n",
      "FunnelForMaskedLM\n",
      "FunnelForMultipleChoice\n",
      "FunnelForPreTraining\n",
      "FunnelForQuestionAnswering\n",
      "FunnelForSequenceClassification\n",
      "FunnelForTokenClassification\n",
      "FunnelModel\n",
      "FunnelPreTrainedModel\n",
      "FunnelTokenizer\n",
      "FunnelTokenizerFast\n",
      "FuyuConfig\n",
      "FuyuForCausalLM\n",
      "FuyuImageProcessor\n",
      "FuyuPreTrainedModel\n",
      "FuyuProcessor\n",
      "GLPNConfig\n",
      "GLPNFeatureExtractor\n",
      "GLPNForDepthEstimation\n",
      "GLPNImageProcessor\n",
      "GLPNModel\n",
      "GLPNPreTrainedModel\n",
      "GPT2Config\n",
      "GPT2DoubleHeadsModel\n",
      "GPT2ForQuestionAnswering\n",
      "GPT2ForSequenceClassification\n",
      "GPT2ForTokenClassification\n",
      "GPT2LMHeadModel\n",
      "GPT2Model\n",
      "GPT2PreTrainedModel\n",
      "GPT2Tokenizer\n",
      "GPT2TokenizerFast\n",
      "GPTBigCodeConfig\n",
      "GPTBigCodeForCausalLM\n",
      "GPTBigCodeForSequenceClassification\n",
      "GPTBigCodeForTokenClassification\n",
      "GPTBigCodeModel\n",
      "GPTBigCodePreTrainedModel\n",
      "GPTJConfig\n",
      "GPTJForCausalLM\n",
      "GPTJForQuestionAnswering\n",
      "GPTJForSequenceClassification\n",
      "GPTJModel\n",
      "GPTJPreTrainedModel\n",
      "GPTNeoConfig\n",
      "GPTNeoForCausalLM\n",
      "GPTNeoForQuestionAnswering\n",
      "GPTNeoForSequenceClassification\n",
      "GPTNeoForTokenClassification\n",
      "GPTNeoModel\n",
      "GPTNeoPreTrainedModel\n",
      "GPTNeoXConfig\n",
      "GPTNeoXForCausalLM\n",
      "GPTNeoXForQuestionAnswering\n",
      "GPTNeoXForSequenceClassification\n",
      "GPTNeoXForTokenClassification\n",
      "GPTNeoXJapaneseConfig\n",
      "GPTNeoXJapaneseForCausalLM\n",
      "GPTNeoXJapaneseLayer\n",
      "GPTNeoXJapaneseModel\n",
      "GPTNeoXJapanesePreTrainedModel\n",
      "GPTNeoXJapaneseTokenizer\n",
      "GPTNeoXLayer\n",
      "GPTNeoXModel\n",
      "GPTNeoXPreTrainedModel\n",
      "GPTNeoXTokenizerFast\n",
      "GPTQConfig\n",
      "GPTSanJapaneseConfig\n",
      "GPTSanJapaneseForConditionalGeneration\n",
      "GPTSanJapaneseModel\n",
      "GPTSanJapanesePreTrainedModel\n",
      "GPTSanJapaneseTokenizer\n",
      "GPTSw3Tokenizer\n",
      "Gemma2Config\n",
      "Gemma2ForCausalLM\n",
      "Gemma2ForSequenceClassification\n",
      "Gemma2ForTokenClassification\n",
      "Gemma2Model\n",
      "Gemma2PreTrainedModel\n",
      "GemmaConfig\n",
      "GemmaForCausalLM\n",
      "GemmaForSequenceClassification\n",
      "GemmaForTokenClassification\n",
      "GemmaModel\n",
      "GemmaPreTrainedModel\n",
      "GemmaTokenizer\n",
      "GemmaTokenizerFast\n",
      "GenerationConfig\n",
      "GenerationMixin\n",
      "GitConfig\n",
      "GitForCausalLM\n",
      "GitModel\n",
      "GitPreTrainedModel\n",
      "GitProcessor\n",
      "GitVisionConfig\n",
      "GitVisionModel\n",
      "GlueDataTrainingArguments\n",
      "GlueDataset\n",
      "GradientAccumulator\n",
      "GraphormerConfig\n",
      "GraphormerForGraphClassification\n",
      "GraphormerModel\n",
      "GraphormerPreTrainedModel\n",
      "GroundingDinoConfig\n",
      "GroundingDinoForObjectDetection\n",
      "GroundingDinoImageProcessor\n",
      "GroundingDinoModel\n",
      "GroundingDinoPreTrainedModel\n",
      "GroundingDinoProcessor\n",
      "GroupViTConfig\n",
      "GroupViTModel\n",
      "GroupViTPreTrainedModel\n",
      "GroupViTTextConfig\n",
      "GroupViTTextModel\n",
      "GroupViTVisionConfig\n",
      "GroupViTVisionModel\n",
      "HQQQuantizedCache\n",
      "HammingDiversityLogitsProcessor\n",
      "HerbertTokenizer\n",
      "HerbertTokenizerFast\n",
      "HfArgumentParser\n",
      "HfEngine\n",
      "HqqConfig\n",
      "HubertConfig\n",
      "HubertForCTC\n",
      "HubertForSequenceClassification\n",
      "HubertModel\n",
      "HubertPreTrainedModel\n",
      "IBertConfig\n",
      "IBertForMaskedLM\n",
      "IBertForMultipleChoice\n",
      "IBertForQuestionAnswering\n",
      "IBertForSequenceClassification\n",
      "IBertForTokenClassification\n",
      "IBertModel\n",
      "IBertPreTrainedModel\n",
      "IMAGE_PROCESSOR_MAPPING\n",
      "Idefics2Config\n",
      "Idefics2ForConditionalGeneration\n",
      "Idefics2ImageProcessor\n",
      "Idefics2Model\n",
      "Idefics2PreTrainedModel\n",
      "Idefics2Processor\n",
      "IdeficsConfig\n",
      "IdeficsForVisionText2Text\n",
      "IdeficsImageProcessor\n",
      "IdeficsModel\n",
      "IdeficsPreTrainedModel\n",
      "IdeficsProcessor\n",
      "ImageClassificationPipeline\n",
      "ImageFeatureExtractionMixin\n",
      "ImageFeatureExtractionPipeline\n",
      "ImageGPTConfig\n",
      "ImageGPTFeatureExtractor\n",
      "ImageGPTForCausalImageModeling\n",
      "ImageGPTForImageClassification\n",
      "ImageGPTImageProcessor\n",
      "ImageGPTModel\n",
      "ImageGPTPreTrainedModel\n",
      "ImageProcessingMixin\n",
      "ImageSegmentationPipeline\n",
      "ImageToImagePipeline\n",
      "ImageToTextPipeline\n",
      "InfNanRemoveLogitsProcessor\n",
      "InformerConfig\n",
      "InformerForPrediction\n",
      "InformerModel\n",
      "InformerPreTrainedModel\n",
      "InputExample\n",
      "InputFeatures\n",
      "InstructBlipConfig\n",
      "InstructBlipForConditionalGeneration\n",
      "InstructBlipPreTrainedModel\n",
      "InstructBlipProcessor\n",
      "InstructBlipQFormerConfig\n",
      "InstructBlipQFormerModel\n",
      "InstructBlipVideoConfig\n",
      "InstructBlipVideoForConditionalGeneration\n",
      "InstructBlipVideoImageProcessor\n",
      "InstructBlipVideoPreTrainedModel\n",
      "InstructBlipVideoProcessor\n",
      "InstructBlipVideoQFormerConfig\n",
      "InstructBlipVideoQFormerModel\n",
      "InstructBlipVideoVisionConfig\n",
      "InstructBlipVideoVisionModel\n",
      "InstructBlipVisionConfig\n",
      "InstructBlipVisionModel\n",
      "IntervalStrategy\n",
      "JambaConfig\n",
      "JambaForCausalLM\n",
      "JambaForSequenceClassification\n",
      "JambaModel\n",
      "JambaPreTrainedModel\n",
      "JetMoeConfig\n",
      "JetMoeForCausalLM\n",
      "JetMoeForSequenceClassification\n",
      "JetMoeModel\n",
      "JetMoePreTrainedModel\n",
      "JsonPipelineDataFormat\n",
      "JukeboxConfig\n",
      "JukeboxModel\n",
      "JukeboxPreTrainedModel\n",
      "JukeboxPrior\n",
      "JukeboxPriorConfig\n",
      "JukeboxTokenizer\n",
      "JukeboxVQVAE\n",
      "JukeboxVQVAEConfig\n",
      "KerasMetricCallback\n",
      "Kosmos2Config\n",
      "Kosmos2ForConditionalGeneration\n",
      "Kosmos2Model\n",
      "Kosmos2PreTrainedModel\n",
      "Kosmos2Processor\n",
      "LEDConfig\n",
      "LEDForConditionalGeneration\n",
      "LEDForQuestionAnswering\n",
      "LEDForSequenceClassification\n",
      "LEDModel\n",
      "LEDPreTrainedModel\n",
      "LEDTokenizer\n",
      "LEDTokenizerFast\n",
      "LayoutLMConfig\n",
      "LayoutLMForMaskedLM\n",
      "LayoutLMForQuestionAnswering\n",
      "LayoutLMForSequenceClassification\n",
      "LayoutLMForTokenClassification\n",
      "LayoutLMModel\n",
      "LayoutLMPreTrainedModel\n",
      "LayoutLMTokenizer\n",
      "LayoutLMTokenizerFast\n",
      "LayoutLMv2Config\n",
      "LayoutLMv2FeatureExtractor\n",
      "LayoutLMv2ForQuestionAnswering\n",
      "LayoutLMv2ForSequenceClassification\n",
      "LayoutLMv2ForTokenClassification\n",
      "LayoutLMv2ImageProcessor\n",
      "LayoutLMv2Model\n",
      "LayoutLMv2PreTrainedModel\n",
      "LayoutLMv2Processor\n",
      "LayoutLMv2Tokenizer\n",
      "LayoutLMv2TokenizerFast\n",
      "LayoutLMv3Config\n",
      "LayoutLMv3FeatureExtractor\n",
      "LayoutLMv3ForQuestionAnswering\n",
      "LayoutLMv3ForSequenceClassification\n",
      "LayoutLMv3ForTokenClassification\n",
      "LayoutLMv3ImageProcessor\n",
      "LayoutLMv3Model\n",
      "LayoutLMv3PreTrainedModel\n",
      "LayoutLMv3Processor\n",
      "LayoutLMv3Tokenizer\n",
      "LayoutLMv3TokenizerFast\n",
      "LayoutXLMProcessor\n",
      "LayoutXLMTokenizer\n",
      "LayoutXLMTokenizerFast\n",
      "LevitConfig\n",
      "LevitFeatureExtractor\n",
      "LevitForImageClassification\n",
      "LevitForImageClassificationWithTeacher\n",
      "LevitImageProcessor\n",
      "LevitModel\n",
      "LevitPreTrainedModel\n",
      "LiltConfig\n",
      "LiltForQuestionAnswering\n",
      "LiltForSequenceClassification\n",
      "LiltForTokenClassification\n",
      "LiltModel\n",
      "LiltPreTrainedModel\n",
      "LineByLineTextDataset\n",
      "LineByLineWithRefDataset\n",
      "LineByLineWithSOPTextDataset\n",
      "LlamaConfig\n",
      "LlamaForCausalLM\n",
      "LlamaForQuestionAnswering\n",
      "LlamaForSequenceClassification\n",
      "LlamaForTokenClassification\n",
      "LlamaModel\n",
      "LlamaPreTrainedModel\n",
      "LlamaTokenizer\n",
      "LlamaTokenizerFast\n",
      "LlavaConfig\n",
      "LlavaForConditionalGeneration\n",
      "LlavaNextConfig\n",
      "LlavaNextForConditionalGeneration\n",
      "LlavaNextImageProcessor\n",
      "LlavaNextPreTrainedModel\n",
      "LlavaNextProcessor\n",
      "LlavaNextVideoConfig\n",
      "LlavaNextVideoForConditionalGeneration\n",
      "LlavaNextVideoImageProcessor\n",
      "LlavaNextVideoPreTrainedModel\n",
      "LlavaNextVideoProcessor\n",
      "LlavaPreTrainedModel\n",
      "LlavaProcessor\n",
      "LogitNormalization\n",
      "LogitsProcessor\n",
      "LogitsProcessorList\n",
      "LogitsWarper\n",
      "LongT5Config\n",
      "LongT5EncoderModel\n",
      "LongT5ForConditionalGeneration\n",
      "LongT5Model\n",
      "LongT5PreTrainedModel\n",
      "LongformerConfig\n",
      "LongformerForMaskedLM\n",
      "LongformerForMultipleChoice\n",
      "LongformerForQuestionAnswering\n",
      "LongformerForSequenceClassification\n",
      "LongformerForTokenClassification\n",
      "LongformerModel\n",
      "LongformerPreTrainedModel\n",
      "LongformerSelfAttention\n",
      "LongformerTokenizer\n",
      "LongformerTokenizerFast\n",
      "LukeConfig\n",
      "LukeForEntityClassification\n",
      "LukeForEntityPairClassification\n",
      "LukeForEntitySpanClassification\n",
      "LukeForMaskedLM\n",
      "LukeForMultipleChoice\n",
      "LukeForQuestionAnswering\n",
      "LukeForSequenceClassification\n",
      "LukeForTokenClassification\n",
      "LukeModel\n",
      "LukePreTrainedModel\n",
      "LukeTokenizer\n",
      "LxmertConfig\n",
      "LxmertEncoder\n",
      "LxmertForPreTraining\n",
      "LxmertForQuestionAnswering\n",
      "LxmertModel\n",
      "LxmertPreTrainedModel\n",
      "LxmertTokenizer\n",
      "LxmertTokenizerFast\n",
      "LxmertVisualFeatureEncoder\n",
      "LxmertXLayer\n",
      "M2M100Config\n",
      "M2M100ForConditionalGeneration\n",
      "M2M100Model\n",
      "M2M100PreTrainedModel\n",
      "M2M100Tokenizer\n",
      "MBart50Tokenizer\n",
      "MBart50TokenizerFast\n",
      "MBartConfig\n",
      "MBartForCausalLM\n",
      "MBartForConditionalGeneration\n",
      "MBartForQuestionAnswering\n",
      "MBartForSequenceClassification\n",
      "MBartModel\n",
      "MBartPreTrainedModel\n",
      "MBartTokenizer\n",
      "MBartTokenizerFast\n",
      "MCTCTConfig\n",
      "MCTCTFeatureExtractor\n",
      "MCTCTForCTC\n",
      "MCTCTModel\n",
      "MCTCTPreTrainedModel\n",
      "MCTCTProcessor\n",
      "MLukeTokenizer\n",
      "MMBTConfig\n",
      "MMBTForClassification\n",
      "MMBTModel\n",
      "MODEL_CARD_NAME\n",
      "MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_AUDIO_FRAME_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_AUDIO_XVECTOR_MAPPING\n",
      "MODEL_FOR_BACKBONE_MAPPING\n",
      "MODEL_FOR_CAUSAL_IMAGE_MODELING_MAPPING\n",
      "MODEL_FOR_CAUSAL_LM_MAPPING\n",
      "MODEL_FOR_CTC_MAPPING\n",
      "MODEL_FOR_DEPTH_ESTIMATION_MAPPING\n",
      "MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\n",
      "MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_IMAGE_MAPPING\n",
      "MODEL_FOR_IMAGE_SEGMENTATION_MAPPING\n",
      "MODEL_FOR_IMAGE_TO_IMAGE_MAPPING\n",
      "MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING\n",
      "MODEL_FOR_KEYPOINT_DETECTION_MAPPING\n",
      "MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\n",
      "MODEL_FOR_MASKED_LM_MAPPING\n",
      "MODEL_FOR_MASK_GENERATION_MAPPING\n",
      "MODEL_FOR_MULTIPLE_CHOICE_MAPPING\n",
      "MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\n",
      "MODEL_FOR_OBJECT_DETECTION_MAPPING\n",
      "MODEL_FOR_PRETRAINING_MAPPING\n",
      "MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
      "MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\n",
      "MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n",
      "MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\n",
      "MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\n",
      "MODEL_FOR_TEXT_ENCODING_MAPPING\n",
      "MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING\n",
      "MODEL_FOR_TEXT_TO_WAVEFORM_MAPPING\n",
      "MODEL_FOR_TIME_SERIES_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_TIME_SERIES_REGRESSION_MAPPING\n",
      "MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\n",
      "MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_VISION_2_SEQ_MAPPING\n",
      "MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\n",
      "MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\n",
      "MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING\n",
      "MODEL_MAPPING\n",
      "MODEL_NAMES_MAPPING\n",
      "MODEL_WITH_LM_HEAD_MAPPING\n",
      "MPNetConfig\n",
      "MPNetForMaskedLM\n",
      "MPNetForMultipleChoice\n",
      "MPNetForQuestionAnswering\n",
      "MPNetForSequenceClassification\n",
      "MPNetForTokenClassification\n",
      "MPNetLayer\n",
      "MPNetModel\n",
      "MPNetPreTrainedModel\n",
      "MPNetTokenizer\n",
      "MPNetTokenizerFast\n",
      "MT5Config\n",
      "MT5EncoderModel\n",
      "MT5ForConditionalGeneration\n",
      "MT5ForQuestionAnswering\n",
      "MT5ForSequenceClassification\n",
      "MT5ForTokenClassification\n",
      "MT5Model\n",
      "MT5PreTrainedModel\n",
      "MT5Tokenizer\n",
      "MT5TokenizerFast\n",
      "MambaConfig\n",
      "MambaForCausalLM\n",
      "MambaModel\n",
      "MambaPreTrainedModel\n",
      "MarianConfig\n",
      "MarianForCausalLM\n",
      "MarianMTModel\n",
      "MarianModel\n",
      "MarianTokenizer\n",
      "MarkupLMConfig\n",
      "MarkupLMFeatureExtractor\n",
      "MarkupLMForQuestionAnswering\n",
      "MarkupLMForSequenceClassification\n",
      "MarkupLMForTokenClassification\n",
      "MarkupLMModel\n",
      "MarkupLMPreTrainedModel\n",
      "MarkupLMProcessor\n",
      "MarkupLMTokenizer\n",
      "MarkupLMTokenizerFast\n",
      "Mask2FormerConfig\n",
      "Mask2FormerForUniversalSegmentation\n",
      "Mask2FormerImageProcessor\n",
      "Mask2FormerModel\n",
      "Mask2FormerPreTrainedModel\n",
      "MaskFormerConfig\n",
      "MaskFormerFeatureExtractor\n",
      "MaskFormerForInstanceSegmentation\n",
      "MaskFormerImageProcessor\n",
      "MaskFormerModel\n",
      "MaskFormerPreTrainedModel\n",
      "MaskFormerSwinBackbone\n",
      "MaskFormerSwinConfig\n",
      "MaskGenerationPipeline\n",
      "MaxLengthCriteria\n",
      "MaxTimeCriteria\n",
      "MecabTokenizer\n",
      "MegaConfig\n",
      "MegaForCausalLM\n",
      "MegaForMaskedLM\n",
      "MegaForMultipleChoice\n",
      "MegaForQuestionAnswering\n",
      "MegaForSequenceClassification\n",
      "MegaForTokenClassification\n",
      "MegaModel\n",
      "MegaPreTrainedModel\n",
      "MegatronBertConfig\n",
      "MegatronBertForCausalLM\n",
      "MegatronBertForMaskedLM\n",
      "MegatronBertForMultipleChoice\n",
      "MegatronBertForNextSentencePrediction\n",
      "MegatronBertForPreTraining\n",
      "MegatronBertForQuestionAnswering\n",
      "MegatronBertForSequenceClassification\n",
      "MegatronBertForTokenClassification\n",
      "MegatronBertModel\n",
      "MegatronBertPreTrainedModel\n",
      "MgpstrConfig\n",
      "MgpstrForSceneTextRecognition\n",
      "MgpstrModel\n",
      "MgpstrPreTrainedModel\n",
      "MgpstrProcessor\n",
      "MgpstrTokenizer\n",
      "MinLengthLogitsProcessor\n",
      "MinNewTokensLengthLogitsProcessor\n",
      "MinPLogitsWarper\n",
      "MistralConfig\n",
      "MistralForCausalLM\n",
      "MistralForSequenceClassification\n",
      "MistralForTokenClassification\n",
      "MistralModel\n",
      "MistralPreTrainedModel\n",
      "MixtralConfig\n",
      "MixtralForCausalLM\n",
      "MixtralForSequenceClassification\n",
      "MixtralForTokenClassification\n",
      "MixtralModel\n",
      "MixtralPreTrainedModel\n",
      "MobileBertConfig\n",
      "MobileBertForMaskedLM\n",
      "MobileBertForMultipleChoice\n",
      "MobileBertForNextSentencePrediction\n",
      "MobileBertForPreTraining\n",
      "MobileBertForQuestionAnswering\n",
      "MobileBertForSequenceClassification\n",
      "MobileBertForTokenClassification\n",
      "MobileBertLayer\n",
      "MobileBertModel\n",
      "MobileBertPreTrainedModel\n",
      "MobileBertTokenizer\n",
      "MobileBertTokenizerFast\n",
      "MobileNetV1Config\n",
      "MobileNetV1FeatureExtractor\n",
      "MobileNetV1ForImageClassification\n",
      "MobileNetV1ImageProcessor\n",
      "MobileNetV1Model\n",
      "MobileNetV1PreTrainedModel\n",
      "MobileNetV2Config\n",
      "MobileNetV2FeatureExtractor\n",
      "MobileNetV2ForImageClassification\n",
      "MobileNetV2ForSemanticSegmentation\n",
      "MobileNetV2ImageProcessor\n",
      "MobileNetV2Model\n",
      "MobileNetV2PreTrainedModel\n",
      "MobileViTConfig\n",
      "MobileViTFeatureExtractor\n",
      "MobileViTForImageClassification\n",
      "MobileViTForSemanticSegmentation\n",
      "MobileViTImageProcessor\n",
      "MobileViTModel\n",
      "MobileViTPreTrainedModel\n",
      "MobileViTV2Config\n",
      "MobileViTV2ForImageClassification\n",
      "MobileViTV2ForSemanticSegmentation\n",
      "MobileViTV2Model\n",
      "MobileViTV2PreTrainedModel\n",
      "ModalEmbeddings\n",
      "ModelCard\n",
      "MptConfig\n",
      "MptForCausalLM\n",
      "MptForQuestionAnswering\n",
      "MptForSequenceClassification\n",
      "MptForTokenClassification\n",
      "MptModel\n",
      "MptPreTrainedModel\n",
      "MraConfig\n",
      "MraForMaskedLM\n",
      "MraForMultipleChoice\n",
      "MraForQuestionAnswering\n",
      "MraForSequenceClassification\n",
      "MraForTokenClassification\n",
      "MraModel\n",
      "MraPreTrainedModel\n",
      "MusicgenConfig\n",
      "MusicgenDecoderConfig\n",
      "MusicgenForCausalLM\n",
      "MusicgenForConditionalGeneration\n",
      "MusicgenMelodyConfig\n",
      "MusicgenMelodyDecoderConfig\n",
      "MusicgenMelodyFeatureExtractor\n",
      "MusicgenMelodyForCausalLM\n",
      "MusicgenMelodyForConditionalGeneration\n",
      "MusicgenMelodyModel\n",
      "MusicgenMelodyPreTrainedModel\n",
      "MusicgenMelodyProcessor\n",
      "MusicgenModel\n",
      "MusicgenPreTrainedModel\n",
      "MusicgenProcessor\n",
      "MvpConfig\n",
      "MvpForCausalLM\n",
      "MvpForConditionalGeneration\n",
      "MvpForQuestionAnswering\n",
      "MvpForSequenceClassification\n",
      "MvpModel\n",
      "MvpPreTrainedModel\n",
      "MvpTokenizer\n",
      "MvpTokenizerFast\n",
      "NatBackbone\n",
      "NatConfig\n",
      "NatForImageClassification\n",
      "NatModel\n",
      "NatPreTrainedModel\n",
      "NerPipeline\n",
      "NezhaConfig\n",
      "NezhaForMaskedLM\n",
      "NezhaForMultipleChoice\n",
      "NezhaForNextSentencePrediction\n",
      "NezhaForPreTraining\n",
      "NezhaForQuestionAnswering\n",
      "NezhaForSequenceClassification\n",
      "NezhaForTokenClassification\n",
      "NezhaModel\n",
      "NezhaPreTrainedModel\n",
      "NllbMoeConfig\n",
      "NllbMoeForConditionalGeneration\n",
      "NllbMoeModel\n",
      "NllbMoePreTrainedModel\n",
      "NllbMoeSparseMLP\n",
      "NllbMoeTop2Router\n",
      "NllbTokenizer\n",
      "NllbTokenizerFast\n",
      "NoBadWordsLogitsProcessor\n",
      "NoRepeatNGramLogitsProcessor\n",
      "NougatImageProcessor\n",
      "NougatProcessor\n",
      "NougatTokenizerFast\n",
      "NystromformerConfig\n",
      "NystromformerForMaskedLM\n",
      "NystromformerForMultipleChoice\n",
      "NystromformerForQuestionAnswering\n",
      "NystromformerForSequenceClassification\n",
      "NystromformerForTokenClassification\n",
      "NystromformerLayer\n",
      "NystromformerModel\n",
      "NystromformerPreTrainedModel\n",
      "OPTConfig\n",
      "OPTForCausalLM\n",
      "OPTForQuestionAnswering\n",
      "OPTForSequenceClassification\n",
      "OPTModel\n",
      "OPTPreTrainedModel\n",
      "ObjectDetectionPipeline\n",
      "OlmoConfig\n",
      "OlmoForCausalLM\n",
      "OlmoModel\n",
      "OlmoPreTrainedModel\n",
      "OneFormerConfig\n",
      "OneFormerForUniversalSegmentation\n",
      "OneFormerImageProcessor\n",
      "OneFormerModel\n",
      "OneFormerPreTrainedModel\n",
      "OneFormerProcessor\n",
      "OpenAIGPTConfig\n",
      "OpenAIGPTDoubleHeadsModel\n",
      "OpenAIGPTForSequenceClassification\n",
      "OpenAIGPTLMHeadModel\n",
      "OpenAIGPTModel\n",
      "OpenAIGPTPreTrainedModel\n",
      "OpenAIGPTTokenizer\n",
      "OpenAIGPTTokenizerFast\n",
      "OpenLlamaConfig\n",
      "OpenLlamaForCausalLM\n",
      "OpenLlamaForSequenceClassification\n",
      "OpenLlamaModel\n",
      "OpenLlamaPreTrainedModel\n",
      "OwlViTConfig\n",
      "OwlViTFeatureExtractor\n",
      "OwlViTForObjectDetection\n",
      "OwlViTImageProcessor\n",
      "OwlViTModel\n",
      "OwlViTPreTrainedModel\n",
      "OwlViTProcessor\n",
      "OwlViTTextConfig\n",
      "OwlViTTextModel\n",
      "OwlViTVisionConfig\n",
      "OwlViTVisionModel\n",
      "Owlv2Config\n",
      "Owlv2ForObjectDetection\n",
      "Owlv2ImageProcessor\n",
      "Owlv2Model\n",
      "Owlv2PreTrainedModel\n",
      "Owlv2Processor\n",
      "Owlv2TextConfig\n",
      "Owlv2TextModel\n",
      "Owlv2VisionConfig\n",
      "Owlv2VisionModel\n",
      "PLBartConfig\n",
      "PLBartForCausalLM\n",
      "PLBartForConditionalGeneration\n",
      "PLBartForSequenceClassification\n",
      "PLBartModel\n",
      "PLBartPreTrainedModel\n",
      "PLBartTokenizer\n",
      "PROCESSOR_MAPPING\n",
      "PYTORCH_PRETRAINED_BERT_CACHE\n",
      "PYTORCH_TRANSFORMERS_CACHE\n",
      "PaliGemmaConfig\n",
      "PaliGemmaForConditionalGeneration\n",
      "PaliGemmaPreTrainedModel\n",
      "PaliGemmaProcessor\n",
      "PatchTSMixerConfig\n",
      "PatchTSMixerForPrediction\n",
      "PatchTSMixerForPretraining\n",
      "PatchTSMixerForRegression\n",
      "PatchTSMixerForTimeSeriesClassification\n",
      "PatchTSMixerModel\n",
      "PatchTSMixerPreTrainedModel\n",
      "PatchTSTConfig\n",
      "PatchTSTForClassification\n",
      "PatchTSTForPrediction\n",
      "PatchTSTForPretraining\n",
      "PatchTSTForRegression\n",
      "PatchTSTModel\n",
      "PatchTSTPreTrainedModel\n",
      "PegasusConfig\n",
      "PegasusForCausalLM\n",
      "PegasusForConditionalGeneration\n",
      "PegasusModel\n",
      "PegasusPreTrainedModel\n",
      "PegasusTokenizer\n",
      "PegasusTokenizerFast\n",
      "PegasusXConfig\n",
      "PegasusXForConditionalGeneration\n",
      "PegasusXModel\n",
      "PegasusXPreTrainedModel\n",
      "PerceiverConfig\n",
      "PerceiverFeatureExtractor\n",
      "PerceiverForImageClassificationConvProcessing\n",
      "PerceiverForImageClassificationFourier\n",
      "PerceiverForImageClassificationLearned\n",
      "PerceiverForMaskedLM\n",
      "PerceiverForMultimodalAutoencoding\n",
      "PerceiverForOpticalFlow\n",
      "PerceiverForSequenceClassification\n",
      "PerceiverImageProcessor\n",
      "PerceiverLayer\n",
      "PerceiverModel\n",
      "PerceiverPreTrainedModel\n",
      "PerceiverTokenizer\n",
      "PersimmonConfig\n",
      "PersimmonForCausalLM\n",
      "PersimmonForSequenceClassification\n",
      "PersimmonForTokenClassification\n",
      "PersimmonModel\n",
      "PersimmonPreTrainedModel\n",
      "Phi3Config\n",
      "Phi3ForCausalLM\n",
      "Phi3ForSequenceClassification\n",
      "Phi3ForTokenClassification\n",
      "Phi3Model\n",
      "Phi3PreTrainedModel\n",
      "PhiConfig\n",
      "PhiForCausalLM\n",
      "PhiForSequenceClassification\n",
      "PhiForTokenClassification\n",
      "PhiModel\n",
      "PhiPreTrainedModel\n",
      "PhobertTokenizer\n",
      "PhrasalConstraint\n",
      "PipedPipelineDataFormat\n",
      "Pipeline\n",
      "PipelineDataFormat\n",
      "PipelineTool\n",
      "Pix2StructConfig\n",
      "Pix2StructForConditionalGeneration\n",
      "Pix2StructImageProcessor\n",
      "Pix2StructPreTrainedModel\n",
      "Pix2StructProcessor\n",
      "Pix2StructTextConfig\n",
      "Pix2StructTextModel\n",
      "Pix2StructVisionConfig\n",
      "Pix2StructVisionModel\n",
      "PoolFormerConfig\n",
      "PoolFormerFeatureExtractor\n",
      "PoolFormerForImageClassification\n",
      "PoolFormerImageProcessor\n",
      "PoolFormerModel\n",
      "PoolFormerPreTrainedModel\n",
      "Pop2PianoConfig\n",
      "Pop2PianoFeatureExtractor\n",
      "Pop2PianoForConditionalGeneration\n",
      "Pop2PianoPreTrainedModel\n",
      "Pop2PianoProcessor\n",
      "Pop2PianoTokenizer\n",
      "PreTrainedModel\n",
      "PreTrainedTokenizer\n",
      "PreTrainedTokenizerBase\n",
      "PreTrainedTokenizerFast\n",
      "PrefixConstrainedLogitsProcessor\n",
      "PretrainedBartModel\n",
      "PretrainedConfig\n",
      "PretrainedFSMTModel\n",
      "PrinterCallback\n",
      "ProcessorMixin\n",
      "ProgressCallback\n",
      "ProphetNetConfig\n",
      "ProphetNetDecoder\n",
      "ProphetNetEncoder\n",
      "ProphetNetForCausalLM\n",
      "ProphetNetForConditionalGeneration\n",
      "ProphetNetModel\n",
      "ProphetNetPreTrainedModel\n",
      "ProphetNetTokenizer\n",
      "PushToHubCallback\n",
      "PvtConfig\n",
      "PvtForImageClassification\n",
      "PvtImageProcessor\n",
      "PvtModel\n",
      "PvtPreTrainedModel\n",
      "PvtV2Backbone\n",
      "PvtV2Config\n",
      "PvtV2ForImageClassification\n",
      "PvtV2Model\n",
      "PvtV2PreTrainedModel\n",
      "PyTorchBenchmark\n",
      "PyTorchBenchmarkArguments\n",
      "QDQBertConfig\n",
      "QDQBertForMaskedLM\n",
      "QDQBertForMultipleChoice\n",
      "QDQBertForNextSentencePrediction\n",
      "QDQBertForQuestionAnswering\n",
      "QDQBertForSequenceClassification\n",
      "QDQBertForTokenClassification\n",
      "QDQBertLMHeadModel\n",
      "QDQBertLayer\n",
      "QDQBertModel\n",
      "QDQBertPreTrainedModel\n",
      "QuantizedCache\n",
      "QuantizedCacheConfig\n",
      "QuantoConfig\n",
      "QuantoQuantizedCache\n",
      "QuestionAnsweringPipeline\n",
      "Qwen2Config\n",
      "Qwen2ForCausalLM\n",
      "Qwen2ForSequenceClassification\n",
      "Qwen2ForTokenClassification\n",
      "Qwen2Model\n",
      "Qwen2MoeConfig\n",
      "Qwen2MoeForCausalLM\n",
      "Qwen2MoeForSequenceClassification\n",
      "Qwen2MoeForTokenClassification\n",
      "Qwen2MoeModel\n",
      "Qwen2MoePreTrainedModel\n",
      "Qwen2PreTrainedModel\n",
      "Qwen2Tokenizer\n",
      "Qwen2TokenizerFast\n",
      "RTDetrConfig\n",
      "RTDetrForObjectDetection\n",
      "RTDetrImageProcessor\n",
      "RTDetrModel\n",
      "RTDetrPreTrainedModel\n",
      "RTDetrResNetBackbone\n",
      "RTDetrResNetConfig\n",
      "RTDetrResNetPreTrainedModel\n",
      "RagConfig\n",
      "RagModel\n",
      "RagPreTrainedModel\n",
      "RagRetriever\n",
      "RagSequenceForGeneration\n",
      "RagTokenForGeneration\n",
      "RagTokenizer\n",
      "ReactAgent\n",
      "ReactCodeAgent\n",
      "ReactJsonAgent\n",
      "RealmConfig\n",
      "RealmEmbedder\n",
      "RealmForOpenQA\n",
      "RealmKnowledgeAugEncoder\n",
      "RealmPreTrainedModel\n",
      "RealmReader\n",
      "RealmRetriever\n",
      "RealmScorer\n",
      "RealmTokenizer\n",
      "RealmTokenizerFast\n",
      "RecurrentGemmaConfig\n",
      "RecurrentGemmaForCausalLM\n",
      "RecurrentGemmaModel\n",
      "RecurrentGemmaPreTrainedModel\n",
      "ReformerAttention\n",
      "ReformerConfig\n",
      "ReformerForMaskedLM\n",
      "ReformerForQuestionAnswering\n",
      "ReformerForSequenceClassification\n",
      "ReformerLayer\n",
      "ReformerModel\n",
      "ReformerModelWithLMHead\n",
      "ReformerPreTrainedModel\n",
      "ReformerTokenizer\n",
      "ReformerTokenizerFast\n",
      "RegNetConfig\n",
      "RegNetForImageClassification\n",
      "RegNetModel\n",
      "RegNetPreTrainedModel\n",
      "RemBertConfig\n",
      "RemBertForCausalLM\n",
      "RemBertForMaskedLM\n",
      "RemBertForMultipleChoice\n",
      "RemBertForQuestionAnswering\n",
      "RemBertForSequenceClassification\n",
      "RemBertForTokenClassification\n",
      "RemBertLayer\n",
      "RemBertModel\n",
      "RemBertPreTrainedModel\n",
      "RemBertTokenizer\n",
      "RemBertTokenizerFast\n",
      "RepetitionPenaltyLogitsProcessor\n",
      "ResNetBackbone\n",
      "ResNetConfig\n",
      "ResNetForImageClassification\n",
      "ResNetModel\n",
      "ResNetPreTrainedModel\n",
      "RetriBertConfig\n",
      "RetriBertModel\n",
      "RetriBertPreTrainedModel\n",
      "RetriBertTokenizer\n",
      "RetriBertTokenizerFast\n",
      "RoCBertConfig\n",
      "RoCBertForCausalLM\n",
      "RoCBertForMaskedLM\n",
      "RoCBertForMultipleChoice\n",
      "RoCBertForPreTraining\n",
      "RoCBertForQuestionAnswering\n",
      "RoCBertForSequenceClassification\n",
      "RoCBertForTokenClassification\n",
      "RoCBertLayer\n",
      "RoCBertModel\n",
      "RoCBertPreTrainedModel\n",
      "RoCBertTokenizer\n",
      "RoFormerConfig\n",
      "RoFormerForCausalLM\n",
      "RoFormerForMaskedLM\n",
      "RoFormerForMultipleChoice\n",
      "RoFormerForQuestionAnswering\n",
      "RoFormerForSequenceClassification\n",
      "RoFormerForTokenClassification\n",
      "RoFormerLayer\n",
      "RoFormerModel\n",
      "RoFormerPreTrainedModel\n",
      "RoFormerTokenizer\n",
      "RoFormerTokenizerFast\n",
      "RobertaConfig\n",
      "RobertaForCausalLM\n",
      "RobertaForMaskedLM\n",
      "RobertaForMultipleChoice\n",
      "RobertaForQuestionAnswering\n",
      "RobertaForSequenceClassification\n",
      "RobertaForTokenClassification\n",
      "RobertaModel\n",
      "RobertaPreLayerNormConfig\n",
      "RobertaPreLayerNormForCausalLM\n",
      "RobertaPreLayerNormForMaskedLM\n",
      "RobertaPreLayerNormForMultipleChoice\n",
      "RobertaPreLayerNormForQuestionAnswering\n",
      "RobertaPreLayerNormForSequenceClassification\n",
      "RobertaPreLayerNormForTokenClassification\n",
      "RobertaPreLayerNormModel\n",
      "RobertaPreLayerNormPreTrainedModel\n",
      "RobertaPreTrainedModel\n",
      "RobertaTokenizer\n",
      "RobertaTokenizerFast\n",
      "RwkvConfig\n",
      "RwkvForCausalLM\n",
      "RwkvModel\n",
      "RwkvPreTrainedModel\n",
      "SEWConfig\n",
      "SEWDConfig\n",
      "SEWDForCTC\n",
      "SEWDForSequenceClassification\n",
      "SEWDModel\n",
      "SEWDPreTrainedModel\n",
      "SEWForCTC\n",
      "SEWForSequenceClassification\n",
      "SEWModel\n",
      "SEWPreTrainedModel\n",
      "SLOW_TO_FAST_CONVERTERS\n",
      "SPIECE_UNDERLINE\n",
      "SamConfig\n",
      "SamImageProcessor\n",
      "SamMaskDecoderConfig\n",
      "SamModel\n",
      "SamPreTrainedModel\n",
      "SamProcessor\n",
      "SamPromptEncoderConfig\n",
      "SamVisionConfig\n",
      "SchedulerType\n",
      "SeamlessM4TCodeHifiGan\n",
      "SeamlessM4TConfig\n",
      "SeamlessM4TFeatureExtractor\n",
      "SeamlessM4TForSpeechToSpeech\n",
      "SeamlessM4TForSpeechToText\n",
      "SeamlessM4TForTextToSpeech\n",
      "SeamlessM4TForTextToText\n",
      "SeamlessM4THifiGan\n",
      "SeamlessM4TModel\n",
      "SeamlessM4TPreTrainedModel\n",
      "SeamlessM4TProcessor\n",
      "SeamlessM4TTextToUnitForConditionalGeneration\n",
      "SeamlessM4TTextToUnitModel\n",
      "SeamlessM4TTokenizer\n",
      "SeamlessM4TTokenizerFast\n",
      "SeamlessM4Tv2Config\n",
      "SeamlessM4Tv2ForSpeechToSpeech\n",
      "SeamlessM4Tv2ForSpeechToText\n",
      "SeamlessM4Tv2ForTextToSpeech\n",
      "SeamlessM4Tv2ForTextToText\n",
      "SeamlessM4Tv2Model\n",
      "SeamlessM4Tv2PreTrainedModel\n",
      "SegGptConfig\n",
      "SegGptForImageSegmentation\n",
      "SegGptImageProcessor\n",
      "SegGptModel\n",
      "SegGptPreTrainedModel\n",
      "SegformerConfig\n",
      "SegformerDecodeHead\n",
      "SegformerFeatureExtractor\n",
      "SegformerForImageClassification\n",
      "SegformerForSemanticSegmentation\n",
      "SegformerImageProcessor\n",
      "SegformerLayer\n",
      "SegformerModel\n",
      "SegformerPreTrainedModel\n",
      "Seq2SeqTrainer\n",
      "Seq2SeqTrainingArguments\n",
      "SequenceBiasLogitsProcessor\n",
      "SequenceFeatureExtractor\n",
      "SiglipConfig\n",
      "SiglipForImageClassification\n",
      "SiglipImageProcessor\n",
      "SiglipModel\n",
      "SiglipPreTrainedModel\n",
      "SiglipProcessor\n",
      "SiglipTextConfig\n",
      "SiglipTextModel\n",
      "SiglipTokenizer\n",
      "SiglipVisionConfig\n",
      "SiglipVisionModel\n",
      "SingleSentenceClassificationProcessor\n",
      "SinkCache\n",
      "SpecialTokensMixin\n",
      "Speech2Text2Config\n",
      "Speech2Text2ForCausalLM\n",
      "Speech2Text2PreTrainedModel\n",
      "Speech2Text2Processor\n",
      "Speech2Text2Tokenizer\n",
      "Speech2TextConfig\n",
      "Speech2TextFeatureExtractor\n",
      "Speech2TextForConditionalGeneration\n",
      "Speech2TextModel\n",
      "Speech2TextPreTrainedModel\n",
      "Speech2TextProcessor\n",
      "Speech2TextTokenizer\n",
      "SpeechEncoderDecoderConfig\n",
      "SpeechEncoderDecoderModel\n",
      "SpeechT5Config\n",
      "SpeechT5FeatureExtractor\n",
      "SpeechT5ForSpeechToSpeech\n",
      "SpeechT5ForSpeechToText\n",
      "SpeechT5ForTextToSpeech\n",
      "SpeechT5HifiGan\n",
      "SpeechT5HifiGanConfig\n",
      "SpeechT5Model\n",
      "SpeechT5PreTrainedModel\n",
      "SpeechT5Processor\n",
      "SpeechT5Tokenizer\n",
      "SplinterConfig\n",
      "SplinterForPreTraining\n",
      "SplinterForQuestionAnswering\n",
      "SplinterLayer\n",
      "SplinterModel\n",
      "SplinterPreTrainedModel\n",
      "SplinterTokenizer\n",
      "SplinterTokenizerFast\n",
      "SquadDataTrainingArguments\n",
      "SquadDataset\n",
      "SquadExample\n",
      "SquadFeatures\n",
      "SquadV1Processor\n",
      "SquadV2Processor\n",
      "SqueezeBertConfig\n",
      "SqueezeBertForMaskedLM\n",
      "SqueezeBertForMultipleChoice\n",
      "SqueezeBertForQuestionAnswering\n",
      "SqueezeBertForSequenceClassification\n",
      "SqueezeBertForTokenClassification\n",
      "SqueezeBertModel\n",
      "SqueezeBertModule\n",
      "SqueezeBertPreTrainedModel\n",
      "SqueezeBertTokenizer\n",
      "SqueezeBertTokenizerFast\n",
      "StableLmConfig\n",
      "StableLmForCausalLM\n",
      "StableLmForSequenceClassification\n",
      "StableLmForTokenClassification\n",
      "StableLmModel\n",
      "StableLmPreTrainedModel\n",
      "Starcoder2Config\n",
      "Starcoder2ForCausalLM\n",
      "Starcoder2ForSequenceClassification\n",
      "Starcoder2ForTokenClassification\n",
      "Starcoder2Model\n",
      "Starcoder2PreTrainedModel\n",
      "StaticCache\n",
      "StopStringCriteria\n",
      "StoppingCriteria\n",
      "StoppingCriteriaList\n",
      "SummarizationPipeline\n",
      "SuperPointConfig\n",
      "SuperPointForKeypointDetection\n",
      "SuperPointImageProcessor\n",
      "SuperPointPreTrainedModel\n",
      "SuppressTokensAtBeginLogitsProcessor\n",
      "SuppressTokensLogitsProcessor\n",
      "SwiftFormerConfig\n",
      "SwiftFormerForImageClassification\n",
      "SwiftFormerModel\n",
      "SwiftFormerPreTrainedModel\n",
      "Swin2SRConfig\n",
      "Swin2SRForImageSuperResolution\n",
      "Swin2SRImageProcessor\n",
      "Swin2SRModel\n",
      "Swin2SRPreTrainedModel\n",
      "SwinBackbone\n",
      "SwinConfig\n",
      "SwinForImageClassification\n",
      "SwinForMaskedImageModeling\n",
      "SwinModel\n",
      "SwinPreTrainedModel\n",
      "Swinv2Backbone\n",
      "Swinv2Config\n",
      "Swinv2ForImageClassification\n",
      "Swinv2ForMaskedImageModeling\n",
      "Swinv2Model\n",
      "Swinv2PreTrainedModel\n",
      "SwitchTransformersConfig\n",
      "SwitchTransformersEncoderModel\n",
      "SwitchTransformersForConditionalGeneration\n",
      "SwitchTransformersModel\n",
      "SwitchTransformersPreTrainedModel\n",
      "SwitchTransformersSparseMLP\n",
      "SwitchTransformersTop1Router\n",
      "T5Config\n",
      "T5EncoderModel\n",
      "T5ForConditionalGeneration\n",
      "T5ForQuestionAnswering\n",
      "T5ForSequenceClassification\n",
      "T5ForTokenClassification\n",
      "T5Model\n",
      "T5PreTrainedModel\n",
      "T5Tokenizer\n",
      "T5TokenizerFast\n",
      "TF2_WEIGHTS_NAME\n",
      "TFAdaptiveEmbedding\n",
      "TFAlbertForMaskedLM\n",
      "TFAlbertForMultipleChoice\n",
      "TFAlbertForPreTraining\n",
      "TFAlbertForQuestionAnswering\n",
      "TFAlbertForSequenceClassification\n",
      "TFAlbertForTokenClassification\n",
      "TFAlbertMainLayer\n",
      "TFAlbertModel\n",
      "TFAlbertPreTrainedModel\n",
      "TFAutoModel\n",
      "TFAutoModelForAudioClassification\n",
      "TFAutoModelForCausalLM\n",
      "TFAutoModelForDocumentQuestionAnswering\n",
      "TFAutoModelForImageClassification\n",
      "TFAutoModelForMaskGeneration\n",
      "TFAutoModelForMaskedImageModeling\n",
      "TFAutoModelForMaskedLM\n",
      "TFAutoModelForMultipleChoice\n",
      "TFAutoModelForNextSentencePrediction\n",
      "TFAutoModelForPreTraining\n",
      "TFAutoModelForQuestionAnswering\n",
      "TFAutoModelForSemanticSegmentation\n",
      "TFAutoModelForSeq2SeqLM\n",
      "TFAutoModelForSequenceClassification\n",
      "TFAutoModelForSpeechSeq2Seq\n",
      "TFAutoModelForTableQuestionAnswering\n",
      "TFAutoModelForTextEncoding\n",
      "TFAutoModelForTokenClassification\n",
      "TFAutoModelForVision2Seq\n",
      "TFAutoModelForZeroShotImageClassification\n",
      "TFAutoModelWithLMHead\n",
      "TFBartForConditionalGeneration\n",
      "TFBartForSequenceClassification\n",
      "TFBartModel\n",
      "TFBartPretrainedModel\n",
      "TFBertEmbeddings\n",
      "TFBertForMaskedLM\n",
      "TFBertForMultipleChoice\n",
      "TFBertForNextSentencePrediction\n",
      "TFBertForPreTraining\n",
      "TFBertForQuestionAnswering\n",
      "TFBertForSequenceClassification\n",
      "TFBertForTokenClassification\n",
      "TFBertLMHeadModel\n",
      "TFBertMainLayer\n",
      "TFBertModel\n",
      "TFBertPreTrainedModel\n",
      "TFBertTokenizer\n",
      "TFBlenderbotForConditionalGeneration\n",
      "TFBlenderbotModel\n",
      "TFBlenderbotPreTrainedModel\n",
      "TFBlenderbotSmallForConditionalGeneration\n",
      "TFBlenderbotSmallModel\n",
      "TFBlenderbotSmallPreTrainedModel\n",
      "TFBlipForConditionalGeneration\n",
      "TFBlipForImageTextRetrieval\n",
      "TFBlipForQuestionAnswering\n",
      "TFBlipModel\n",
      "TFBlipPreTrainedModel\n",
      "TFBlipTextModel\n",
      "TFBlipVisionModel\n",
      "TFCLIPModel\n",
      "TFCLIPPreTrainedModel\n",
      "TFCLIPTextModel\n",
      "TFCLIPVisionModel\n",
      "TFCTRLForSequenceClassification\n",
      "TFCTRLLMHeadModel\n",
      "TFCTRLModel\n",
      "TFCTRLPreTrainedModel\n",
      "TFCamembertForCausalLM\n",
      "TFCamembertForMaskedLM\n",
      "TFCamembertForMultipleChoice\n",
      "TFCamembertForQuestionAnswering\n",
      "TFCamembertForSequenceClassification\n",
      "TFCamembertForTokenClassification\n",
      "TFCamembertModel\n",
      "TFCamembertPreTrainedModel\n",
      "TFConvBertForMaskedLM\n",
      "TFConvBertForMultipleChoice\n",
      "TFConvBertForQuestionAnswering\n",
      "TFConvBertForSequenceClassification\n",
      "TFConvBertForTokenClassification\n",
      "TFConvBertLayer\n",
      "TFConvBertModel\n",
      "TFConvBertPreTrainedModel\n",
      "TFConvNextForImageClassification\n",
      "TFConvNextModel\n",
      "TFConvNextPreTrainedModel\n",
      "TFConvNextV2ForImageClassification\n",
      "TFConvNextV2Model\n",
      "TFConvNextV2PreTrainedModel\n",
      "TFCvtForImageClassification\n",
      "TFCvtModel\n",
      "TFCvtPreTrainedModel\n",
      "TFDPRContextEncoder\n",
      "TFDPRPretrainedContextEncoder\n",
      "TFDPRPretrainedQuestionEncoder\n",
      "TFDPRPretrainedReader\n",
      "TFDPRQuestionEncoder\n",
      "TFDPRReader\n",
      "TFData2VecVisionForImageClassification\n",
      "TFData2VecVisionForSemanticSegmentation\n",
      "TFData2VecVisionModel\n",
      "TFData2VecVisionPreTrainedModel\n",
      "TFDebertaForMaskedLM\n",
      "TFDebertaForQuestionAnswering\n",
      "TFDebertaForSequenceClassification\n",
      "TFDebertaForTokenClassification\n",
      "TFDebertaModel\n",
      "TFDebertaPreTrainedModel\n",
      "TFDebertaV2ForMaskedLM\n",
      "TFDebertaV2ForMultipleChoice\n",
      "TFDebertaV2ForQuestionAnswering\n",
      "TFDebertaV2ForSequenceClassification\n",
      "TFDebertaV2ForTokenClassification\n",
      "TFDebertaV2Model\n",
      "TFDebertaV2PreTrainedModel\n",
      "TFDeiTForImageClassification\n",
      "TFDeiTForImageClassificationWithTeacher\n",
      "TFDeiTForMaskedImageModeling\n",
      "TFDeiTModel\n",
      "TFDeiTPreTrainedModel\n",
      "TFDistilBertForMaskedLM\n",
      "TFDistilBertForMultipleChoice\n",
      "TFDistilBertForQuestionAnswering\n",
      "TFDistilBertForSequenceClassification\n",
      "TFDistilBertForTokenClassification\n",
      "TFDistilBertMainLayer\n",
      "TFDistilBertModel\n",
      "TFDistilBertPreTrainedModel\n",
      "TFEfficientFormerForImageClassification\n",
      "TFEfficientFormerForImageClassificationWithTeacher\n",
      "TFEfficientFormerModel\n",
      "TFEfficientFormerPreTrainedModel\n",
      "TFElectraForMaskedLM\n",
      "TFElectraForMultipleChoice\n",
      "TFElectraForPreTraining\n",
      "TFElectraForQuestionAnswering\n",
      "TFElectraForSequenceClassification\n",
      "TFElectraForTokenClassification\n",
      "TFElectraModel\n",
      "TFElectraPreTrainedModel\n",
      "TFEncoderDecoderModel\n",
      "TFEsmForMaskedLM\n",
      "TFEsmForSequenceClassification\n",
      "TFEsmForTokenClassification\n",
      "TFEsmModel\n",
      "TFEsmPreTrainedModel\n",
      "TFFlaubertForMultipleChoice\n",
      "TFFlaubertForQuestionAnsweringSimple\n",
      "TFFlaubertForSequenceClassification\n",
      "TFFlaubertForTokenClassification\n",
      "TFFlaubertModel\n",
      "TFFlaubertPreTrainedModel\n",
      "TFFlaubertWithLMHeadModel\n",
      "TFForceTokensLogitsProcessor\n",
      "TFForcedBOSTokenLogitsProcessor\n",
      "TFForcedEOSTokenLogitsProcessor\n",
      "TFFunnelBaseModel\n",
      "TFFunnelForMaskedLM\n",
      "TFFunnelForMultipleChoice\n",
      "TFFunnelForPreTraining\n",
      "TFFunnelForQuestionAnswering\n",
      "TFFunnelForSequenceClassification\n",
      "TFFunnelForTokenClassification\n",
      "TFFunnelModel\n",
      "TFFunnelPreTrainedModel\n",
      "TFGPT2DoubleHeadsModel\n",
      "TFGPT2ForSequenceClassification\n",
      "TFGPT2LMHeadModel\n",
      "TFGPT2MainLayer\n",
      "TFGPT2Model\n",
      "TFGPT2PreTrainedModel\n",
      "TFGPT2Tokenizer\n",
      "TFGPTJForCausalLM\n",
      "TFGPTJForQuestionAnswering\n",
      "TFGPTJForSequenceClassification\n",
      "TFGPTJModel\n",
      "TFGPTJPreTrainedModel\n",
      "TFGenerationMixin\n",
      "TFGroupViTModel\n",
      "TFGroupViTPreTrainedModel\n",
      "TFGroupViTTextModel\n",
      "TFGroupViTVisionModel\n",
      "TFHubertForCTC\n",
      "TFHubertModel\n",
      "TFHubertPreTrainedModel\n",
      "TFIdeficsForVisionText2Text\n",
      "TFIdeficsModel\n",
      "TFIdeficsPreTrainedModel\n",
      "TFLEDForConditionalGeneration\n",
      "TFLEDModel\n",
      "TFLEDPreTrainedModel\n",
      "TFLayoutLMForMaskedLM\n",
      "TFLayoutLMForQuestionAnswering\n",
      "TFLayoutLMForSequenceClassification\n",
      "TFLayoutLMForTokenClassification\n",
      "TFLayoutLMMainLayer\n",
      "TFLayoutLMModel\n",
      "TFLayoutLMPreTrainedModel\n",
      "TFLayoutLMv3ForQuestionAnswering\n",
      "TFLayoutLMv3ForSequenceClassification\n",
      "TFLayoutLMv3ForTokenClassification\n",
      "TFLayoutLMv3Model\n",
      "TFLayoutLMv3PreTrainedModel\n",
      "TFLogitsProcessor\n",
      "TFLogitsProcessorList\n",
      "TFLogitsWarper\n",
      "TFLongformerForMaskedLM\n",
      "TFLongformerForMultipleChoice\n",
      "TFLongformerForQuestionAnswering\n",
      "TFLongformerForSequenceClassification\n",
      "TFLongformerForTokenClassification\n",
      "TFLongformerModel\n",
      "TFLongformerPreTrainedModel\n",
      "TFLongformerSelfAttention\n",
      "TFLxmertForPreTraining\n",
      "TFLxmertMainLayer\n",
      "TFLxmertModel\n",
      "TFLxmertPreTrainedModel\n",
      "TFLxmertVisualFeatureEncoder\n",
      "TFMBartForConditionalGeneration\n",
      "TFMBartModel\n",
      "TFMBartPreTrainedModel\n",
      "TFMPNetForMaskedLM\n",
      "TFMPNetForMultipleChoice\n",
      "TFMPNetForQuestionAnswering\n",
      "TFMPNetForSequenceClassification\n",
      "TFMPNetForTokenClassification\n",
      "TFMPNetMainLayer\n",
      "TFMPNetModel\n",
      "TFMPNetPreTrainedModel\n",
      "TFMT5EncoderModel\n",
      "TFMT5ForConditionalGeneration\n",
      "TFMT5Model\n",
      "TFMarianMTModel\n",
      "TFMarianModel\n",
      "TFMarianPreTrainedModel\n",
      "TFMinLengthLogitsProcessor\n",
      "TFMistralForCausalLM\n",
      "TFMistralForSequenceClassification\n",
      "TFMistralModel\n",
      "TFMistralPreTrainedModel\n",
      "TFMobileBertForMaskedLM\n",
      "TFMobileBertForMultipleChoice\n",
      "TFMobileBertForNextSentencePrediction\n",
      "TFMobileBertForPreTraining\n",
      "TFMobileBertForQuestionAnswering\n",
      "TFMobileBertForSequenceClassification\n",
      "TFMobileBertForTokenClassification\n",
      "TFMobileBertMainLayer\n",
      "TFMobileBertModel\n",
      "TFMobileBertPreTrainedModel\n",
      "TFMobileViTForImageClassification\n",
      "TFMobileViTForSemanticSegmentation\n",
      "TFMobileViTModel\n",
      "TFMobileViTPreTrainedModel\n",
      "TFNoBadWordsLogitsProcessor\n",
      "TFNoRepeatNGramLogitsProcessor\n",
      "TFOPTForCausalLM\n",
      "TFOPTModel\n",
      "TFOPTPreTrainedModel\n",
      "TFOpenAIGPTDoubleHeadsModel\n",
      "TFOpenAIGPTForSequenceClassification\n",
      "TFOpenAIGPTLMHeadModel\n",
      "TFOpenAIGPTMainLayer\n",
      "TFOpenAIGPTModel\n",
      "TFOpenAIGPTPreTrainedModel\n",
      "TFPegasusForConditionalGeneration\n",
      "TFPegasusModel\n",
      "TFPegasusPreTrainedModel\n",
      "TFPreTrainedModel\n",
      "TFRagModel\n",
      "TFRagPreTrainedModel\n",
      "TFRagSequenceForGeneration\n",
      "TFRagTokenForGeneration\n",
      "TFRegNetForImageClassification\n",
      "TFRegNetModel\n",
      "TFRegNetPreTrainedModel\n",
      "TFRemBertForCausalLM\n",
      "TFRemBertForMaskedLM\n",
      "TFRemBertForMultipleChoice\n",
      "TFRemBertForQuestionAnswering\n",
      "TFRemBertForSequenceClassification\n",
      "TFRemBertForTokenClassification\n",
      "TFRemBertLayer\n",
      "TFRemBertModel\n",
      "TFRemBertPreTrainedModel\n",
      "TFRepetitionPenaltyLogitsProcessor\n",
      "TFResNetForImageClassification\n",
      "TFResNetModel\n",
      "TFResNetPreTrainedModel\n",
      "TFRoFormerForCausalLM\n",
      "TFRoFormerForMaskedLM\n",
      "TFRoFormerForMultipleChoice\n",
      "TFRoFormerForQuestionAnswering\n",
      "TFRoFormerForSequenceClassification\n",
      "TFRoFormerForTokenClassification\n",
      "TFRoFormerLayer\n",
      "TFRoFormerModel\n",
      "TFRoFormerPreTrainedModel\n",
      "TFRobertaForCausalLM\n",
      "TFRobertaForMaskedLM\n",
      "TFRobertaForMultipleChoice\n",
      "TFRobertaForQuestionAnswering\n",
      "TFRobertaForSequenceClassification\n",
      "TFRobertaForTokenClassification\n",
      "TFRobertaMainLayer\n",
      "TFRobertaModel\n",
      "TFRobertaPreLayerNormForCausalLM\n",
      "TFRobertaPreLayerNormForMaskedLM\n",
      "TFRobertaPreLayerNormForMultipleChoice\n",
      "TFRobertaPreLayerNormForQuestionAnswering\n",
      "TFRobertaPreLayerNormForSequenceClassification\n",
      "TFRobertaPreLayerNormForTokenClassification\n",
      "TFRobertaPreLayerNormMainLayer\n",
      "TFRobertaPreLayerNormModel\n",
      "TFRobertaPreLayerNormPreTrainedModel\n",
      "TFRobertaPreTrainedModel\n",
      "TFSamModel\n",
      "TFSamPreTrainedModel\n",
      "TFSegformerDecodeHead\n",
      "TFSegformerForImageClassification\n",
      "TFSegformerForSemanticSegmentation\n",
      "TFSegformerModel\n",
      "TFSegformerPreTrainedModel\n",
      "TFSequenceSummary\n",
      "TFSharedEmbeddings\n",
      "TFSpeech2TextForConditionalGeneration\n",
      "TFSpeech2TextModel\n",
      "TFSpeech2TextPreTrainedModel\n",
      "TFSuppressTokensAtBeginLogitsProcessor\n",
      "TFSuppressTokensLogitsProcessor\n",
      "TFSwiftFormerForImageClassification\n",
      "TFSwiftFormerModel\n",
      "TFSwiftFormerPreTrainedModel\n",
      "TFSwinForImageClassification\n",
      "TFSwinForMaskedImageModeling\n",
      "TFSwinModel\n",
      "TFSwinPreTrainedModel\n",
      "TFT5EncoderModel\n",
      "TFT5ForConditionalGeneration\n",
      "TFT5Model\n",
      "TFT5PreTrainedModel\n",
      "TFTapasForMaskedLM\n",
      "TFTapasForQuestionAnswering\n",
      "TFTapasForSequenceClassification\n",
      "TFTapasModel\n",
      "TFTapasPreTrainedModel\n",
      "TFTemperatureLogitsWarper\n",
      "TFTopKLogitsWarper\n",
      "TFTopPLogitsWarper\n",
      "TFTrainingArguments\n",
      "TFTransfoXLForSequenceClassification\n",
      "TFTransfoXLLMHeadModel\n",
      "TFTransfoXLMainLayer\n",
      "TFTransfoXLModel\n",
      "TFTransfoXLPreTrainedModel\n",
      "TFViTForImageClassification\n",
      "TFViTMAEForPreTraining\n",
      "TFViTMAEModel\n",
      "TFViTMAEPreTrainedModel\n",
      "TFViTModel\n",
      "TFViTPreTrainedModel\n",
      "TFVisionEncoderDecoderModel\n",
      "TFVisionTextDualEncoderModel\n",
      "TFWav2Vec2ForCTC\n",
      "TFWav2Vec2ForSequenceClassification\n",
      "TFWav2Vec2Model\n",
      "TFWav2Vec2PreTrainedModel\n",
      "TFWhisperForConditionalGeneration\n",
      "TFWhisperModel\n",
      "TFWhisperPreTrainedModel\n",
      "TFXGLMForCausalLM\n",
      "TFXGLMModel\n",
      "TFXGLMPreTrainedModel\n",
      "TFXLMForMultipleChoice\n",
      "TFXLMForQuestionAnsweringSimple\n",
      "TFXLMForSequenceClassification\n",
      "TFXLMForTokenClassification\n",
      "TFXLMMainLayer\n",
      "TFXLMModel\n",
      "TFXLMPreTrainedModel\n",
      "TFXLMRobertaForCausalLM\n",
      "TFXLMRobertaForMaskedLM\n",
      "TFXLMRobertaForMultipleChoice\n",
      "TFXLMRobertaForQuestionAnswering\n",
      "TFXLMRobertaForSequenceClassification\n",
      "TFXLMRobertaForTokenClassification\n",
      "TFXLMRobertaModel\n",
      "TFXLMRobertaPreTrainedModel\n",
      "TFXLMWithLMHeadModel\n",
      "TFXLNetForMultipleChoice\n",
      "TFXLNetForQuestionAnsweringSimple\n",
      "TFXLNetForSequenceClassification\n",
      "TFXLNetForTokenClassification\n",
      "TFXLNetLMHeadModel\n",
      "TFXLNetMainLayer\n",
      "TFXLNetModel\n",
      "TFXLNetPreTrainedModel\n",
      "TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n",
      "TF_MODEL_FOR_CAUSAL_LM_MAPPING\n",
      "TF_MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING\n",
      "TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\n",
      "TF_MODEL_FOR_MASKED_IMAGE_MODELING_MAPPING\n",
      "TF_MODEL_FOR_MASKED_LM_MAPPING\n",
      "TF_MODEL_FOR_MASK_GENERATION_MAPPING\n",
      "TF_MODEL_FOR_MULTIPLE_CHOICE_MAPPING\n",
      "TF_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING\n",
      "TF_MODEL_FOR_PRETRAINING_MAPPING\n",
      "TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING\n",
      "TF_MODEL_FOR_SEMANTIC_SEGMENTATION_MAPPING\n",
      "TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING\n",
      "TF_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING\n",
      "TF_MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING\n",
      "TF_MODEL_FOR_TABLE_QUESTION_ANSWERING_MAPPING\n",
      "TF_MODEL_FOR_TEXT_ENCODING_MAPPING\n",
      "TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING\n",
      "TF_MODEL_FOR_VISION_2_SEQ_MAPPING\n",
      "TF_MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING\n",
      "TF_MODEL_MAPPING\n",
      "TF_MODEL_WITH_LM_HEAD_MAPPING\n",
      "TF_WEIGHTS_NAME\n",
      "TOKENIZER_MAPPING\n",
      "TRANSFORMERS_CACHE\n",
      "TableQuestionAnsweringPipeline\n",
      "TableTransformerConfig\n",
      "TableTransformerForObjectDetection\n",
      "TableTransformerModel\n",
      "TableTransformerPreTrainedModel\n",
      "TapasConfig\n",
      "TapasForMaskedLM\n",
      "TapasForQuestionAnswering\n",
      "TapasForSequenceClassification\n",
      "TapasModel\n",
      "TapasPreTrainedModel\n",
      "TapasTokenizer\n",
      "TapexTokenizer\n",
      "TemperatureLogitsWarper\n",
      "TensorFlowBenchmark\n",
      "TensorFlowBenchmarkArguments\n",
      "TensorType\n",
      "Text2TextGenerationPipeline\n",
      "TextClassificationPipeline\n",
      "TextDataset\n",
      "TextDatasetForNextSentencePrediction\n",
      "TextGenerationPipeline\n",
      "TextIteratorStreamer\n",
      "TextStreamer\n",
      "TextToAudioPipeline\n",
      "TimeSeriesTransformerConfig\n",
      "TimeSeriesTransformerForPrediction\n",
      "TimeSeriesTransformerModel\n",
      "TimeSeriesTransformerPreTrainedModel\n",
      "TimesformerConfig\n",
      "TimesformerForVideoClassification\n",
      "TimesformerModel\n",
      "TimesformerPreTrainedModel\n",
      "TimmBackbone\n",
      "TimmBackboneConfig\n",
      "TokenClassificationPipeline\n",
      "TokenSpan\n",
      "Tool\n",
      "ToolCollection\n",
      "Toolbox\n",
      "TopKLogitsWarper\n",
      "TopPLogitsWarper\n",
      "TrOCRConfig\n",
      "TrOCRForCausalLM\n",
      "TrOCRPreTrainedModel\n",
      "TrOCRProcessor\n",
      "Trainer\n",
      "TrainerCallback\n",
      "TrainerControl\n",
      "TrainerState\n",
      "TrainingArguments\n",
      "TrajectoryTransformerConfig\n",
      "TrajectoryTransformerModel\n",
      "TrajectoryTransformerPreTrainedModel\n",
      "TransfoXLConfig\n",
      "TransfoXLCorpus\n",
      "TransfoXLForSequenceClassification\n",
      "TransfoXLLMHeadModel\n",
      "TransfoXLModel\n",
      "TransfoXLPreTrainedModel\n",
      "TransfoXLTokenizer\n",
      "TranslationPipeline\n",
      "TvltConfig\n",
      "TvltFeatureExtractor\n",
      "TvltForAudioVisualClassification\n",
      "TvltForPreTraining\n",
      "TvltImageProcessor\n",
      "TvltModel\n",
      "TvltPreTrainedModel\n",
      "TvltProcessor\n",
      "TvpConfig\n",
      "TvpForVideoGrounding\n",
      "TvpImageProcessor\n",
      "TvpModel\n",
      "TvpPreTrainedModel\n",
      "TvpProcessor\n",
      "TypicalLogitsWarper\n",
      "UMT5Config\n",
      "UMT5EncoderModel\n",
      "UMT5ForConditionalGeneration\n",
      "UMT5ForQuestionAnswering\n",
      "UMT5ForSequenceClassification\n",
      "UMT5ForTokenClassification\n",
      "UMT5Model\n",
      "UMT5PreTrainedModel\n",
      "UdopConfig\n",
      "UdopEncoderModel\n",
      "UdopForConditionalGeneration\n",
      "UdopModel\n",
      "UdopPreTrainedModel\n",
      "UdopProcessor\n",
      "UdopTokenizer\n",
      "UdopTokenizerFast\n",
      "UnbatchedClassifierFreeGuidanceLogitsProcessor\n",
      "UniSpeechConfig\n",
      "UniSpeechForCTC\n",
      "UniSpeechForPreTraining\n",
      "UniSpeechForSequenceClassification\n",
      "UniSpeechModel\n",
      "UniSpeechPreTrainedModel\n",
      "UniSpeechSatConfig\n",
      "UniSpeechSatForAudioFrameClassification\n",
      "UniSpeechSatForCTC\n",
      "UniSpeechSatForPreTraining\n",
      "UniSpeechSatForSequenceClassification\n",
      "UniSpeechSatForXVector\n",
      "UniSpeechSatModel\n",
      "UniSpeechSatPreTrainedModel\n",
      "UnivNetConfig\n",
      "UnivNetFeatureExtractor\n",
      "UnivNetModel\n",
      "UperNetConfig\n",
      "UperNetForSemanticSegmentation\n",
      "UperNetPreTrainedModel\n",
      "VanConfig\n",
      "VanForImageClassification\n",
      "VanModel\n",
      "VanPreTrainedModel\n",
      "ViTConfig\n",
      "ViTFeatureExtractor\n",
      "ViTForImageClassification\n",
      "ViTForMaskedImageModeling\n",
      "ViTHybridConfig\n",
      "ViTHybridForImageClassification\n",
      "ViTHybridImageProcessor\n",
      "ViTHybridModel\n",
      "ViTHybridPreTrainedModel\n",
      "ViTImageProcessor\n",
      "ViTImageProcessorFast\n",
      "ViTMAEConfig\n",
      "ViTMAEForPreTraining\n",
      "ViTMAELayer\n",
      "ViTMAEModel\n",
      "ViTMAEPreTrainedModel\n",
      "ViTMSNConfig\n",
      "ViTMSNForImageClassification\n",
      "ViTMSNModel\n",
      "ViTMSNPreTrainedModel\n",
      "ViTModel\n",
      "ViTPreTrainedModel\n",
      "VideoClassificationPipeline\n",
      "VideoLlavaConfig\n",
      "VideoLlavaForConditionalGeneration\n",
      "VideoLlavaImageProcessor\n",
      "VideoLlavaPreTrainedModel\n",
      "VideoLlavaProcessor\n",
      "VideoMAEConfig\n",
      "VideoMAEFeatureExtractor\n",
      "VideoMAEForPreTraining\n",
      "VideoMAEForVideoClassification\n",
      "VideoMAEImageProcessor\n",
      "VideoMAEModel\n",
      "VideoMAEPreTrainedModel\n",
      "ViltConfig\n",
      "ViltFeatureExtractor\n",
      "ViltForImageAndTextRetrieval\n",
      "ViltForImagesAndTextClassification\n",
      "ViltForMaskedLM\n",
      "ViltForQuestionAnswering\n",
      "ViltForTokenClassification\n",
      "ViltImageProcessor\n",
      "ViltLayer\n",
      "ViltModel\n",
      "ViltPreTrainedModel\n",
      "ViltProcessor\n",
      "VipLlavaConfig\n",
      "VipLlavaForConditionalGeneration\n",
      "VipLlavaPreTrainedModel\n",
      "VisionEncoderDecoderConfig\n",
      "VisionEncoderDecoderModel\n",
      "VisionTextDualEncoderConfig\n",
      "VisionTextDualEncoderModel\n",
      "VisionTextDualEncoderProcessor\n",
      "VisualBertConfig\n",
      "VisualBertForMultipleChoice\n",
      "VisualBertForPreTraining\n",
      "VisualBertForQuestionAnswering\n",
      "VisualBertForRegionToPhraseAlignment\n",
      "VisualBertForVisualReasoning\n",
      "VisualBertLayer\n",
      "VisualBertModel\n",
      "VisualBertPreTrainedModel\n",
      "VisualQuestionAnsweringPipeline\n",
      "VitDetBackbone\n",
      "VitDetConfig\n",
      "VitDetModel\n",
      "VitDetPreTrainedModel\n",
      "VitMatteConfig\n",
      "VitMatteForImageMatting\n",
      "VitMatteImageProcessor\n",
      "VitMattePreTrainedModel\n",
      "VitsConfig\n",
      "VitsModel\n",
      "VitsPreTrainedModel\n",
      "VitsTokenizer\n",
      "VivitConfig\n",
      "VivitForVideoClassification\n",
      "VivitImageProcessor\n",
      "VivitModel\n",
      "VivitPreTrainedModel\n",
      "WEIGHTS_NAME\n",
      "WarmUp\n",
      "WatermarkDetector\n",
      "WatermarkLogitsProcessor\n",
      "WatermarkingConfig\n",
      "Wav2Vec2BertConfig\n",
      "Wav2Vec2BertForAudioFrameClassification\n",
      "Wav2Vec2BertForCTC\n",
      "Wav2Vec2BertForSequenceClassification\n",
      "Wav2Vec2BertForXVector\n",
      "Wav2Vec2BertModel\n",
      "Wav2Vec2BertPreTrainedModel\n",
      "Wav2Vec2BertProcessor\n",
      "Wav2Vec2CTCTokenizer\n",
      "Wav2Vec2Config\n",
      "Wav2Vec2ConformerConfig\n",
      "Wav2Vec2ConformerForAudioFrameClassification\n",
      "Wav2Vec2ConformerForCTC\n",
      "Wav2Vec2ConformerForPreTraining\n",
      "Wav2Vec2ConformerForSequenceClassification\n",
      "Wav2Vec2ConformerForXVector\n",
      "Wav2Vec2ConformerModel\n",
      "Wav2Vec2ConformerPreTrainedModel\n",
      "Wav2Vec2FeatureExtractor\n",
      "Wav2Vec2ForAudioFrameClassification\n",
      "Wav2Vec2ForCTC\n",
      "Wav2Vec2ForMaskedLM\n",
      "Wav2Vec2ForPreTraining\n",
      "Wav2Vec2ForSequenceClassification\n",
      "Wav2Vec2ForXVector\n",
      "Wav2Vec2Model\n",
      "Wav2Vec2PhonemeCTCTokenizer\n",
      "Wav2Vec2PreTrainedModel\n",
      "Wav2Vec2Processor\n",
      "Wav2Vec2ProcessorWithLM\n",
      "Wav2Vec2Tokenizer\n",
      "WavLMConfig\n",
      "WavLMForAudioFrameClassification\n",
      "WavLMForCTC\n",
      "WavLMForSequenceClassification\n",
      "WavLMForXVector\n",
      "WavLMModel\n",
      "WavLMPreTrainedModel\n",
      "WhisperConfig\n",
      "WhisperFeatureExtractor\n",
      "WhisperForAudioClassification\n",
      "WhisperForCausalLM\n",
      "WhisperForConditionalGeneration\n",
      "WhisperModel\n",
      "WhisperPreTrainedModel\n",
      "WhisperProcessor\n",
      "WhisperTimeStampLogitsProcessor\n",
      "WhisperTokenizer\n",
      "WhisperTokenizerFast\n",
      "WordpieceTokenizer\n",
      "XCLIPConfig\n",
      "XCLIPModel\n",
      "XCLIPPreTrainedModel\n",
      "XCLIPProcessor\n",
      "XCLIPTextConfig\n",
      "XCLIPTextModel\n",
      "XCLIPVisionConfig\n",
      "XCLIPVisionModel\n",
      "XGLMConfig\n",
      "XGLMForCausalLM\n",
      "XGLMModel\n",
      "XGLMPreTrainedModel\n",
      "XGLMTokenizer\n",
      "XGLMTokenizerFast\n",
      "XLMConfig\n",
      "XLMForMultipleChoice\n",
      "XLMForQuestionAnswering\n",
      "XLMForQuestionAnsweringSimple\n",
      "XLMForSequenceClassification\n",
      "XLMForTokenClassification\n",
      "XLMModel\n",
      "XLMPreTrainedModel\n",
      "XLMProphetNetConfig\n",
      "XLMProphetNetDecoder\n",
      "XLMProphetNetEncoder\n",
      "XLMProphetNetForCausalLM\n",
      "XLMProphetNetForConditionalGeneration\n",
      "XLMProphetNetModel\n",
      "XLMProphetNetPreTrainedModel\n",
      "XLMProphetNetTokenizer\n",
      "XLMRobertaConfig\n",
      "XLMRobertaForCausalLM\n",
      "XLMRobertaForMaskedLM\n",
      "XLMRobertaForMultipleChoice\n",
      "XLMRobertaForQuestionAnswering\n",
      "XLMRobertaForSequenceClassification\n",
      "XLMRobertaForTokenClassification\n",
      "XLMRobertaModel\n",
      "XLMRobertaPreTrainedModel\n",
      "XLMRobertaTokenizer\n",
      "XLMRobertaTokenizerFast\n",
      "XLMRobertaXLConfig\n",
      "XLMRobertaXLForCausalLM\n",
      "XLMRobertaXLForMaskedLM\n",
      "XLMRobertaXLForMultipleChoice\n",
      "XLMRobertaXLForQuestionAnswering\n",
      "XLMRobertaXLForSequenceClassification\n",
      "XLMRobertaXLForTokenClassification\n",
      "XLMRobertaXLModel\n",
      "XLMRobertaXLPreTrainedModel\n",
      "XLMTokenizer\n",
      "XLMWithLMHeadModel\n",
      "XLNetConfig\n",
      "XLNetForMultipleChoice\n",
      "XLNetForQuestionAnswering\n",
      "XLNetForQuestionAnsweringSimple\n",
      "XLNetForSequenceClassification\n",
      "XLNetForTokenClassification\n",
      "XLNetLMHeadModel\n",
      "XLNetModel\n",
      "XLNetPreTrainedModel\n",
      "XLNetTokenizer\n",
      "XLNetTokenizerFast\n",
      "XmodConfig\n",
      "XmodForCausalLM\n",
      "XmodForMaskedLM\n",
      "XmodForMultipleChoice\n",
      "XmodForQuestionAnswering\n",
      "XmodForSequenceClassification\n",
      "XmodForTokenClassification\n",
      "XmodModel\n",
      "XmodPreTrainedModel\n",
      "YolosConfig\n",
      "YolosFeatureExtractor\n",
      "YolosForObjectDetection\n",
      "YolosImageProcessor\n",
      "YolosModel\n",
      "YolosPreTrainedModel\n",
      "YosoConfig\n",
      "YosoForMaskedLM\n",
      "YosoForMultipleChoice\n",
      "YosoForQuestionAnswering\n",
      "YosoForSequenceClassification\n",
      "YosoForTokenClassification\n",
      "YosoLayer\n",
      "YosoModel\n",
      "YosoPreTrainedModel\n",
      "ZeroShotAudioClassificationPipeline\n",
      "ZeroShotClassificationPipeline\n",
      "ZeroShotImageClassificationPipeline\n",
      "ZeroShotObjectDetectionPipeline\n",
      "__all__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__path__\n",
      "__spec__\n",
      "_class_to_module\n",
      "_import_structure\n",
      "_modules\n",
      "_name\n",
      "_objects\n",
      "activations\n",
      "add_end_docstrings\n",
      "add_start_docstrings\n",
      "agents\n",
      "apply_chunking_to_forward\n",
      "audio_utils\n",
      "benchmark\n",
      "benchmark.benchmark\n",
      "benchmark.benchmark_args\n",
      "cache_utils\n",
      "commands\n",
      "configuration_utils\n",
      "convert_graph_to_onnx\n",
      "convert_slow_tokenizer\n",
      "convert_slow_tokenizers_checkpoints_to_fast\n",
      "convert_tf_hub_seq_to_seq_bert_to_pytorch\n",
      "convert_tf_weight_name_to_pt_weight_name\n",
      "create_optimizer\n",
      "data\n",
      "data.data_collator\n",
      "data.datasets\n",
      "data.metrics\n",
      "data.processors\n",
      "debug_utils\n",
      "deepspeed\n",
      "default_data_collator\n",
      "dependency_versions_check\n",
      "dependency_versions_table\n",
      "dynamic_module_utils\n",
      "enable_full_determinism\n",
      "feature_extraction_sequence_utils\n",
      "feature_extraction_utils\n",
      "file_utils\n",
      "generation\n",
      "get_constant_schedule\n",
      "get_constant_schedule_with_warmup\n",
      "get_cosine_schedule_with_warmup\n",
      "get_cosine_with_hard_restarts_schedule_with_warmup\n",
      "get_inverse_sqrt_schedule\n",
      "get_linear_schedule_with_warmup\n",
      "get_polynomial_decay_schedule_with_warmup\n",
      "get_scheduler\n",
      "get_wsd_schedule\n",
      "glue_compute_metrics\n",
      "glue_convert_examples_to_features\n",
      "glue_output_modes\n",
      "glue_processors\n",
      "glue_tasks_num_labels\n",
      "hf_argparser\n",
      "hyperparameter_search\n",
      "image_processing_base\n",
      "image_processing_utils\n",
      "image_processing_utils_fast\n",
      "image_transforms\n",
      "image_utils\n",
      "integrations\n",
      "is_apex_available\n",
      "is_av_available\n",
      "is_bitsandbytes_available\n",
      "is_clearml_available\n",
      "is_comet_available\n",
      "is_datasets_available\n",
      "is_decord_available\n",
      "is_dvclive_available\n",
      "is_faiss_available\n",
      "is_flax_available\n",
      "is_keras_nlp_available\n",
      "is_neptune_available\n",
      "is_optuna_available\n",
      "is_phonemizer_available\n",
      "is_psutil_available\n",
      "is_py3nvml_available\n",
      "is_pyctcdecode_available\n",
      "is_ray_available\n",
      "is_ray_tune_available\n",
      "is_sacremoses_available\n",
      "is_safetensors_available\n",
      "is_scipy_available\n",
      "is_sentencepiece_available\n",
      "is_sigopt_available\n",
      "is_sklearn_available\n",
      "is_speech_available\n",
      "is_tensorboard_available\n",
      "is_tensorflow_text_available\n",
      "is_tf_available\n",
      "is_timm_available\n",
      "is_tokenizers_available\n",
      "is_torch_available\n",
      "is_torch_mlu_available\n",
      "is_torch_neuroncore_available\n",
      "is_torch_npu_available\n",
      "is_torch_tpu_available\n",
      "is_torch_xla_available\n",
      "is_torch_xpu_available\n",
      "is_torchvision_available\n",
      "is_vision_available\n",
      "is_wandb_available\n",
      "launch_gradio_demo\n",
      "load_pytorch_checkpoint_in_tf2_model\n",
      "load_pytorch_model_in_tf2_model\n",
      "load_pytorch_weights_in_tf2_model\n",
      "load_tf2_checkpoint_in_pytorch_model\n",
      "load_tf2_model_in_pytorch_model\n",
      "load_tf2_weights_in_pytorch_model\n",
      "load_tf_weights_in_albert\n",
      "load_tf_weights_in_bert\n",
      "load_tf_weights_in_bert_generation\n",
      "load_tf_weights_in_big_bird\n",
      "load_tf_weights_in_canine\n",
      "load_tf_weights_in_convbert\n",
      "load_tf_weights_in_electra\n",
      "load_tf_weights_in_funnel\n",
      "load_tf_weights_in_gpt2\n",
      "load_tf_weights_in_gpt_neo\n",
      "load_tf_weights_in_imagegpt\n",
      "load_tf_weights_in_mobilebert\n",
      "load_tf_weights_in_mobilenet_v1\n",
      "load_tf_weights_in_mobilenet_v2\n",
      "load_tf_weights_in_openai_gpt\n",
      "load_tf_weights_in_qdqbert\n",
      "load_tf_weights_in_realm\n",
      "load_tf_weights_in_rembert\n",
      "load_tf_weights_in_roc_bert\n",
      "load_tf_weights_in_roformer\n",
      "load_tf_weights_in_t5\n",
      "load_tf_weights_in_tapas\n",
      "load_tf_weights_in_transfo_xl\n",
      "load_tf_weights_in_xlnet\n",
      "load_tool\n",
      "logging\n",
      "modelcard\n",
      "modeling_attn_mask_utils\n",
      "modeling_outputs\n",
      "modeling_tf_pytorch_utils\n",
      "modeling_utils\n",
      "models\n",
      "models.albert\n",
      "models.align\n",
      "models.altclip\n",
      "models.audio_spectrogram_transformer\n",
      "models.auto\n",
      "models.autoformer\n",
      "models.bark\n",
      "models.bart\n",
      "models.barthez\n",
      "models.bartpho\n",
      "models.beit\n",
      "models.bert\n",
      "models.bert_generation\n",
      "models.bert_japanese\n",
      "models.bertweet\n",
      "models.big_bird\n",
      "models.bigbird_pegasus\n",
      "models.biogpt\n",
      "models.bit\n",
      "models.blenderbot\n",
      "models.blenderbot_small\n",
      "models.blip\n",
      "models.blip_2\n",
      "models.bloom\n",
      "models.bridgetower\n",
      "models.bros\n",
      "models.byt5\n",
      "models.camembert\n",
      "models.canine\n",
      "models.chinese_clip\n",
      "models.clap\n",
      "models.clip\n",
      "models.clipseg\n",
      "models.clvp\n",
      "models.code_llama\n",
      "models.codegen\n",
      "models.cohere\n",
      "models.conditional_detr\n",
      "models.convbert\n",
      "models.convnext\n",
      "models.convnextv2\n",
      "models.cpm\n",
      "models.cpmant\n",
      "models.ctrl\n",
      "models.cvt\n",
      "models.data2vec\n",
      "models.dbrx\n",
      "models.deberta\n",
      "models.deberta_v2\n",
      "models.decision_transformer\n",
      "models.deformable_detr\n",
      "models.deit\n",
      "models.deprecated\n",
      "models.deprecated.bort\n",
      "models.deprecated.deta\n",
      "models.deprecated.efficientformer\n",
      "models.deprecated.ernie_m\n",
      "models.deprecated.gptsan_japanese\n",
      "models.deprecated.graphormer\n",
      "models.deprecated.jukebox\n",
      "models.deprecated.mctct\n",
      "models.deprecated.mega\n",
      "models.deprecated.mmbt\n",
      "models.deprecated.nat\n",
      "models.deprecated.nezha\n",
      "models.deprecated.open_llama\n",
      "models.deprecated.qdqbert\n",
      "models.deprecated.realm\n",
      "models.deprecated.retribert\n",
      "models.deprecated.speech_to_text_2\n",
      "models.deprecated.tapex\n",
      "models.deprecated.trajectory_transformer\n",
      "models.deprecated.transfo_xl\n",
      "models.deprecated.tvlt\n",
      "models.deprecated.van\n",
      "models.deprecated.vit_hybrid\n",
      "models.deprecated.xlm_prophetnet\n",
      "models.depth_anything\n",
      "models.detr\n",
      "models.dialogpt\n",
      "models.dinat\n",
      "models.dinov2\n",
      "models.distilbert\n",
      "models.dit\n",
      "models.donut\n",
      "models.dpr\n",
      "models.dpt\n",
      "models.efficientnet\n",
      "models.electra\n",
      "models.encodec\n",
      "models.encoder_decoder\n",
      "models.ernie\n",
      "models.esm\n",
      "models.falcon\n",
      "models.fastspeech2_conformer\n",
      "models.flaubert\n",
      "models.flava\n",
      "models.fnet\n",
      "models.focalnet\n",
      "models.fsmt\n",
      "models.funnel\n",
      "models.fuyu\n",
      "models.gemma\n",
      "models.gemma2\n",
      "models.git\n",
      "models.glpn\n",
      "models.gpt2\n",
      "models.gpt_bigcode\n",
      "models.gpt_neo\n",
      "models.gpt_neox\n",
      "models.gpt_neox_japanese\n",
      "models.gpt_sw3\n",
      "models.gptj\n",
      "models.grounding_dino\n",
      "models.groupvit\n",
      "models.herbert\n",
      "models.hubert\n",
      "models.ibert\n",
      "models.idefics\n",
      "models.idefics2\n",
      "models.imagegpt\n",
      "models.informer\n",
      "models.instructblip\n",
      "models.instructblipvideo\n",
      "models.jamba\n",
      "models.jetmoe\n",
      "models.kosmos2\n",
      "models.layoutlm\n",
      "models.layoutlmv2\n",
      "models.layoutlmv3\n",
      "models.layoutxlm\n",
      "models.led\n",
      "models.levit\n",
      "models.lilt\n",
      "models.llama\n",
      "models.llava\n",
      "models.llava_next\n",
      "models.llava_next_video\n",
      "models.longformer\n",
      "models.longt5\n",
      "models.luke\n",
      "models.lxmert\n",
      "models.m2m_100\n",
      "models.mamba\n",
      "models.marian\n",
      "models.markuplm\n",
      "models.mask2former\n",
      "models.maskformer\n",
      "models.mbart\n",
      "models.mbart50\n",
      "models.megatron_bert\n",
      "models.megatron_gpt2\n",
      "models.mgp_str\n",
      "models.mistral\n",
      "models.mixtral\n",
      "models.mluke\n",
      "models.mobilebert\n",
      "models.mobilenet_v1\n",
      "models.mobilenet_v2\n",
      "models.mobilevit\n",
      "models.mobilevitv2\n",
      "models.mpnet\n",
      "models.mpt\n",
      "models.mra\n",
      "models.mt5\n",
      "models.musicgen\n",
      "models.musicgen_melody\n",
      "models.mvp\n",
      "models.nllb\n",
      "models.nllb_moe\n",
      "models.nougat\n",
      "models.nystromformer\n",
      "models.olmo\n",
      "models.oneformer\n",
      "models.openai\n",
      "models.opt\n",
      "models.owlv2\n",
      "models.owlvit\n",
      "models.paligemma\n",
      "models.patchtsmixer\n",
      "models.patchtst\n",
      "models.pegasus\n",
      "models.pegasus_x\n",
      "models.perceiver\n",
      "models.persimmon\n",
      "models.phi\n",
      "models.phi3\n",
      "models.phobert\n",
      "models.pix2struct\n",
      "models.plbart\n",
      "models.poolformer\n",
      "models.pop2piano\n",
      "models.prophetnet\n",
      "models.pvt\n",
      "models.pvt_v2\n",
      "models.qwen2\n",
      "models.qwen2_moe\n",
      "models.rag\n",
      "models.recurrent_gemma\n",
      "models.reformer\n",
      "models.regnet\n",
      "models.rembert\n",
      "models.resnet\n",
      "models.roberta\n",
      "models.roberta_prelayernorm\n",
      "models.roc_bert\n",
      "models.roformer\n",
      "models.rt_detr\n",
      "models.rwkv\n",
      "models.sam\n",
      "models.seamless_m4t\n",
      "models.seamless_m4t_v2\n",
      "models.segformer\n",
      "models.seggpt\n",
      "models.sew\n",
      "models.sew_d\n",
      "models.siglip\n",
      "models.speech_encoder_decoder\n",
      "models.speech_to_text\n",
      "models.speecht5\n",
      "models.splinter\n",
      "models.squeezebert\n",
      "models.stablelm\n",
      "models.starcoder2\n",
      "models.superpoint\n",
      "models.swiftformer\n",
      "models.swin\n",
      "models.swin2sr\n",
      "models.swinv2\n",
      "models.switch_transformers\n",
      "models.t5\n",
      "models.table_transformer\n",
      "models.tapas\n",
      "models.time_series_transformer\n",
      "models.timesformer\n",
      "models.timm_backbone\n",
      "models.trocr\n",
      "models.tvp\n",
      "models.udop\n",
      "models.umt5\n",
      "models.unispeech\n",
      "models.unispeech_sat\n",
      "models.univnet\n",
      "models.upernet\n",
      "models.video_llava\n",
      "models.videomae\n",
      "models.vilt\n",
      "models.vipllava\n",
      "models.vision_encoder_decoder\n",
      "models.vision_text_dual_encoder\n",
      "models.visual_bert\n",
      "models.vit\n",
      "models.vit_mae\n",
      "models.vit_msn\n",
      "models.vitdet\n",
      "models.vitmatte\n",
      "models.vits\n",
      "models.vivit\n",
      "models.wav2vec2\n",
      "models.wav2vec2_bert\n",
      "models.wav2vec2_conformer\n",
      "models.wav2vec2_phoneme\n",
      "models.wav2vec2_with_lm\n",
      "models.wavlm\n",
      "models.whisper\n",
      "models.x_clip\n",
      "models.xglm\n",
      "models.xlm\n",
      "models.xlm_roberta\n",
      "models.xlm_roberta_xl\n",
      "models.xlnet\n",
      "models.xmod\n",
      "models.yolos\n",
      "models.yoso\n",
      "onnx\n",
      "optimization\n",
      "pipeline\n",
      "pipelines\n",
      "processing_utils\n",
      "prune_layer\n",
      "pytorch_utils\n",
      "quantizers\n",
      "requires_backends\n",
      "safetensors_conversion\n",
      "sagemaker\n",
      "set_seed\n",
      "shape_list\n",
      "squad_convert_examples_to_features\n",
      "testing_utils\n",
      "time_series_utils\n",
      "tokenization_utils\n",
      "tokenization_utils_base\n",
      "tokenization_utils_fast\n",
      "torch_distributed_zero_first\n",
      "trainer\n",
      "trainer_callback\n",
      "trainer_pt_utils\n",
      "trainer_seq2seq\n",
      "trainer_utils\n",
      "training_args\n",
      "training_args_seq2seq\n",
      "training_args_tf\n",
      "utils\n",
      "utils.dummy_essentia_and_librosa_and_pretty_midi_and_scipy_and_torch_objects\n",
      "utils.dummy_flax_objects\n",
      "utils.dummy_keras_nlp_objects\n",
      "utils.dummy_sentencepiece_and_tokenizers_objects\n",
      "utils.dummy_sentencepiece_objects\n",
      "utils.dummy_tensorflow_text_objects\n",
      "utils.dummy_tf_objects\n",
      "utils.dummy_torchaudio_objects\n",
      "utils.quantization_config\n",
      "xnli_compute_metrics\n",
      "xnli_output_modes\n",
      "xnli_processors\n",
      "xnli_tasks_num_labels\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "# Print all the attributes of the transformers module\n",
    "for attr in dir(transformers):\n",
    "    print(attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/deepspeed.py:24: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total attributes: 3258\n",
      "Functions: 2799\n",
      "Modules: 346\n",
      "Others: 113\n",
      "\n",
      "Sample Functions (first 10):\n",
      "['ASTConfig', 'ASTFeatureExtractor', 'ASTForAudioClassification', 'ASTModel', 'ASTPreTrainedModel', 'Adafactor', 'AdamW', 'AdamWeightDecay', 'AdaptiveEmbedding', 'AddedToken']\n",
      "\n",
      "Sample Modules (first 10):\n",
      "['activations', 'agents', 'audio_utils', 'benchmark', 'benchmark.benchmark', 'benchmark.benchmark_args', 'cache_utils', 'commands', 'configuration_utils', 'convert_graph_to_onnx']\n",
      "\n",
      "Other Components (first 10):\n",
      "['CONFIG_MAPPING', 'CONFIG_NAME', 'FEATURE_EXTRACTOR_MAPPING', 'FLAX_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_CAUSAL_LM_MAPPING', 'FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING', 'FLAX_MODEL_FOR_MASKED_LM_MAPPING', 'FLAX_MODEL_FOR_MULTIPLE_CHOICE_MAPPING', 'FLAX_MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING', 'FLAX_MODEL_FOR_PRETRAINING_MAPPING']\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import inspect\n",
    "\n",
    "#  This script allows to categorize the attributes into functions, subpackages and other utilities of the transformers module.\n",
    "def list_attributes(package):\n",
    "    attributes = dir(package)\n",
    "    categorized_attributes = {\n",
    "        'functions': [],\n",
    "        'modules': [],\n",
    "        'others': []\n",
    "    }\n",
    "    for attr in attributes:\n",
    "        obj = getattr(package, attr)\n",
    "        if callable(obj):\n",
    "            categorized_attributes['functions'].append(attr)\n",
    "        elif inspect.ismodule(obj):\n",
    "            categorized_attributes['modules'].append(attr)\n",
    "        else:\n",
    "            categorized_attributes['others'].append(attr)\n",
    "    return categorized_attributes\n",
    "\n",
    "attrs = list_attributes(transformers)\n",
    "\n",
    "print(f\"Total attributes: {len(attrs['functions']) + len(attrs['modules']) + len(attrs['others'])}\")\n",
    "print(f\"Functions: {len(attrs['functions'])}\")\n",
    "print(f\"Modules: {len(attrs['modules'])}\")\n",
    "print(f\"Others: {len(attrs['others'])}\")\n",
    "\n",
    "print(\"\\nSample Functions (first 10):\")\n",
    "print(attrs['functions'][:10])\n",
    "\n",
    "print(\"\\nSample Modules (first 10):\")\n",
    "print(attrs['modules'][:10])\n",
    "\n",
    "print(\"\\nOther Utilities (first 10):\")\n",
    "print(attrs['others'][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function: XLNetForMultipleChoice\n",
      "Documentation:\n",
      "\n",
      "    XLNet Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a\n",
      "    softmax) e.g. for RACE/SWAG tasks.\n",
      "    \n",
      "\n",
      "    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n",
      "    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n",
      "    etc.)\n",
      "\n",
      "    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n",
      "    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n",
      "    and behavior.\n",
      "\n",
      "    Parameters:\n",
      "        config ([`XLNetConfig`]): Model configuration class with all the parameters of the model.\n",
      "            Initializing with a config file does not load the weights associated with the model, only the\n",
      "            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This script allows to display the documentation of function as shown above.\n",
    "\n",
    "import transformers\n",
    "\n",
    "def display_function(func_name):\n",
    "    func = getattr(transformers, func_name, None)\n",
    "    if callable(func):\n",
    "        print(f\"Function: {func_name}\")\n",
    "        print(\"Documentation:\")\n",
    "        print(func.__doc__)\n",
    "    else:\n",
    "        print(f\"{func_name} is not a function in transformers.\")\n",
    "\n",
    "# Example usage\n",
    "display_function(\"XLNetForMultipleChoice\")  # Change 'pipeline' to any function name you want to inspect\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package transformers.generation in transformers:\n",
      "\n",
      "NAME\n",
      "    transformers.generation\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright 2022 The HuggingFace Team. All rights reserved.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    beam_constraints\n",
      "    beam_search\n",
      "    candidate_generator\n",
      "    configuration_utils\n",
      "    flax_logits_process\n",
      "    flax_utils\n",
      "    logits_process\n",
      "    stopping_criteria\n",
      "    streamers\n",
      "    tf_logits_process\n",
      "    tf_utils\n",
      "    utils\n",
      "    watermarking\n",
      "\n",
      "CLASSES\n",
      "    abc.ABC(builtins.object)\n",
      "        transformers.generation.beam_constraints.Constraint\n",
      "            transformers.generation.beam_constraints.DisjunctiveConstraint\n",
      "            transformers.generation.beam_constraints.PhrasalConstraint\n",
      "        transformers.generation.beam_search.BeamScorer\n",
      "            transformers.generation.beam_search.BeamSearchScorer\n",
      "            transformers.generation.beam_search.ConstrainedBeamSearchScorer\n",
      "        transformers.generation.stopping_criteria.StoppingCriteria\n",
      "            transformers.generation.stopping_criteria.EosTokenCriteria\n",
      "            transformers.generation.stopping_criteria.MaxLengthCriteria\n",
      "            transformers.generation.stopping_criteria.MaxNewTokensCriteria\n",
      "            transformers.generation.stopping_criteria.MaxTimeCriteria\n",
      "            transformers.generation.stopping_criteria.StopStringCriteria\n",
      "    builtins.list(builtins.object)\n",
      "        transformers.generation.logits_process.LogitsProcessorList\n",
      "        transformers.generation.stopping_criteria.StoppingCriteriaList\n",
      "    builtins.object\n",
      "        transformers.generation.beam_constraints.ConstraintListState\n",
      "        transformers.generation.beam_search.BeamHypotheses\n",
      "        transformers.generation.candidate_generator.CandidateGenerator\n",
      "            transformers.generation.candidate_generator.AssistedCandidateGenerator\n",
      "            transformers.generation.candidate_generator.PromptLookupCandidateGenerator\n",
      "        transformers.generation.configuration_utils.WatermarkingConfig\n",
      "        transformers.generation.logits_process.LogitsProcessor\n",
      "            transformers.generation.logits_process.AlternatingCodebooksLogitsProcessor\n",
      "            transformers.generation.logits_process.ClassifierFreeGuidanceLogitsProcessor\n",
      "            transformers.generation.logits_process.EncoderNoRepeatNGramLogitsProcessor\n",
      "            transformers.generation.logits_process.EncoderRepetitionPenaltyLogitsProcessor\n",
      "            transformers.generation.logits_process.ExponentialDecayLengthPenalty\n",
      "            transformers.generation.logits_process.ForceTokensLogitsProcessor\n",
      "            transformers.generation.logits_process.ForcedBOSTokenLogitsProcessor\n",
      "            transformers.generation.logits_process.ForcedEOSTokenLogitsProcessor\n",
      "            transformers.generation.logits_process.HammingDiversityLogitsProcessor\n",
      "            transformers.generation.logits_process.InfNanRemoveLogitsProcessor\n",
      "            transformers.generation.logits_process.LogitNormalization(transformers.generation.logits_process.LogitsProcessor, transformers.generation.logits_process.LogitsWarper)\n",
      "            transformers.generation.logits_process.MinLengthLogitsProcessor\n",
      "            transformers.generation.logits_process.MinNewTokensLengthLogitsProcessor\n",
      "            transformers.generation.logits_process.NoRepeatNGramLogitsProcessor\n",
      "            transformers.generation.logits_process.PrefixConstrainedLogitsProcessor\n",
      "            transformers.generation.logits_process.RepetitionPenaltyLogitsProcessor\n",
      "            transformers.generation.logits_process.SequenceBiasLogitsProcessor\n",
      "                transformers.generation.logits_process.NoBadWordsLogitsProcessor\n",
      "            transformers.generation.logits_process.SuppressTokensAtBeginLogitsProcessor\n",
      "            transformers.generation.logits_process.SuppressTokensLogitsProcessor\n",
      "            transformers.generation.logits_process.UnbatchedClassifierFreeGuidanceLogitsProcessor\n",
      "            transformers.generation.logits_process.WatermarkLogitsProcessor\n",
      "            transformers.generation.logits_process.WhisperTimeStampLogitsProcessor\n",
      "        transformers.generation.logits_process.LogitsWarper\n",
      "            transformers.generation.logits_process.EpsilonLogitsWarper\n",
      "            transformers.generation.logits_process.EtaLogitsWarper\n",
      "            transformers.generation.logits_process.MinPLogitsWarper\n",
      "            transformers.generation.logits_process.TemperatureLogitsWarper\n",
      "            transformers.generation.logits_process.TopKLogitsWarper\n",
      "            transformers.generation.logits_process.TopPLogitsWarper\n",
      "            transformers.generation.logits_process.TypicalLogitsWarper\n",
      "        transformers.generation.utils.GenerationMixin\n",
      "        transformers.generation.watermarking.WatermarkDetector\n",
      "        transformers.generation.watermarking.WatermarkDetectorOutput\n",
      "    transformers.generation.streamers.BaseStreamer(builtins.object)\n",
      "        transformers.generation.streamers.TextStreamer\n",
      "            transformers.generation.streamers.TextIteratorStreamer\n",
      "    transformers.utils.generic.ExplicitEnum(builtins.str, enum.Enum)\n",
      "        transformers.generation.configuration_utils.GenerationMode\n",
      "    transformers.utils.generic.ModelOutput(collections.OrderedDict)\n",
      "        transformers.generation.utils.GenerateBeamDecoderOnlyOutput\n",
      "        transformers.generation.utils.GenerateBeamEncoderDecoderOutput\n",
      "        transformers.generation.utils.GenerateDecoderOnlyOutput\n",
      "        transformers.generation.utils.GenerateEncoderDecoderOutput\n",
      "    transformers.utils.hub.PushToHubMixin(builtins.object)\n",
      "        transformers.generation.configuration_utils.GenerationConfig\n",
      "    \n",
      "    class AlternatingCodebooksLogitsProcessor(LogitsProcessor)\n",
      "     |  AlternatingCodebooksLogitsProcessor(input_start_len: int, semantic_vocab_size: int, codebook_size: int)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] enforcing alternated generation between the two codebooks of Bark.\n",
      "     |  \n",
      "     |  <Tip warning={true}>\n",
      "     |  \n",
      "     |  This logits processor is exclusively compatible with\n",
      "     |  [Bark](https://huggingface.co/docs/transformers/en/model_doc/bark)'s fine submodel. See the model documentation\n",
      "     |  for examples.\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      input_start_len (`int`):\n",
      "     |          The length of the initial input sequence.\n",
      "     |      semantic_vocab_size (`int`):\n",
      "     |          Vocabulary size of the semantic part, i.e number of tokens associated to the semantic vocabulary.\n",
      "     |      codebook_size (`int`):\n",
      "     |          Number of tokens associated to the codebook.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AlternatingCodebooksLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, input_start_len: int, semantic_vocab_size: int, codebook_size: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AssistedCandidateGenerator(CandidateGenerator)\n",
      "     |  AssistedCandidateGenerator(input_ids: torch.LongTensor, assistant_model: 'PreTrainedModel', generation_config: 'GenerationConfig', model_kwargs: Dict, inputs_tensor: Optional[torch.Tensor] = None, logits_processor: 'LogitsProcessorList' = None)\n",
      "     |  \n",
      "     |  `CandidateGenerator` class to be used for assisted generation and speculative decoding. This class generates\n",
      "     |  candidates through the use of a smaller model. Read the following blog post for more information:\n",
      "     |  https://huggingface.co/blog/assisted-generation\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |          Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |      assistant_model (`PreTrainedModel`):\n",
      "     |          The model to be used for generating candidates. This model should be smaller than the main model.\n",
      "     |      generation_config (`~generation.GenerationConfig`, *optional*):\n",
      "     |          The generation configuration to be used as base parametrization for the generation call.\n",
      "     |      logits_processor (`LogitsProcessorList`):\n",
      "     |          An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n",
      "     |          used to modify the prediction scores of the language modeling head applied at each generation step.\n",
      "     |      model_kwargs (`Dict`):\n",
      "     |          The keyword arguments that will be passed to the main model, and are used as base inputs for the assistant\n",
      "     |          model as well.\n",
      "     |      inputs_tensor (`torch.Tensor`, *optional*):\n",
      "     |          The model input tensor. In encoder-decoder models, this is the encoder input.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AssistedCandidateGenerator\n",
      "     |      CandidateGenerator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, input_ids: torch.LongTensor, assistant_model: 'PreTrainedModel', generation_config: 'GenerationConfig', model_kwargs: Dict, inputs_tensor: Optional[torch.Tensor] = None, logits_processor: 'LogitsProcessorList' = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]\n",
      "     |      Fetches the candidates to be tried for the current input.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(batch_size, candidate_length)` containing the candidate sequences to be\n",
      "     |          assessed by the model and a `torch.FloatTensor` of shape `(batch_size, candidate_length,\n",
      "     |          vocabulary_size)` containing the logits associated to each candidate.\n",
      "     |  \n",
      "     |  update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int)\n",
      "     |      Updates the candidate generation strategy based on the outcomes.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, candidate_length, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n",
      "     |              beam search or log softmax for each vocabulary token when using beam search\n",
      "     |          num_matches (`int`):\n",
      "     |              The number of matches between the candidate sequences and the model predictions.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from CandidateGenerator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BeamHypotheses(builtins.object)\n",
      "     |  BeamHypotheses(num_beams: int, length_penalty: float, early_stopping: bool, max_length: Optional[int] = None)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_beams: int, length_penalty: float, early_stopping: bool, max_length: Optional[int] = None)\n",
      "     |      Initialize n-best list of hypotheses.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Number of hypotheses in the list.\n",
      "     |  \n",
      "     |  add(self, hyp: torch.LongTensor, sum_logprobs: float, beam_indices: Optional[torch.LongTensor] = None, generated_len: Optional[int] = None)\n",
      "     |      Add a new hypothesis to the list.\n",
      "     |  \n",
      "     |  is_done(self, best_sum_logprobs: float, cur_len: int, decoder_prompt_len: Optional[int] = 0) -> bool\n",
      "     |      If there are enough hypotheses and that none of the hypotheses being generated can become better than the worst\n",
      "     |      one in the heap, then we are done with this sentence.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    BeamSampleDecoderOnlyOutput = class GenerateBeamDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  BeamSampleDecoderOnlyOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    BeamSampleEncoderDecoderOutput = class GenerateBeamEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  BeamSampleEncoderDecoderOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, num_heads, generated_length,\n",
      "     |          sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'beam_indices': typing.Optional[torch.LongTensor], ...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'beam_indices': Field(name='beam_indices',type...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BeamScorer(abc.ABC)\n",
      "     |  Abstract base class for all beam scorers that are used for [`~PreTrainedModel.beam_search`] and\n",
      "     |  [`~PreTrainedModel.beam_sample`].\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BeamScorer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  finalize(self, input_ids: torch.LongTensor, next_scores: torch.FloatTensor, next_tokens: torch.LongTensor, next_indices: torch.LongTensor, max_length: int, **kwargs) -> torch.LongTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          final_beam_scores (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The final scores of all non-finished beams.\n",
      "     |          final_beam_tokens (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The last tokens to be added to the non-finished beam_hypotheses.\n",
      "     |          final_beam_indices (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The beam indices indicating to which beam the `final_beam_tokens` shall be added.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`: The generated sequences.\n",
      "     |          The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished early\n",
      "     |          due to the `eos_token_id`.\n",
      "     |  \n",
      "     |  process(self, input_ids: torch.LongTensor, next_scores: torch.FloatTensor, next_tokens: torch.LongTensor, next_indices: torch.LongTensor, **kwargs) -> Tuple[torch.Tensor]\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          next_scores (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Current scores of the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_tokens (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_indices (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |          beam_indices (`torch.LongTensor`, *optional*):\n",
      "     |              Beam indices indicating to which beam hypothesis each token correspond.\n",
      "     |          group_index (`int`, *optional*):\n",
      "     |              The index of the group of beams. Used with [`~PreTrainedModel.group_beam_search`].\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `UserDict`: A dictionary composed of the fields as defined above:\n",
      "     |      \n",
      "     |              - **next_beam_scores** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Updated scores of all\n",
      "     |                non-finished beams.\n",
      "     |              - **next_beam_tokens** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Next tokens to be added\n",
      "     |                to the non-finished beam_hypotheses.\n",
      "     |              - **next_beam_indices** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Beam indices\n",
      "     |                indicating to which beam the next tokens shall be added.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'finalize', 'process'})\n",
      "    \n",
      "    BeamSearchDecoderOnlyOutput = class GenerateBeamDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  BeamSearchDecoderOnlyOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    BeamSearchEncoderDecoderOutput = class GenerateBeamEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  BeamSearchEncoderDecoderOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, num_heads, generated_length,\n",
      "     |          sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'beam_indices': typing.Optional[torch.LongTensor], ...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'beam_indices': Field(name='beam_indices',type...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class BeamSearchScorer(BeamScorer)\n",
      "     |  BeamSearchScorer(batch_size: int, num_beams: int, device: torch.device, length_penalty: Optional[float] = 1.0, do_early_stopping: Union[bool, str, NoneType] = False, num_beam_hyps_to_keep: Optional[int] = 1, num_beam_groups: Optional[int] = 1, max_length: Optional[int] = None)\n",
      "     |  \n",
      "     |  [`BeamScorer`] implementing standard beam search decoding.\n",
      "     |  \n",
      "     |  Adapted in part from [Facebook's XLM beam search\n",
      "     |  code](https://github.com/facebookresearch/XLM/blob/9e6f6814d17be4fe5b15f2e6c43eb2b2d76daeb4/src/model/transformer.py#L529).\n",
      "     |  \n",
      "     |  Reference for the diverse beam search algorithm and implementation [Ashwin Kalyan's DBS\n",
      "     |  implementation](https://github.com/ashwinkalyan/dbs/blob/master/dbs/beam_utils.lua)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      batch_size (`int`):\n",
      "     |          Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n",
      "     |      num_beams (`int`):\n",
      "     |          Number of beams for beam search.\n",
      "     |      device (`torch.device`):\n",
      "     |          Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n",
      "     |          allocated.\n",
      "     |      length_penalty (`float`, *optional*, defaults to 1.0):\n",
      "     |          Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n",
      "     |          the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n",
      "     |          likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n",
      "     |          `length_penalty` < 0.0 encourages shorter sequences.\n",
      "     |      do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n",
      "     |          Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n",
      "     |          `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n",
      "     |          heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n",
      "     |          `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n",
      "     |          beam search algorithm).\n",
      "     |      num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          The number of beam hypotheses that shall be returned upon calling\n",
      "     |          [`~transformers.BeamSearchScorer.finalize`].\n",
      "     |      num_beam_groups (`int`, *optional*, defaults to 1):\n",
      "     |          Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n",
      "     |          See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      "     |      max_length (`int`, *optional*):\n",
      "     |          The maximum length of the sequence to be generated.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BeamSearchScorer\n",
      "     |      BeamScorer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, batch_size: int, num_beams: int, device: torch.device, length_penalty: Optional[float] = 1.0, do_early_stopping: Union[bool, str, NoneType] = False, num_beam_hyps_to_keep: Optional[int] = 1, num_beam_groups: Optional[int] = 1, max_length: Optional[int] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  finalize(self, input_ids: torch.LongTensor, final_beam_scores: torch.FloatTensor, final_beam_tokens: torch.LongTensor, final_beam_indices: torch.LongTensor, max_length: int, pad_token_id: Union[int, torch.Tensor, NoneType] = None, eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None, beam_indices: Optional[torch.LongTensor] = None, decoder_prompt_len: Optional[int] = 0) -> Tuple[torch.LongTensor]\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          final_beam_scores (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The final scores of all non-finished beams.\n",
      "     |          final_beam_tokens (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The last tokens to be added to the non-finished beam_hypotheses.\n",
      "     |          final_beam_indices (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The beam indices indicating to which beam the `final_beam_tokens` shall be added.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`: The generated sequences.\n",
      "     |          The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished early\n",
      "     |          due to the `eos_token_id`.\n",
      "     |  \n",
      "     |  process(self, input_ids: torch.LongTensor, next_scores: torch.FloatTensor, next_tokens: torch.LongTensor, next_indices: torch.LongTensor, pad_token_id: Union[int, torch.Tensor, NoneType] = None, eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None, beam_indices: Optional[torch.LongTensor] = None, group_index: Optional[int] = 0, decoder_prompt_len: Optional[int] = 0) -> Dict[str, torch.Tensor]\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          next_scores (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Current scores of the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_tokens (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_indices (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |          beam_indices (`torch.LongTensor`, *optional*):\n",
      "     |              Beam indices indicating to which beam hypothesis each token correspond.\n",
      "     |          group_index (`int`, *optional*):\n",
      "     |              The index of the group of beams. Used with [`~PreTrainedModel.group_beam_search`].\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `UserDict`: A dictionary composed of the fields as defined above:\n",
      "     |      \n",
      "     |              - **next_beam_scores** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Updated scores of all\n",
      "     |                non-finished beams.\n",
      "     |              - **next_beam_tokens** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Next tokens to be added\n",
      "     |                to the non-finished beam_hypotheses.\n",
      "     |              - **next_beam_indices** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Beam indices\n",
      "     |                indicating to which beam the next tokens shall be added.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_done\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BeamScorer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CandidateGenerator(builtins.object)\n",
      "     |  Abstract base class for all candidate generators that can be applied during assisted generation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]\n",
      "     |      Fetches the candidates to be tried for the current input.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(batch_size, candidate_length)` containing the candidate sequences to be\n",
      "     |          assessed by the model and, optionally, a `torch.FloatTensor` of shape `(batch_size, candidate_length,\n",
      "     |          vocabulary_size)` containing the logits associated to each candidate.\n",
      "     |  \n",
      "     |  update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int)\n",
      "     |      Updates the candidate generation strategy based on the outcomes.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, candidate_length, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n",
      "     |              beam search or log softmax for each vocabulary token when using beam search\n",
      "     |          num_matches (`int`):\n",
      "     |              The number of matches between the candidate sequences and the model predictions.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ClassifierFreeGuidanceLogitsProcessor(LogitsProcessor)\n",
      "     |  ClassifierFreeGuidanceLogitsProcessor(guidance_scale)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] for classifier free guidance (CFG). The scores are split over the batch dimension,\n",
      "     |  where the first half correspond to the conditional logits (predicted from the input prompt) and the second half\n",
      "     |  correspond to the unconditional logits (predicted from an empty or 'null' prompt). The processor computes a\n",
      "     |  weighted average across the conditional and unconditional logits, parameterised by the `guidance_scale`.\n",
      "     |  \n",
      "     |  See [the paper](https://arxiv.org/abs/2306.05284) for more information.\n",
      "     |  \n",
      "     |  <Tip warning={true}>\n",
      "     |  \n",
      "     |  This logits processor is exclusively compatible with\n",
      "     |  [MusicGen](https://huggingface.co/docs/transformers/main/en/model_doc/musicgen)\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      guidance_scale (float):\n",
      "     |          The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n",
      "     |          Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n",
      "     |          prompt, usually at the expense of poorer quality.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoProcessor, MusicgenForConditionalGeneration\n",
      "     |  \n",
      "     |  >>> processor = AutoProcessor.from_pretrained(\"facebook/musicgen-small\")\n",
      "     |  >>> model = MusicgenForConditionalGeneration.from_pretrained(\"facebook/musicgen-small\")\n",
      "     |  \n",
      "     |  >>> inputs = processor(\n",
      "     |  ...     text=[\"80s pop track with bassy drums and synth\", \"90s rock song with loud guitars and heavy drums\"],\n",
      "     |  ...     padding=True,\n",
      "     |  ...     return_tensors=\"pt\",\n",
      "     |  ... )\n",
      "     |  >>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ClassifierFreeGuidanceLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, guidance_scale)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ConstrainedBeamSearchScorer(BeamScorer)\n",
      "     |  ConstrainedBeamSearchScorer(batch_size: int, num_beams: int, constraints: List[transformers.generation.beam_constraints.Constraint], device: torch.device, length_penalty: Optional[float] = 1.0, do_early_stopping: Union[bool, str, NoneType] = False, num_beam_hyps_to_keep: Optional[int] = 1, num_beam_groups: Optional[int] = 1, max_length: Optional[int] = None)\n",
      "     |  \n",
      "     |  [`BeamScorer`] implementing constrained beam search decoding.\n",
      "     |  \n",
      "     |  \n",
      "     |  Args:\n",
      "     |      batch_size (`int`):\n",
      "     |          Batch Size of `input_ids` for which standard beam search decoding is run in parallel.\n",
      "     |      num_beams (`int`):\n",
      "     |          Number of beams for beam search.\n",
      "     |      constraints (`List[Constraint]`):\n",
      "     |          A list of positive constraints represented as `Constraint` objects that must be fulfilled in the generation\n",
      "     |          output. For more information, the documentation of [`Constraint`] should be read.\n",
      "     |      device (`torch.device`):\n",
      "     |          Defines the device type (*e.g.*, `\"cpu\"` or `\"cuda\"`) on which this instance of `BeamSearchScorer` will be\n",
      "     |          allocated.\n",
      "     |      length_penalty (`float`, *optional*, defaults to 1.0):\n",
      "     |          Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n",
      "     |          the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n",
      "     |          likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n",
      "     |          `length_penalty` < 0.0 encourages shorter sequences.\n",
      "     |      do_early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n",
      "     |          Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n",
      "     |          `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n",
      "     |          heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n",
      "     |          `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n",
      "     |          beam search algorithm).\n",
      "     |      num_beam_hyps_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          The number of beam hypotheses that shall be returned upon calling\n",
      "     |          [`~transformers.BeamSearchScorer.finalize`].\n",
      "     |      num_beam_groups (`int`, *optional*, defaults to 1):\n",
      "     |          Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n",
      "     |          See [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      "     |      max_length (`int`, *optional*):\n",
      "     |          The maximum length of the sequence to be generated.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ConstrainedBeamSearchScorer\n",
      "     |      BeamScorer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, batch_size: int, num_beams: int, constraints: List[transformers.generation.beam_constraints.Constraint], device: torch.device, length_penalty: Optional[float] = 1.0, do_early_stopping: Union[bool, str, NoneType] = False, num_beam_hyps_to_keep: Optional[int] = 1, num_beam_groups: Optional[int] = 1, max_length: Optional[int] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  check_completes_constraints(self, sequence)\n",
      "     |  \n",
      "     |  finalize(self, input_ids: torch.LongTensor, final_beam_scores: torch.FloatTensor, final_beam_tokens: torch.LongTensor, final_beam_indices: torch.LongTensor, max_length: int, pad_token_id: Union[int, torch.Tensor, NoneType] = None, eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None, beam_indices: Optional[torch.LongTensor] = None, decoder_prompt_len: Optional[int] = 0) -> Tuple[torch.LongTensor]\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          final_beam_scores (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The final scores of all non-finished beams.\n",
      "     |          final_beam_tokens (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The last tokens to be added to the non-finished beam_hypotheses.\n",
      "     |          final_beam_indices (`torch.FloatTensor` of shape `(batch_size * num_beams)`):\n",
      "     |              The beam indices indicating to which beam the `final_beam_tokens` shall be added.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(batch_size * num_return_sequences, sequence_length)`: The generated sequences.\n",
      "     |          The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished early\n",
      "     |          due to the `eos_token_id`.\n",
      "     |  \n",
      "     |  make_constraint_states(self, n)\n",
      "     |  \n",
      "     |  process(self, input_ids: torch.LongTensor, next_scores: torch.FloatTensor, next_tokens: torch.LongTensor, next_indices: torch.LongTensor, scores_for_all_vocab: torch.FloatTensor, pad_token_id: Union[int, torch.Tensor, NoneType] = None, eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None, beam_indices: Optional[torch.LongTensor] = None, decoder_prompt_len: Optional[int] = 0) -> Tuple[torch.Tensor]\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using any class inheriting from [`PreTrainedTokenizer`]. See\n",
      "     |              [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          next_scores (`torch.FloatTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Current scores of the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_tokens (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              `input_ids` of the tokens corresponding to the top `2 * num_beams` non-finished beam hypotheses.\n",
      "     |          next_indices (`torch.LongTensor` of shape `(batch_size, 2 * num_beams)`):\n",
      "     |              Beam indices indicating to which beam hypothesis the `next_tokens` correspond.\n",
      "     |          scores_for_all_vocab (`torch.FloatTensor` of shape `(batch_size * num_beams, sequence_length)`):\n",
      "     |              The scores of all tokens in the vocabulary for each of the beam hypotheses.\n",
      "     |          pad_token_id (`int`, *optional*):\n",
      "     |              The id of the *padding* token.\n",
      "     |          eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |              The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |          beam_indices (`torch.LongTensor`, *optional*):\n",
      "     |              Beam indices indicating to which beam hypothesis each token correspond.\n",
      "     |          decoder_prompt_len (`int`, *optional*):\n",
      "     |              The length of prompt that is included in the input to decoder.\n",
      "     |      Return:\n",
      "     |          `UserDict`: A dictionary composed of the fields as defined above:\n",
      "     |      \n",
      "     |              - **next_beam_scores** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Updated scores of\n",
      "     |                all\n",
      "     |              non-finished beams.\n",
      "     |      \n",
      "     |              - **next_beam_tokens** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Next tokens to be\n",
      "     |                added\n",
      "     |              to the non-finished beam_hypotheses.\n",
      "     |              - **next_beam_indices** (`torch.FloatTensor` of shape `(batch_size * num_beams)`) -- Beam indices\n",
      "     |              indicating to which beam the next tokens shall be added.\n",
      "     |  \n",
      "     |  step_sentence_constraint(self, batch_idx: int, input_ids: torch.LongTensor, vocab_scores: torch.FloatTensor, sent_beam_scores: torch.FloatTensor, sent_beam_tokens: torch.LongTensor, sent_beam_indices: torch.LongTensor, push_progress: bool = False)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  is_done\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BeamScorer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Constraint(abc.ABC)\n",
      "     |  Abstract base class for all constraints that can be applied during generation.\n",
      "     |  It must define how the constraint can be satisfied.\n",
      "     |  \n",
      "     |  All classes that inherit Constraint must follow the requirement that\n",
      "     |  \n",
      "     |  ```py\n",
      "     |  completed = False\n",
      "     |  while not completed:\n",
      "     |      _, completed = constraint.update(constraint.advance())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  will always terminate (halt).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Constraint\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  advance(self)\n",
      "     |      When called, returns the token that would take this constraint one step closer to being fulfilled.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          token_ids(`torch.tensor`): Must be a tensor of a list of indexable tokens, not some integer.\n",
      "     |  \n",
      "     |  copy(self, stateful=False)\n",
      "     |      Creates a new instance of this constraint.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          stateful(`bool`): Whether to not only copy the constraint for new instance, but also its state.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          constraint(`Constraint`): The same constraint as the one being called from.\n",
      "     |  \n",
      "     |  does_advance(self, token_id: int)\n",
      "     |      Reads in a token and returns whether it creates progress.\n",
      "     |  \n",
      "     |  remaining(self)\n",
      "     |      Returns the number of remaining steps of `advance()` in order to complete this constraint.\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Resets the state of this constraint to its initialization. We would call this in cases where the fulfillment of\n",
      "     |      a constraint is abrupted by an unwanted token.\n",
      "     |  \n",
      "     |  test(self)\n",
      "     |      Tests whether this constraint has been properly defined.\n",
      "     |  \n",
      "     |  update(self, token_id: int)\n",
      "     |      Reads in a token and returns booleans that indicate the progress made by it. This function will update the\n",
      "     |      state of this object unlikes `does_advance(self, token_id: int)`.\n",
      "     |      \n",
      "     |      This isn't to test whether a certain token will advance the progress; it's to update its state as if it has\n",
      "     |      been generated. This becomes important if token_id != desired token (refer to else statement in\n",
      "     |      PhrasalConstraint)\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_id(`int`):\n",
      "     |              The id of a newly generated token in the beam search.\n",
      "     |      Return:\n",
      "     |          stepped(`bool`):\n",
      "     |              Whether this constraint has become one step closer to being fulfuilled.\n",
      "     |          completed(`bool`):\n",
      "     |              Whether this constraint has been completely fulfilled by this token being generated.\n",
      "     |          reset (`bool`):\n",
      "     |              Whether this constraint has reset its progress by this token being generated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'advance', 'copy', 'does_advance', 'r...\n",
      "    \n",
      "    class ConstraintListState(builtins.object)\n",
      "     |  ConstraintListState(constraints: List[transformers.generation.beam_constraints.Constraint])\n",
      "     |  \n",
      "     |  A class for beam scorers to track its progress through a list of constraints.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      constraints (`List[Constraint]`):\n",
      "     |          A list of [`Constraint`] objects that must be fulfilled by the beam scorer.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, constraints: List[transformers.generation.beam_constraints.Constraint])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add(self, token_id: int)\n",
      "     |  \n",
      "     |  advance(self)\n",
      "     |      The list of tokens to generate such that we can make progress.\n",
      "     |      By \"list\" we don't mean the list of token that will fully fulfill a constraint.\n",
      "     |      \n",
      "     |      Given constraints `c_i = {t_ij | j == # of tokens}`, If we're not in the middle of progressing through a\n",
      "     |      specific constraint `c_i`, we return:\n",
      "     |      \n",
      "     |      `[t_k1 for k in indices of unfulfilled constraints]`\n",
      "     |      \n",
      "     |      If we are in the middle of a constraint, then we return:\n",
      "     |          `[t_ij]`, where `i` is the index of the inprogress constraint, `j` is the next step for the constraint.\n",
      "     |      \n",
      "     |      Though we don't care which constraint is fulfilled first, if we are in the progress of fulfilling a constraint,\n",
      "     |      that's the only one we'll return.\n",
      "     |  \n",
      "     |  copy(self, stateful=True)\n",
      "     |  \n",
      "     |  get_bank(self)\n",
      "     |  \n",
      "     |  init_state(self)\n",
      "     |  \n",
      "     |  reset(self, token_ids: Optional[List[int]])\n",
      "     |      token_ids: the tokens generated thus far to reset the state of the progress through constraints.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    ContrastiveSearchDecoderOnlyOutput = class GenerateDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  ContrastiveSearchDecoderOnlyOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'attentions', 'hidd...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    ContrastiveSearchEncoderDecoderOutput = class GenerateEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  ContrastiveSearchEncoderDecoderOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cross_attentions': typing.Optional[typing.Tuple[ty...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'cross_attentions': Field(name='cross_attentio...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'encoder_attentions...\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class DisjunctiveConstraint(Constraint)\n",
      "     |  DisjunctiveConstraint(nested_token_ids: List[List[int]])\n",
      "     |  \n",
      "     |  A special [`Constraint`] that is fulfilled by fulfilling just one of several constraints.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      nested_token_ids (`List[List[int]]`):\n",
      "     |          A list of words, where each word is a list of ids. This constraint is fulfilled by generating just one from\n",
      "     |          the list of words.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DisjunctiveConstraint\n",
      "     |      Constraint\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, nested_token_ids: List[List[int]])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  advance(self)\n",
      "     |      When called, returns the token that would take this constraint one step closer to being fulfilled.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          token_ids(`torch.tensor`): Must be a tensor of a list of indexable tokens, not some integer.\n",
      "     |  \n",
      "     |  copy(self, stateful=False)\n",
      "     |      Creates a new instance of this constraint.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          stateful(`bool`): Whether to not only copy the constraint for new instance, but also its state.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          constraint(`Constraint`): The same constraint as the one being called from.\n",
      "     |  \n",
      "     |  does_advance(self, token_id: int)\n",
      "     |      Reads in a token and returns whether it creates progress.\n",
      "     |  \n",
      "     |  remaining(self)\n",
      "     |      Returns the number of remaining steps of `advance()` in order to complete this constraint.\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Resets the state of this constraint to its initialization. We would call this in cases where the fulfillment of\n",
      "     |      a constraint is abrupted by an unwanted token.\n",
      "     |  \n",
      "     |  update(self, token_id: int)\n",
      "     |      Reads in a token and returns booleans that indicate the progress made by it. This function will update the\n",
      "     |      state of this object unlikes `does_advance(self, token_id: int)`.\n",
      "     |      \n",
      "     |      This isn't to test whether a certain token will advance the progress; it's to update its state as if it has\n",
      "     |      been generated. This becomes important if token_id != desired token (refer to else statement in\n",
      "     |      PhrasalConstraint)\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_id(`int`):\n",
      "     |              The id of a newly generated token in the beam search.\n",
      "     |      Return:\n",
      "     |          stepped(`bool`):\n",
      "     |              Whether this constraint has become one step closer to being fulfuilled.\n",
      "     |          completed(`bool`):\n",
      "     |              Whether this constraint has been completely fulfilled by this token being generated.\n",
      "     |          reset (`bool`):\n",
      "     |              Whether this constraint has reset its progress by this token being generated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Constraint:\n",
      "     |  \n",
      "     |  test(self)\n",
      "     |      Tests whether this constraint has been properly defined.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Constraint:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EncoderNoRepeatNGramLogitsProcessor(LogitsProcessor)\n",
      "     |  EncoderNoRepeatNGramLogitsProcessor(encoder_ngram_size: int, encoder_input_ids: torch.LongTensor)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that works similarly to [`NoRepeatNGramLogitsProcessor`], but applied exclusively to prevent\n",
      "     |  the repetition of n-grams present in the prompt.\n",
      "     |  \n",
      "     |  It was designed to promote chattiness in a language model, by preventing the generation of n-grams present in\n",
      "     |  previous conversation rounds.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      encoder_ngram_size (`int`):\n",
      "     |          All ngrams of size `ngram_size` can only occur within the encoder input ids.\n",
      "     |      encoder_input_ids (`int`):\n",
      "     |          The encoder_input_ids that should not be repeated within the decoder ids.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```py\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"Alice: I love cats. What do you love?\\nBob:\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With greedy decoding, we see Bob repeating Alice's opinion. If Bob was a chatbot, it would be a poor one.\n",
      "     |  >>> outputs = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  Alice: I love cats. What do you love?\n",
      "     |  Bob: I love cats. What do you\n",
      "     |  \n",
      "     |  >>> # With this logits processor, we can prevent Bob from repeating Alice's opinion.\n",
      "     |  >>> outputs = model.generate(**inputs, encoder_no_repeat_ngram_size=2)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  Alice: I love cats. What do you love?\n",
      "     |  Bob: My cats are very cute.\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EncoderNoRepeatNGramLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, encoder_ngram_size: int, encoder_input_ids: torch.LongTensor)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EncoderRepetitionPenaltyLogitsProcessor(LogitsProcessor)\n",
      "     |  EncoderRepetitionPenaltyLogitsProcessor(penalty: float, encoder_input_ids: torch.LongTensor)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that works similarly to [`RepetitionPenaltyLogitsProcessor`], but with an *inverse* penalty\n",
      "     |  that is applied to the tokens present in the prompt. In other words, a penalty above 1.0 increases the odds of\n",
      "     |  selecting tokens that were present in the prompt.\n",
      "     |  \n",
      "     |  It was designed to avoid hallucination in input-grounded tasks, like summarization. Although originally intended\n",
      "     |  for encoder-decoder models, it can also be used with decoder-only models like LLMs.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      penalty (`float`):\n",
      "     |          The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 rewards prompt tokens. Between 0.0\n",
      "     |          and 1.0 penalizes prompt tokens.\n",
      "     |      encoder_input_ids (`torch.LongTensor`):\n",
      "     |          The encoder_input_ids that should be repeated within the decoder ids.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "     |  \n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer([\"Alice and Bob. The third member's name was\"], return_tensors=\"pt\")\n",
      "     |  >>> gen_out = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  Alice and Bob. The third member's name was not mentioned.\n",
      "     |  \n",
      "     |  >>> # With the `encoder_repetition_penalty` argument we can trigger this logits processor in `generate`, which can\n",
      "     |  >>> # promote the use of prompt tokens (\"Bob\" in this example)\n",
      "     |  >>> gen_out = model.generate(**inputs, encoder_repetition_penalty=1.2)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  Alice and Bob. The third member's name was Bob. The third member's name was Bob.\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EncoderRepetitionPenaltyLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, penalty: float, encoder_input_ids: torch.LongTensor)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EosTokenCriteria(StoppingCriteria)\n",
      "     |  EosTokenCriteria(eos_token_id: Union[int, List[int], torch.Tensor])\n",
      "     |  \n",
      "     |  This class can be used to stop generation whenever the \"end-of-sequence\" token is generated.\n",
      "     |  By default, it uses the `model.generation_config.eos_token_id`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EosTokenCriteria\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  __init__(self, eos_token_id: Union[int, List[int], torch.Tensor])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from StoppingCriteria:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EpsilonLogitsWarper(LogitsWarper)\n",
      "     |  EpsilonLogitsWarper(epsilon: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs epsilon-sampling, i.e. restricting to tokens with `prob >= epsilon`. Takes the\n",
      "     |  largest min_tokens_to_keep tokens if no tokens satisfy this constraint. See [Truncation Sampling as Language Model\n",
      "     |  Desmoothing](https://arxiv.org/abs/2210.15191) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      epsilon (`float`):\n",
      "     |          If set to > 0, only the most tokens with probabilities `epsilon` or higher are kept for generation.\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All filtered values will be set to this float value.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Minimum number of tokens that cannot be filtered.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With sampling, the output is unexpected -- sometimes too unexpected.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3 | < 4 (left-hand pointer) ;\n",
      "     |  <BLANKLINE>\n",
      "     |  <BLANKLINE>\n",
      "     |  \n",
      "     |  >>> # With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to\n",
      "     |  >>> # Top P sampling, which restricts tokens based on their cumulative probability.\n",
      "     |  >>> # Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EpsilonLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, epsilon: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class EtaLogitsWarper(LogitsWarper)\n",
      "     |  EtaLogitsWarper(epsilon: float, filter_value: float = -inf, min_tokens_to_keep: int = 1, device: str = 'cpu')\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs eta-sampling, a technique to filter out tokens with probabilities below a dynamic\n",
      "     |  cutoff value, `eta`, which is calculated based on a combination of the hyperparameter `epsilon` and the entropy of\n",
      "     |  the token probabilities, i.e. `eta := min(epsilon, sqrt(epsilon * e^-entropy(probabilities)))`. Takes the largest\n",
      "     |  min_tokens_to_keep tokens if no tokens satisfy this constraint. It addresses the issue of poor quality in long\n",
      "     |  samples of text generated by neural language models leading to more coherent and fluent text. See [Truncation\n",
      "     |  Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more information. Note: `do_sample`\n",
      "     |  must be set to `True` for this `LogitsWarper` to work.\n",
      "     |  \n",
      "     |  \n",
      "     |  Args:\n",
      "     |      epsilon (`float`):\n",
      "     |          A float value in the range (0, 1). Hyperparameter used to calculate the dynamic cutoff value, `eta`. The\n",
      "     |          suggested values from the paper ranges from 3e-4 to 4e-3 depending on the size of the model.\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All values that are found to be below the dynamic cutoff value, `eta`, are set to this float value. This\n",
      "     |          parameter is useful when logits need to be modified for very low probability tokens that should be excluded\n",
      "     |          from generation entirely.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Specifies the minimum number of tokens that must be kept for generation, regardless of their probabilities.\n",
      "     |          For example, if `min_tokens_to_keep` is set to 1, at least one token will always be kept for generation,\n",
      "     |          even if all tokens have probabilities below the cutoff `eta`.\n",
      "     |      device (`str`, *optional*, defaults to `\"cpu\"`):\n",
      "     |          The device to allocate the tensors.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With sampling, the output is unexpected -- sometimes too unexpected.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3 | < 4 (left-hand pointer) ;\n",
      "     |  <BLANKLINE>\n",
      "     |  <BLANKLINE>\n",
      "     |  \n",
      "     |  >>> # With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of\n",
      "     |  >>> # epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).\n",
      "     |  >>> # Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EtaLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, epsilon: float, filter_value: float = -inf, min_tokens_to_keep: int = 1, device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ExponentialDecayLengthPenalty(LogitsProcessor)\n",
      "     |  ExponentialDecayLengthPenalty(exponential_decay_length_penalty: Tuple[int, float], eos_token_id: Union[int, List[int], torch.Tensor], input_ids_seq_length: int)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that exponentially increases the score of the `eos_token_id` after `start_index` has been\n",
      "     |  reached. This allows generating shorter sequences without having a hard cutoff, allowing the `eos_token` to be\n",
      "     |  predicted in a meaningful position.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      exponential_decay_length_penalty (`tuple(int, float)`):\n",
      "     |          This tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where penalty\n",
      "     |          starts and `decay_factor` represents the factor of exponential decay\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |      input_ids_seq_length (`int`):\n",
      "     |          The length of the input sequence.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  \n",
      "     |  >>> text = \"Just wanted to let you know, I\"\n",
      "     |  >>> inputs = tokenizer(text, return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # Let's consider that we want short sentences, so we limit `max_length=30`. However, we observe that the answer\n",
      "     |  >>> # tends to end abruptly.\n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=30, pad_token_id=50256)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was\n",
      "     |  published in 2010. Although\n",
      "     |  \n",
      "     |  >>> # To promote the appearance of the EOS token at the right time, we add the `exponential_decay_length_penalty =\n",
      "     |  >>> # (start_index, decay_factor)`. Instead of cutting at max_tokens, the output comes to an end before and usually\n",
      "     |  >>> # with more meaning. What happens is that starting from `start_index` the EOS token score will be increased\n",
      "     |  >>> # by `decay_factor` exponentially. However, if you set a high decay factor, you may also end up with abruptly\n",
      "     |  >>> # ending sequences.\n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> outputs = model.generate(\n",
      "     |  ...     **inputs,\n",
      "     |  ...     do_sample=True,\n",
      "     |  ...     temperature=0.9,\n",
      "     |  ...     max_length=30,\n",
      "     |  ...     pad_token_id=50256,\n",
      "     |  ...     exponential_decay_length_penalty=(15, 1.6),\n",
      "     |  ... )\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network\n",
      "     |  which<|endoftext|>\n",
      "     |  \n",
      "     |  >>> # With a small decay factor, you will have a higher chance of getting a meaningful sequence.\n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> outputs = model.generate(\n",
      "     |  ...     **inputs,\n",
      "     |  ...     do_sample=True,\n",
      "     |  ...     temperature=0.9,\n",
      "     |  ...     max_length=30,\n",
      "     |  ...     pad_token_id=50256,\n",
      "     |  ...     exponential_decay_length_penalty=(15, 1.01),\n",
      "     |  ... )\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  Just wanted to let you know, I received a link to an ebook, the book How To Start A Social Network which was\n",
      "     |  published in 2010.<|endoftext|>\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ExponentialDecayLengthPenalty\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, exponential_decay_length_penalty: Tuple[int, float], eos_token_id: Union[int, List[int], torch.Tensor], input_ids_seq_length: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ForceTokensLogitsProcessor(LogitsProcessor)\n",
      "     |  ForceTokensLogitsProcessor(force_token_map: List[List[int]], _has_warned: Optional[bool] = False)\n",
      "     |  \n",
      "     |  This processor takes a list of pairs of integers which indicates a mapping from generation indices to token\n",
      "     |  indices that will be forced before generation. The processor will set their log probs to `inf` so that they are\n",
      "     |  sampled at their corresponding index. Originally created for\n",
      "     |  [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ForceTokensLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, force_token_map: List[List[int]], _has_warned: Optional[bool] = False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ForcedBOSTokenLogitsProcessor(LogitsProcessor)\n",
      "     |  ForcedBOSTokenLogitsProcessor(bos_token_id: int)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that enforces the specified token as the first generated token. Used with encoder-decoder\n",
      "     |  models.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      bos_token_id (`int`):\n",
      "     |          The id of the token to force as the first generated token.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"Translate from English to German: I love cats.\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # By default, it continues generating according to the model's logits\n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=10)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  <pad> Ich liebe Kitty.</s>\n",
      "     |  \n",
      "     |  >>> # We can use `forced_bos_token_id` to force the start of generation with an encoder-decoder model\n",
      "     |  >>> # (including forcing it to end straight away with an EOS token)\n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=10, forced_bos_token_id=tokenizer.eos_token_id)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  <pad></s>\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ForcedBOSTokenLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, bos_token_id: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ForcedEOSTokenLogitsProcessor(LogitsProcessor)\n",
      "     |  ForcedEOSTokenLogitsProcessor(max_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that enforces the specified token as the last generated token when `max_length` is reached.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      max_length (`int`):\n",
      "     |          The maximum length of the sequence to be generated.\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |      device (`str`, *optional*, defaults to `\"cpu\"`):\n",
      "     |          The device to allocate the tensors.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2, 3\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # By default, it continues generating according to the model's logits\n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=10)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7, 8\n",
      "     |  \n",
      "     |  >>> # `forced_eos_token_id` ensures the generation ends with a EOS token\n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=10, forced_eos_token_id=tokenizer.eos_token_id)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7,<|endoftext|>\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ForcedEOSTokenLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, max_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GenerateBeamDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GenerateBeamDecoderOnlyOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class GenerateBeamEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GenerateBeamEncoderDecoderOutput(sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Final beam scores of the generated `sequences`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |          of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |          Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |          with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |          `(batch_size*num_return_sequences, sequence_length)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size*num_beams*num_return_sequences, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, num_heads, generated_length,\n",
      "     |          sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateBeamEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, sequences_scores: Optional[torch.FloatTensor] = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, beam_indices: Optional[torch.LongTensor] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'beam_indices': typing.Optional[torch.LongTensor], ...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'beam_indices': Field(name='beam_indices',type...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'sequences_scores', 'scores', 'logits',...\n",
      "     |  \n",
      "     |  beam_indices = None\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  sequences_scores = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class GenerateDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GenerateDecoderOnlyOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'attentions', 'hidd...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class GenerateEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GenerateEncoderDecoderOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cross_attentions': typing.Optional[typing.Tuple[ty...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'cross_attentions': Field(name='cross_attentio...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'encoder_attentions...\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class GenerationConfig(transformers.utils.hub.PushToHubMixin)\n",
      "     |  GenerationConfig(**kwargs)\n",
      "     |  \n",
      "     |  Class that holds a configuration for a generation task. A `generate` call supports the following generation methods\n",
      "     |  for text-decoder, text-to-text, speech-to-text, and vision-to-text models:\n",
      "     |  \n",
      "     |      - *greedy decoding* if `num_beams=1` and `do_sample=False`\n",
      "     |      - *contrastive search* if `penalty_alpha>0.` and `top_k>1`\n",
      "     |      - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n",
      "     |      - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n",
      "     |      - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n",
      "     |      - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n",
      "     |      - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n",
      "     |      - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n",
      "     |  \n",
      "     |  To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n",
      "     |  \n",
      "     |  <Tip>\n",
      "     |  \n",
      "     |  A large number of these flags control the logits or the stopping criteria of the generation. Make sure you check\n",
      "     |  the [generate-related classes](https://huggingface.co/docs/transformers/internal/generation_utils) for a full\n",
      "     |  description of the possible manipulations, as well as examples of their usage.\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Arg:\n",
      "     |      > Parameters that control the length of the output\n",
      "     |  \n",
      "     |      max_length (`int`, *optional*, defaults to 20):\n",
      "     |          The maximum length the generated tokens can have. Corresponds to the length of the input prompt +\n",
      "     |          `max_new_tokens`. Its effect is overridden by `max_new_tokens`, if also set.\n",
      "     |      max_new_tokens (`int`, *optional*):\n",
      "     |          The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
      "     |      min_length (`int`, *optional*, defaults to 0):\n",
      "     |          The minimum length of the sequence to be generated. Corresponds to the length of the input prompt +\n",
      "     |          `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set.\n",
      "     |      min_new_tokens (`int`, *optional*):\n",
      "     |          The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n",
      "     |      early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n",
      "     |          Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n",
      "     |          `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n",
      "     |          heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n",
      "     |          `\"never\"`, where the beam search procedure only stops when there cannot be better candidates (canonical\n",
      "     |          beam search algorithm).\n",
      "     |      max_time(`float`, *optional*):\n",
      "     |          The maximum amount of time you allow the computation to run for in seconds. generation will still finish\n",
      "     |          the current pass after allocated time has been passed.\n",
      "     |      stop_strings(`str or List[str]`, *optional*):\n",
      "     |          A string or a list of strings that should terminate generation if the model outputs them.\n",
      "     |  \n",
      "     |      > Parameters that control the generation strategy used\n",
      "     |  \n",
      "     |      do_sample (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether or not to use sampling ; use greedy decoding otherwise.\n",
      "     |      num_beams (`int`, *optional*, defaults to 1):\n",
      "     |          Number of beams for beam search. 1 means no beam search.\n",
      "     |      num_beam_groups (`int`, *optional*, defaults to 1):\n",
      "     |          Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n",
      "     |          [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      "     |      penalty_alpha (`float`, *optional*):\n",
      "     |          The values balance the model confidence and the degeneration penalty in contrastive search decoding.\n",
      "     |      use_cache (`bool`, *optional*, defaults to `True`):\n",
      "     |          Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n",
      "     |          speed up decoding.\n",
      "     |  \n",
      "     |      > Parameters for manipulation of the model output logits\n",
      "     |  \n",
      "     |      temperature (`float`, *optional*, defaults to 1.0):\n",
      "     |          The value used to modulate the next token probabilities.\n",
      "     |      top_k (`int`, *optional*, defaults to 50):\n",
      "     |          The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "     |      top_p (`float`, *optional*, defaults to 1.0):\n",
      "     |          If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n",
      "     |          `top_p` or higher are kept for generation.\n",
      "     |      min_p (`float`, *optional*):\n",
      "     |          Minimum token probability, which will be scaled by the probability of the most likely token. It must be a\n",
      "     |          value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in\n",
      "     |          the 0.99-0.8 range (use the opposite of normal `top_p` values).\n",
      "     |      typical_p (`float`, *optional*, defaults to 1.0):\n",
      "     |          Local typicality measures how similar the conditional probability of predicting a target token next is to\n",
      "     |          the expected conditional probability of predicting a random token next, given the partial text already\n",
      "     |          generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that\n",
      "     |          add up to `typical_p` or higher are kept for generation. See [this\n",
      "     |          paper](https://arxiv.org/pdf/2202.00666.pdf) for more details.\n",
      "     |      epsilon_cutoff (`float`, *optional*, defaults to 0.0):\n",
      "     |          If set to float strictly between 0 and 1, only tokens with a conditional probability greater than\n",
      "     |          `epsilon_cutoff` will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on the\n",
      "     |          size of the model. See [Truncation Sampling as Language Model\n",
      "     |          Desmoothing](https://arxiv.org/abs/2210.15191) for more details.\n",
      "     |      eta_cutoff (`float`, *optional*, defaults to 0.0):\n",
      "     |          Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly between\n",
      "     |          0 and 1, a token is only considered if it is greater than either `eta_cutoff` or `sqrt(eta_cutoff) *\n",
      "     |          exp(-entropy(softmax(next_token_logits)))`. The latter term is intuitively the expected next token\n",
      "     |          probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3,\n",
      "     |          depending on the size of the model. See [Truncation Sampling as Language Model\n",
      "     |          Desmoothing](https://arxiv.org/abs/2210.15191) for more details.\n",
      "     |      diversity_penalty (`float`, *optional*, defaults to 0.0):\n",
      "     |          This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n",
      "     |          particular time. Note that `diversity_penalty` is only effective if `group beam search` is enabled.\n",
      "     |      repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
      "     |          The parameter for repetition penalty. 1.0 means no penalty. See [this\n",
      "     |          paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n",
      "     |      encoder_repetition_penalty (`float`, *optional*, defaults to 1.0):\n",
      "     |          The paramater for encoder_repetition_penalty. An exponential penalty on sequences that are not in the\n",
      "     |          original input. 1.0 means no penalty.\n",
      "     |      length_penalty (`float`, *optional*, defaults to 1.0):\n",
      "     |          Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n",
      "     |          the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n",
      "     |          likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n",
      "     |          `length_penalty` < 0.0 encourages shorter sequences.\n",
      "     |      no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
      "     |          If set to int > 0, all ngrams of that size can only occur once.\n",
      "     |      bad_words_ids(`List[List[int]]`, *optional*):\n",
      "     |          List of list of token ids that are not allowed to be generated. Check\n",
      "     |          [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\n",
      "     |      force_words_ids(`List[List[int]]` or `List[List[List[int]]]`, *optional*):\n",
      "     |          List of token ids that must be generated. If given a `List[List[int]]`, this is treated as a simple list of\n",
      "     |          words that must be included, the opposite to `bad_words_ids`. If given `List[List[List[int]]]`, this\n",
      "     |          triggers a [disjunctive constraint](https://github.com/huggingface/transformers/issues/14081), where one\n",
      "     |          can allow different forms of each word.\n",
      "     |      renormalize_logits (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether to renormalize the logits after applying all the logits processors or warpers (including the custom\n",
      "     |          ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the score logits\n",
      "     |          are normalized but some logit processors or warpers break the normalization.\n",
      "     |      constraints (`List[Constraint]`, *optional*):\n",
      "     |          Custom constraints that can be added to the generation to ensure that the output will contain the use of\n",
      "     |          certain tokens as defined by `Constraint` objects, in the most sensible way possible.\n",
      "     |      forced_bos_token_id (`int`, *optional*, defaults to `model.config.forced_bos_token_id`):\n",
      "     |          The id of the token to force as the first generated token after the `decoder_start_token_id`. Useful for\n",
      "     |          multilingual models like [mBART](../model_doc/mbart) where the first generated token needs to be the target\n",
      "     |          language token.\n",
      "     |      forced_eos_token_id (`Union[int, List[int]]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n",
      "     |          The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a\n",
      "     |          list to set multiple *end-of-sequence* tokens.\n",
      "     |      remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n",
      "     |          Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash.\n",
      "     |          Note that using `remove_invalid_values` can slow down generation.\n",
      "     |      exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n",
      "     |          This Tuple adds an exponentially increasing length penalty, after a certain amount of tokens have been\n",
      "     |          generated. The tuple shall consist of: `(start_index, decay_factor)` where `start_index` indicates where\n",
      "     |          penalty starts and `decay_factor` represents the factor of exponential decay\n",
      "     |      suppress_tokens  (`List[int]`, *optional*):\n",
      "     |          A list of tokens that will be suppressed at generation. The `SupressTokens` logit processor will set their\n",
      "     |          log probs to `-inf` so that they are not sampled.\n",
      "     |      begin_suppress_tokens  (`List[int]`, *optional*):\n",
      "     |          A list of tokens that will be suppressed at the beginning of the generation. The `SupressBeginTokens` logit\n",
      "     |          processor will set their log probs to `-inf` so that they are not sampled.\n",
      "     |      forced_decoder_ids (`List[List[int]]`, *optional*):\n",
      "     |          A list of pairs of integers which indicates a mapping from generation indices to token indices that will be\n",
      "     |          forced before sampling. For example, `[[1, 123]]` means the second generated token will always be a token\n",
      "     |          of index 123.\n",
      "     |      sequence_bias (`Dict[Tuple[int], float]`, *optional*)):\n",
      "     |          Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n",
      "     |          sequence being selected, while negative biases do the opposite. Check\n",
      "     |          [`~generation.SequenceBiasLogitsProcessor`] for further documentation and examples.\n",
      "     |      token_healing (`bool`, *optional*, defaults to `False`):\n",
      "     |          Heal tail tokens of prompts by replacing them with their appropriate extensions.\n",
      "     |          This enhances the quality of completions for prompts affected by greedy tokenization bias.\n",
      "     |      guidance_scale (`float`, *optional*):\n",
      "     |          The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale > 1`.\n",
      "     |          Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n",
      "     |          prompt, usually at the expense of poorer quality.\n",
      "     |      low_memory (`bool`, *optional*):\n",
      "     |          Switch to sequential beam search and sequential topk for contrastive search to reduce peak memory.\n",
      "     |          Used with beam search and contrastive search.\n",
      "     |      watermarking_config (Union[`WatermarkingConfig`, `dict`], *optional*):\n",
      "     |          Arguments used to watermark the model outputs by adding a small bias to randomly selected set of \"green\" tokens.\n",
      "     |          If passed as `Dict`, it will be converted to a `WatermarkingConfig` internally.\n",
      "     |          See [this paper](https://arxiv.org/abs/2306.04634) for more details. Accepts the following keys:\n",
      "     |          - greenlist_ratio (`float`):\n",
      "     |              Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n",
      "     |          - bias (`float`):\n",
      "     |              Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n",
      "     |          - hashing_key (`int`):\n",
      "     |              Hahsing key used for watermarking. Defaults to 15485863 (the millionth prime).\n",
      "     |          - seeding_scheme (`str`):\n",
      "     |              Algorithm to use for watermarking. Accepts values:\n",
      "     |                  - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n",
      "     |                  - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n",
      "     |                      The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n",
      "     |          - context_width(`int`):\n",
      "     |              The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n",
      "     |  \n",
      "     |      > Parameters that define the output variables of generate\n",
      "     |  \n",
      "     |      num_return_sequences(`int`, *optional*, defaults to 1):\n",
      "     |          The number of independently computed returned sequences for each element in the batch.\n",
      "     |      output_attentions (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n",
      "     |          tensors for more details.\n",
      "     |      output_hidden_states (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n",
      "     |          more details.\n",
      "     |      output_scores (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n",
      "     |      output_logits (`bool`, *optional*):\n",
      "     |          Whether or not to return the unprocessed prediction logit scores. See `logits` under returned tensors for\n",
      "     |          more details.\n",
      "     |      return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n",
      "     |  \n",
      "     |      > Special tokens that can be used at generation time\n",
      "     |  \n",
      "     |      pad_token_id (`int`, *optional*):\n",
      "     |          The id of the *padding* token.\n",
      "     |      bos_token_id (`int`, *optional*):\n",
      "     |          The id of the *beginning-of-sequence* token.\n",
      "     |      eos_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |          The id of the *end-of-sequence* token. Optionally, use a list to set multiple *end-of-sequence* tokens.\n",
      "     |  \n",
      "     |      > Generation parameters exclusive to encoder-decoder models\n",
      "     |  \n",
      "     |      encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n",
      "     |          If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n",
      "     |          `decoder_input_ids`.\n",
      "     |      decoder_start_token_id (`Union[int, List[int]]`, *optional*):\n",
      "     |          If an encoder-decoder model starts decoding with a different token than *bos*, the id of that token or a list of length\n",
      "     |          `batch_size`. Indicating a list enables different start ids for each element in the batch\n",
      "     |          (e.g. multilingual models with different target languages in one batch)\n",
      "     |  \n",
      "     |      > Generation parameters exclusive to assistant generation\n",
      "     |  \n",
      "     |      num_assistant_tokens (`int`, *optional*, defaults to 5):\n",
      "     |          Defines the number of _speculative tokens_ that shall be generated by the assistant model before being\n",
      "     |          checked by the target model at each iteration. Higher values for `num_assistant_tokens` make the generation\n",
      "     |          more _speculative_ : If the assistant model is performant larger speed-ups can be reached, if the assistant\n",
      "     |          model requires lots of corrections, lower speed-ups are reached.\n",
      "     |      num_assistant_tokens_schedule (`str`, *optional*, defaults to `\"heuristic\"`):\n",
      "     |          Defines the schedule at which max assistant tokens shall be changed during inference.\n",
      "     |          - `\"heuristic\"`: When all speculative tokens are correct, increase `num_assistant_tokens` by 2 else\n",
      "     |            reduce by 1. `num_assistant_tokens` value is persistent over multiple generation calls with the same assistant model.\n",
      "     |          - `\"heuristic_transient\"`: Same as `\"heuristic\"` but `num_assistant_tokens` is reset to its initial value after each generation call.\n",
      "     |          - `\"constant\"`: `num_assistant_tokens` stays unchanged during generation\n",
      "     |      prompt_lookup_num_tokens (`int`, *optional*, default to `None`):\n",
      "     |          The number of tokens to be output as candidate tokens.\n",
      "     |      max_matching_ngram_size (`int`, *optional*, default to `None`):\n",
      "     |          The maximum ngram size to be considered for matching in the prompt. Default to 2 if not provided.\n",
      "     |  \n",
      "     |      > Parameters specific to the caching mechanism:\n",
      "     |  \n",
      "     |      cache_implementation (`str`, *optional*, default to `None`):\n",
      "     |          Cache class that should be used when generating.\n",
      "     |      cache_config (`Union[CacheConfig, dict]`, *optional*, default to `None`):\n",
      "     |          Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n",
      "     |          it will be converted to its repsective `CacheConfig` internally.\n",
      "     |          Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n",
      "     |      return_legacy_cache (`bool`, *optional*, default to `True`):\n",
      "     |          Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n",
      "     |  \n",
      "     |      > Wild card\n",
      "     |  \n",
      "     |      generation_kwargs:\n",
      "     |          Additional generation kwargs will be forwarded to the `generate` function of the model. Kwargs that are not\n",
      "     |          present in `generate`'s signature will be used in the model forward pass.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerationConfig\n",
      "     |      transformers.utils.hub.PushToHubMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  dict_torch_dtype_to_str(self, d: Dict[str, Any]) -> None\n",
      "     |      Checks whether the passed dictionary and its nested dicts have a *torch_dtype* key and if it's not None,\n",
      "     |      converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n",
      "     |      string, which can then be stored in the json format.\n",
      "     |  \n",
      "     |  get_generation_mode(self, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None) -> transformers.generation.configuration_utils.GenerationMode\n",
      "     |      Returns the generation mode triggered by the [`GenerationConfig`] instance.\n",
      "     |      \n",
      "     |      Arg:\n",
      "     |          assistant_model (`PreTrainedModel`, *optional*):\n",
      "     |              The assistant model to be used for assisted generation. If set, the generation mode will be\n",
      "     |              assisted generation.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          `GenerationMode`: The generation mode triggered by the instance.\n",
      "     |  \n",
      "     |  save_pretrained(self, save_directory: Union[str, os.PathLike], config_file_name: Union[str, os.PathLike, NoneType] = None, push_to_hub: bool = False, **kwargs)\n",
      "     |      Save a generation configuration object to the directory `save_directory`, so that it can be re-loaded using the\n",
      "     |      [`~GenerationConfig.from_pretrained`] class method.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          save_directory (`str` or `os.PathLike`):\n",
      "     |              Directory where the configuration JSON file will be saved (will be created if it does not exist).\n",
      "     |          config_file_name (`str` or `os.PathLike`, *optional*, defaults to `\"generation_config.json\"`):\n",
      "     |              Name of the generation configuration JSON file to be saved in `save_directory`.\n",
      "     |          push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n",
      "     |              repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n",
      "     |              namespace).\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n",
      "     |  \n",
      "     |  to_dict(self) -> Dict[str, Any]\n",
      "     |      Serializes this instance to a Python dictionary.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n",
      "     |  \n",
      "     |  to_diff_dict(self) -> Dict[str, Any]\n",
      "     |      Removes all attributes from config which correspond to the default config attributes for better readability and\n",
      "     |      serializes to a Python dictionary.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n",
      "     |  \n",
      "     |  to_json_file(self, json_file_path: Union[str, os.PathLike], use_diff: bool = True)\n",
      "     |      Save this instance to a JSON file.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          json_file_path (`str` or `os.PathLike`):\n",
      "     |              Path to the JSON file in which this configuration instance's parameters will be saved.\n",
      "     |          use_diff (`bool`, *optional*, defaults to `True`):\n",
      "     |              If set to `True`, only the difference between the config instance and the default `GenerationConfig()`\n",
      "     |              is serialized to JSON file.\n",
      "     |  \n",
      "     |  to_json_string(self, use_diff: bool = True, ignore_metadata: bool = False) -> str\n",
      "     |      Serializes this instance to a JSON string.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          use_diff (`bool`, *optional*, defaults to `True`):\n",
      "     |              If set to `True`, only the difference between the config instance and the default `GenerationConfig()`\n",
      "     |              is serialized to JSON string.\n",
      "     |          ignore_metadata (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether to ignore the metadata fields present in the instance\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          `str`: String containing all the attributes that make up this configuration instance in JSON format.\n",
      "     |  \n",
      "     |  update(self, **kwargs)\n",
      "     |      Updates attributes of this class instance with attributes from `kwargs` if they match existing attributes,\n",
      "     |      returning all the unused kwargs.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (`Dict[str, Any]`):\n",
      "     |              Dictionary of attributes to tentatively update this class.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          `Dict[str, Any]`: Dictionary containing all the key-value pairs that were not used to update the instance.\n",
      "     |  \n",
      "     |  validate(self, is_init=False)\n",
      "     |      Validates the values of the attributes of the [`GenerationConfig`] instance. Raises exceptions in the presence\n",
      "     |      of parameterization that can be detected as incorrect from the configuration instance alone.\n",
      "     |      \n",
      "     |      Note that some parameters not validated here are best validated at generate runtime, as they may depend on\n",
      "     |      other inputs and/or the model, such as parameters related to the generation length.\n",
      "     |      \n",
      "     |      Arg:\n",
      "     |          is_init (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether the validation is performed during the initialization of the instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_dict(config_dict: Dict[str, Any], **kwargs) -> 'GenerationConfig' from builtins.type\n",
      "     |      Instantiates a [`GenerationConfig`] from a Python dictionary of parameters.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config_dict (`Dict[str, Any]`):\n",
      "     |              Dictionary that will be used to instantiate the configuration object.\n",
      "     |          kwargs (`Dict[str, Any]`):\n",
      "     |              Additional parameters from which to initialize the configuration object.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          [`GenerationConfig`]: The configuration object instantiated from those parameters.\n",
      "     |  \n",
      "     |  from_model_config(model_config: transformers.configuration_utils.PretrainedConfig) -> 'GenerationConfig' from builtins.type\n",
      "     |      Instantiates a [`GenerationConfig`] from a [`PretrainedConfig`]. This function is useful to convert legacy\n",
      "     |      [`PretrainedConfig`] objects, which may contain generation parameters, into a stand-alone [`GenerationConfig`].\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          model_config (`PretrainedConfig`):\n",
      "     |              The model config that will be used to instantiate the generation config.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          [`GenerationConfig`]: The configuration object instantiated from those parameters.\n",
      "     |  \n",
      "     |  from_pretrained(pretrained_model_name: Union[str, os.PathLike], config_file_name: Union[str, os.PathLike, NoneType] = None, cache_dir: Union[str, os.PathLike, NoneType] = None, force_download: bool = False, local_files_only: bool = False, token: Union[bool, str, NoneType] = None, revision: str = 'main', **kwargs) -> 'GenerationConfig' from builtins.type\n",
      "     |      Instantiate a [`GenerationConfig`] from a generation configuration file.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          pretrained_model_name (`str` or `os.PathLike`):\n",
      "     |              This can be either:\n",
      "     |      \n",
      "     |              - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n",
      "     |                huggingface.co.\n",
      "     |              - a path to a *directory* containing a configuration file saved using the\n",
      "     |                [`~GenerationConfig.save_pretrained`] method, e.g., `./my_model_directory/`.\n",
      "     |          config_file_name (`str` or `os.PathLike`, *optional*, defaults to `\"generation_config.json\"`):\n",
      "     |              Name of the generation configuration JSON file to be loaded from `pretrained_model_name`.\n",
      "     |          cache_dir (`str` or `os.PathLike`, *optional*):\n",
      "     |              Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      "     |              standard cache should not be used.\n",
      "     |          force_download (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether or not to force to (re-)download the configuration files and override the cached versions if\n",
      "     |              they exist.\n",
      "     |          resume_download:\n",
      "     |              Deprecated and ignored. All downloads are now resumed by default when possible.\n",
      "     |              Will be removed in v5 of Transformers.\n",
      "     |          proxies (`Dict[str, str]`, *optional*):\n",
      "     |              A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "     |              'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n",
      "     |          token (`str` or `bool`, *optional*):\n",
      "     |              The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n",
      "     |              the token generated when running `huggingface-cli login` (stored in `~/.huggingface`).\n",
      "     |          revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "     |              The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "     |              git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "     |              identifier allowed by git.\n",
      "     |      \n",
      "     |              <Tip>\n",
      "     |      \n",
      "     |              To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\".\n",
      "     |      \n",
      "     |              </Tip>\n",
      "     |      \n",
      "     |          return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n",
      "     |              If `False`, then this function returns just the final configuration object.\n",
      "     |      \n",
      "     |              If `True`, then this functions returns a `Tuple(config, unused_kwargs)` where *unused_kwargs* is a\n",
      "     |              dictionary consisting of the key/value pairs whose keys are not configuration attributes: i.e., the\n",
      "     |              part of `kwargs` which has not been used to update `config` and is otherwise ignored.\n",
      "     |          subfolder (`str`, *optional*, defaults to `\"\"`):\n",
      "     |              In case the relevant files are located inside a subfolder of the model repo on huggingface.co, you can\n",
      "     |              specify the folder name here.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              The values in kwargs of any keys which are configuration attributes will be used to override the loaded\n",
      "     |              values. Behavior concerning key/value pairs whose keys are *not* configuration attributes is controlled\n",
      "     |              by the `return_unused_kwargs` keyword parameter.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          [`GenerationConfig`]: The configuration object instantiated from this pretrained model.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      >>> from transformers import GenerationConfig\n",
      "     |      \n",
      "     |      >>> # Download configuration from huggingface.co and cache.\n",
      "     |      >>> generation_config = GenerationConfig.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      \n",
      "     |      >>> # E.g. config was saved using *save_pretrained('./test/saved_model/')*\n",
      "     |      >>> generation_config.save_pretrained(\"./test/saved_model/\")\n",
      "     |      >>> generation_config = GenerationConfig.from_pretrained(\"./test/saved_model/\")\n",
      "     |      \n",
      "     |      >>> # You can also specify configuration names to your generation configuration file\n",
      "     |      >>> generation_config.save_pretrained(\"./test/saved_model/\", config_file_name=\"my_configuration.json\")\n",
      "     |      >>> generation_config = GenerationConfig.from_pretrained(\"./test/saved_model/\", \"my_configuration.json\")\n",
      "     |      \n",
      "     |      >>> # If you'd like to try a minor variation to an existing configuration, you can also pass generation\n",
      "     |      >>> # arguments to `.from_pretrained()`. Be mindful that typos and unused arguments will be ignored\n",
      "     |      >>> generation_config, unused_kwargs = GenerationConfig.from_pretrained(\n",
      "     |      ...     \"openai-community/gpt2\", top_k=1, foo=False, do_sample=True, return_unused_kwargs=True\n",
      "     |      ... )\n",
      "     |      >>> generation_config.top_k\n",
      "     |      1\n",
      "     |      \n",
      "     |      >>> unused_kwargs\n",
      "     |      {'foo': False}\n",
      "     |      ```\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.hub.PushToHubMixin:\n",
      "     |  \n",
      "     |  push_to_hub(self, repo_id: str, use_temp_dir: Optional[bool] = None, commit_message: Optional[str] = None, private: Optional[bool] = None, token: Union[bool, str, NoneType] = None, max_shard_size: Union[int, str, NoneType] = '5GB', create_pr: bool = False, safe_serialization: bool = True, revision: str = None, commit_description: str = None, tags: Optional[List[str]] = None, **deprecated_kwargs) -> str\n",
      "     |      Upload the {object_files} to the 🤗 Model Hub.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          repo_id (`str`):\n",
      "     |              The name of the repository you want to push your {object} to. It should contain your organization name\n",
      "     |              when pushing to a given organization.\n",
      "     |          use_temp_dir (`bool`, *optional*):\n",
      "     |              Whether or not to use a temporary directory to store the files saved before they are pushed to the Hub.\n",
      "     |              Will default to `True` if there is no directory named like `repo_id`, `False` otherwise.\n",
      "     |          commit_message (`str`, *optional*):\n",
      "     |              Message to commit while pushing. Will default to `\"Upload {object}\"`.\n",
      "     |          private (`bool`, *optional*):\n",
      "     |              Whether or not the repository created should be private.\n",
      "     |          token (`bool` or `str`, *optional*):\n",
      "     |              The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n",
      "     |              when running `huggingface-cli login` (stored in `~/.huggingface`). Will default to `True` if `repo_url`\n",
      "     |              is not specified.\n",
      "     |          max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n",
      "     |              Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n",
      "     |              will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n",
      "     |              by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n",
      "     |              Google Colab instances without any CPU OOM issues.\n",
      "     |          create_pr (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether or not to create a PR with the uploaded files or directly commit.\n",
      "     |          safe_serialization (`bool`, *optional*, defaults to `True`):\n",
      "     |              Whether or not to convert the model weights in safetensors format for safer serialization.\n",
      "     |          revision (`str`, *optional*):\n",
      "     |              Branch to push the uploaded files to.\n",
      "     |          commit_description (`str`, *optional*):\n",
      "     |              The description of the commit that will be created\n",
      "     |          tags (`List[str]`, *optional*):\n",
      "     |              List of tags to push on the Hub.\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      from transformers import {object_class}\n",
      "     |      \n",
      "     |      {object} = {object_class}.from_pretrained(\"google-bert/bert-base-cased\")\n",
      "     |      \n",
      "     |      # Push the {object} to your namespace with the name \"my-finetuned-bert\".\n",
      "     |      {object}.push_to_hub(\"my-finetuned-bert\")\n",
      "     |      \n",
      "     |      # Push the {object} to an organization with the name \"my-finetuned-bert\".\n",
      "     |      {object}.push_to_hub(\"huggingface/my-finetuned-bert\")\n",
      "     |      ```\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from transformers.utils.hub.PushToHubMixin:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GenerationMixin(builtins.object)\n",
      "     |  A class containing all functions for auto-regressive text generation, to be used as a mixin in [`PreTrainedModel`].\n",
      "     |  \n",
      "     |  The class exposes [`~generation.GenerationMixin.generate`], which can be used for:\n",
      "     |      - *greedy decoding* if `num_beams=1` and `do_sample=False`\n",
      "     |      - *contrastive search* if `penalty_alpha>0` and `top_k>1`\n",
      "     |      - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n",
      "     |      - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n",
      "     |      - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n",
      "     |      - *diverse beam-search decoding* if `num_beams>1` and `num_beam_groups>1`\n",
      "     |      - *constrained beam-search decoding* if `constraints!=None` or `force_words_ids!=None`\n",
      "     |      - *assisted decoding* if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`\n",
      "     |  \n",
      "     |  To learn more about decoding strategies refer to the [text generation strategies guide](../generation_strategies).\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  compute_transition_scores(self, sequences: torch.Tensor, scores: Tuple[torch.Tensor], beam_indices: Optional[torch.Tensor] = None, normalize_logits: bool = False) -> torch.Tensor\n",
      "     |      Computes the transition scores of sequences given the generation scores (and beam indices, if beam search was\n",
      "     |      used). This is a convenient method to quicky obtain the scores of the selected tokens at generation time.\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          sequences (`torch.LongTensor`):\n",
      "     |              The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or\n",
      "     |              shorter if all batches finished early due to the `eos_token_id`.\n",
      "     |          scores (`tuple(torch.FloatTensor)`):\n",
      "     |              Transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n",
      "     |              of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n",
      "     |              Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n",
      "     |              with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n",
      "     |          beam_indices (`torch.LongTensor`, *optional*):\n",
      "     |              Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n",
      "     |              `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams>1` at\n",
      "     |              generate-time.\n",
      "     |          normalize_logits (`bool`, *optional*, defaults to `False`):\n",
      "     |              Whether to normalize the logits (which, for legacy reasons, may be unnormalized).\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length)` containing\n",
      "     |              the transition scores (logits)\n",
      "     |      \n",
      "     |      Examples:\n",
      "     |      \n",
      "     |      ```python\n",
      "     |      >>> from transformers import GPT2Tokenizer, AutoModelForCausalLM\n",
      "     |      >>> import numpy as np\n",
      "     |      \n",
      "     |      >>> tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
      "     |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      >>> tokenizer.pad_token_id = tokenizer.eos_token_id\n",
      "     |      >>> inputs = tokenizer([\"Today is\"], return_tensors=\"pt\")\n",
      "     |      \n",
      "     |      >>> # Example 1: Print the scores for each token generated with Greedy Search\n",
      "     |      >>> outputs = model.generate(**inputs, max_new_tokens=5, return_dict_in_generate=True, output_scores=True)\n",
      "     |      >>> transition_scores = model.compute_transition_scores(\n",
      "     |      ...     outputs.sequences, outputs.scores, normalize_logits=True\n",
      "     |      ... )\n",
      "     |      >>> # input_length is the length of the input prompt for decoder-only models, like the GPT family, and 1 for\n",
      "     |      >>> # encoder-decoder models, like BART or T5.\n",
      "     |      >>> input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
      "     |      >>> generated_tokens = outputs.sequences[:, input_length:]\n",
      "     |      >>> for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
      "     |      ...     # | token | token string | log probability | probability\n",
      "     |      ...     print(f\"| {tok:5d} | {tokenizer.decode(tok):8s} | {score.numpy():.3f} | {np.exp(score.numpy()):.2%}\")\n",
      "     |      |   262 |  the     | -1.414 | 24.33%\n",
      "     |      |  1110 |  day     | -2.609 | 7.36%\n",
      "     |      |   618 |  when    | -2.010 | 13.40%\n",
      "     |      |   356 |  we      | -1.859 | 15.58%\n",
      "     |      |   460 |  can     | -2.508 | 8.14%\n",
      "     |      \n",
      "     |      >>> # Example 2: Reconstruct the sequence scores from Beam Search\n",
      "     |      >>> outputs = model.generate(\n",
      "     |      ...     **inputs,\n",
      "     |      ...     max_new_tokens=5,\n",
      "     |      ...     num_beams=4,\n",
      "     |      ...     num_return_sequences=4,\n",
      "     |      ...     return_dict_in_generate=True,\n",
      "     |      ...     output_scores=True,\n",
      "     |      ... )\n",
      "     |      >>> transition_scores = model.compute_transition_scores(\n",
      "     |      ...     outputs.sequences, outputs.scores, outputs.beam_indices, normalize_logits=False\n",
      "     |      ... )\n",
      "     |      >>> # If you sum the generated tokens' scores and apply the length penalty, you'll get the sequence scores.\n",
      "     |      >>> # Tip 1: recomputing the scores is only guaranteed to match with `normalize_logits=False`. Depending on the\n",
      "     |      >>> # use case, you might want to recompute it with `normalize_logits=True`.\n",
      "     |      >>> # Tip 2: the output length does NOT include the input length\n",
      "     |      >>> output_length = np.sum(transition_scores.numpy() < 0, axis=1)\n",
      "     |      >>> length_penalty = model.generation_config.length_penalty\n",
      "     |      >>> reconstructed_scores = transition_scores.sum(axis=1) / (output_length**length_penalty)\n",
      "     |      >>> print(np.allclose(outputs.sequences_scores, reconstructed_scores))\n",
      "     |      True\n",
      "     |      ```\n",
      "     |  \n",
      "     |  contrastive_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  generate(self, inputs: Optional[torch.Tensor] = None, generation_config: Optional[transformers.generation.configuration_utils.GenerationConfig] = None, logits_processor: Optional[transformers.generation.logits_process.LogitsProcessorList] = None, stopping_criteria: Optional[transformers.generation.stopping_criteria.StoppingCriteriaList] = None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], List[int]]] = None, synced_gpus: Optional[bool] = None, assistant_model: Optional[ForwardRef('PreTrainedModel')] = None, streamer: Optional[ForwardRef('BaseStreamer')] = None, negative_prompt_ids: Optional[torch.Tensor] = None, negative_prompt_attention_mask: Optional[torch.Tensor] = None, **kwargs) -> Union[transformers.generation.utils.GenerateDecoderOnlyOutput, transformers.generation.utils.GenerateEncoderDecoderOutput, transformers.generation.utils.GenerateBeamDecoderOnlyOutput, transformers.generation.utils.GenerateBeamEncoderDecoderOutput, torch.LongTensor]\n",
      "     |      Generates sequences of token ids for models with a language modeling head.\n",
      "     |      \n",
      "     |      <Tip warning={true}>\n",
      "     |      \n",
      "     |      Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n",
      "     |      model's default generation configuration. You can override any `generation_config` by passing the corresponding\n",
      "     |      parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n",
      "     |      \n",
      "     |      For an overview of generation strategies and code examples, check out the [following\n",
      "     |      guide](../generation_strategies).\n",
      "     |      \n",
      "     |      </Tip>\n",
      "     |      \n",
      "     |      Parameters:\n",
      "     |          inputs (`torch.Tensor` of varying shape depending on the modality, *optional*):\n",
      "     |              The sequence used as a prompt for the generation or as model inputs to the encoder. If `None` the\n",
      "     |              method initializes it with `bos_token_id` and a batch size of 1. For decoder-only models `inputs`\n",
      "     |              should be in the format of `input_ids`. For encoder-decoder models *inputs* can represent any of\n",
      "     |              `input_ids`, `input_values`, `input_features`, or `pixel_values`.\n",
      "     |          generation_config ([`~generation.GenerationConfig`], *optional*):\n",
      "     |              The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n",
      "     |              passed to generate matching the attributes of `generation_config` will override them. If\n",
      "     |              `generation_config` is not provided, the default will be used, which has the following loading\n",
      "     |              priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n",
      "     |              configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n",
      "     |              default values, whose documentation should be checked to parameterize generation.\n",
      "     |          logits_processor (`LogitsProcessorList`, *optional*):\n",
      "     |              Custom logits processors that complement the default logits processors built from arguments and\n",
      "     |              generation config. If a logit processor is passed that is already created with the arguments or a\n",
      "     |              generation config an error is thrown. This feature is intended for advanced users.\n",
      "     |          stopping_criteria (`StoppingCriteriaList`, *optional*):\n",
      "     |              Custom stopping criteria that complements the default stopping criteria built from arguments and a\n",
      "     |              generation config. If a stopping criteria is passed that is already created with the arguments or a\n",
      "     |              generation config an error is thrown. If your stopping criteria depends on the `scores` input, make\n",
      "     |              sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`. This feature is\n",
      "     |              intended for advanced users.\n",
      "     |          prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n",
      "     |              If provided, this function constraints the beam search to allowed tokens only at each step. If not\n",
      "     |              provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n",
      "     |              `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n",
      "     |              on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n",
      "     |              for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n",
      "     |              Retrieval](https://arxiv.org/abs/2010.00904).\n",
      "     |          synced_gpus (`bool`, *optional*):\n",
      "     |              Whether to continue running the while loop until max_length. Unless overridden this flag will be set to\n",
      "     |              `True` under DeepSpeed ZeRO Stage 3 multiple GPUs environment to avoid hanging if one GPU finished\n",
      "     |              generating before other GPUs. Otherwise it'll be set to `False`.\n",
      "     |          assistant_model (`PreTrainedModel`, *optional*):\n",
      "     |              An assistant model that can be used to accelerate generation. The assistant model must have the exact\n",
      "     |              same tokenizer. The acceleration is achieved when forecasting candidate tokens with the assistent model\n",
      "     |              is much faster than running generation with the model you're calling generate from. As such, the\n",
      "     |              assistant model should be much smaller.\n",
      "     |          streamer (`BaseStreamer`, *optional*):\n",
      "     |              Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n",
      "     |              through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n",
      "     |          negative_prompt_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "     |              The negative prompt needed for some processors such as CFG. The batch size must match the input batch\n",
      "     |              size. This is an experimental feature, subject to breaking API changes in future versions.\n",
      "     |          negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "     |              Attention_mask for `negative_prompt_ids`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Ad hoc parametrization of `generation_config` and/or additional model-specific kwargs that will be\n",
      "     |              forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n",
      "     |              specific kwargs should not be prefixed and decoder specific kwargs should be prefixed with *decoder_*.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n",
      "     |          or when `config.return_dict_in_generate=True`) or a `torch.LongTensor`.\n",
      "     |      \n",
      "     |              If the model is *not* an encoder-decoder model (`model.config.is_encoder_decoder=False`), the possible\n",
      "     |              [`~utils.ModelOutput`] types are:\n",
      "     |      \n",
      "     |                  - [`~generation.GenerateDecoderOnlyOutput`],\n",
      "     |                  - [`~generation.GenerateBeamDecoderOnlyOutput`]\n",
      "     |      \n",
      "     |              If the model is an encoder-decoder model (`model.config.is_encoder_decoder=True`), the possible\n",
      "     |              [`~utils.ModelOutput`] types are:\n",
      "     |      \n",
      "     |                  - [`~generation.GenerateEncoderDecoderOutput`],\n",
      "     |                  - [`~generation.GenerateBeamEncoderDecoderOutput`]\n",
      "     |  \n",
      "     |  heal_tokens(self, input_ids: torch.LongTensor, tokenizer: Optional[ForwardRef('PreTrainedTokenizerBase')] = None) -> torch.LongTensor\n",
      "     |      Generates sequences of token ids for models with a language modeling head.\n",
      "     |      Parameters:\n",
      "     |          input_ids (`torch.LongTensor`): The sequence used as a prompt for the generation.\n",
      "     |          tokenizer (`PreTrainedTokenizerBase`, *optional*): The tokenizer used to decode the input ids.\n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` where each sequence has its tail token replaced with its appropriate extension.\n",
      "     |  \n",
      "     |  prepare_inputs_for_generation(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GenerationMode(transformers.utils.generic.ExplicitEnum)\n",
      "     |  GenerationMode(value, names=None, *, module=None, qualname=None, type=None, start=1)\n",
      "     |  \n",
      "     |  Possible generation modes, downstream of the [`~generation.GenerationMixin.generate`] method.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerationMode\n",
      "     |      transformers.utils.generic.ExplicitEnum\n",
      "     |      builtins.str\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ASSISTED_GENERATION = <GenerationMode.ASSISTED_GENERATION: 'assisted_g...\n",
      "     |  \n",
      "     |  BEAM_SAMPLE = <GenerationMode.BEAM_SAMPLE: 'beam_sample'>\n",
      "     |  \n",
      "     |  BEAM_SEARCH = <GenerationMode.BEAM_SEARCH: 'beam_search'>\n",
      "     |  \n",
      "     |  CONSTRAINED_BEAM_SEARCH = <GenerationMode.CONSTRAINED_BEAM_SEARCH: 'co...\n",
      "     |  \n",
      "     |  CONTRASTIVE_SEARCH = <GenerationMode.CONTRASTIVE_SEARCH: 'contrastive_...\n",
      "     |  \n",
      "     |  GREEDY_SEARCH = <GenerationMode.GREEDY_SEARCH: 'greedy_search'>\n",
      "     |  \n",
      "     |  GROUP_BEAM_SEARCH = <GenerationMode.GROUP_BEAM_SEARCH: 'group_beam_sea...\n",
      "     |  \n",
      "     |  SAMPLE = <GenerationMode.SAMPLE: 'sample'>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumMeta:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    GreedySearchDecoderOnlyOutput = class GenerateDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GreedySearchDecoderOnlyOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'attentions', 'hidd...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    GreedySearchEncoderDecoderOutput = class GenerateEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  GreedySearchEncoderDecoderOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cross_attentions': typing.Optional[typing.Tuple[ty...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'cross_attentions': Field(name='cross_attentio...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'encoder_attentions...\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class HammingDiversityLogitsProcessor(LogitsProcessor)\n",
      "     |  HammingDiversityLogitsProcessor(diversity_penalty: float, num_beams: int, num_beam_groups: int)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that enforces diverse beam search.\n",
      "     |  \n",
      "     |  Note that this logits processor is only effective for [`PreTrainedModel.group_beam_search`]. See [Diverse Beam\n",
      "     |  Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf) for more\n",
      "     |  details.\n",
      "     |  \n",
      "     |  Traditional beam search often generates very similar sequences across different beams.\n",
      "     |  `HammingDiversityLogitsProcessor` addresses this by penalizing beams that generate tokens already chosen by other\n",
      "     |  beams in the same time step.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      diversity_penalty (`float`):\n",
      "     |          This value is subtracted from a beam's score if it generates a token same as any beam from other group at a\n",
      "     |          particular time. A higher `diversity_penalty` will enforce greater diversity among the beams. Adjusting\n",
      "     |          this value can help strike a balance between diversity and natural likelihood.\n",
      "     |      num_beams (`int`):\n",
      "     |          Number of beams for beam search. 1 means no beam search.\n",
      "     |      num_beam_groups (`int`):\n",
      "     |          Number of groups to divide `num_beams` into in order to ensure diversity among different groups of beams.\n",
      "     |          [this paper](https://arxiv.org/pdf/1610.02424.pdf) for more details.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
      "     |  >>> import torch\n",
      "     |  \n",
      "     |  >>> # Initialize the model and tokenizer\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-base\")\n",
      "     |  >>> model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-base\")\n",
      "     |  \n",
      "     |  >>> # A long text about the solar system\n",
      "     |  >>> text = (\n",
      "     |  ...     \"The Solar System is a gravitationally bound system comprising the Sun and the objects that orbit it, \"\n",
      "     |  ...     \"either directly or indirectly. Of the objects that orbit the Sun directly, the largest are the eight \"\n",
      "     |  ...     \"planets, with the remainder being smaller objects, such as the five dwarf planets and small Solar System \"\n",
      "     |  ...     \"bodies. The Solar System formed 4.6 billion years ago from the gravitational collapse of a giant \"\n",
      "     |  ...     \"interstellar molecular cloud.\"\n",
      "     |  ... )\n",
      "     |  >>> inputs = tokenizer(\"summarize: \" + text, return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # Generate diverse summary\n",
      "     |  >>> outputs_diverse = model.generate(\n",
      "     |  ...     **inputs,\n",
      "     |  ...     num_beam_groups=2,\n",
      "     |  ...     diversity_penalty=10.0,\n",
      "     |  ...     max_length=100,\n",
      "     |  ...     num_beams=4,\n",
      "     |  ...     num_return_sequences=2,\n",
      "     |  ... )\n",
      "     |  >>> summaries_diverse = tokenizer.batch_decode(outputs_diverse, skip_special_tokens=True)\n",
      "     |  \n",
      "     |  >>> # Generate non-diverse summary\n",
      "     |  >>> outputs_non_diverse = model.generate(\n",
      "     |  ...     **inputs,\n",
      "     |  ...     max_length=100,\n",
      "     |  ...     num_beams=4,\n",
      "     |  ...     num_return_sequences=2,\n",
      "     |  ... )\n",
      "     |  >>> summary_non_diverse = tokenizer.batch_decode(outputs_non_diverse, skip_special_tokens=True)\n",
      "     |  \n",
      "     |  >>> # With `diversity_penalty`, the resulting beams are much more diverse\n",
      "     |  >>> print(summary_non_diverse)\n",
      "     |  ['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n",
      "     |  'the Solar System formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.']\n",
      "     |  \n",
      "     |  >>> print(summaries_diverse)\n",
      "     |  ['the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets.',\n",
      "     |  'the solar system formed 4.6 billion years ago from the collapse of a giant interstellar molecular cloud. of the objects that orbit the Sun directly, the largest are the eight planets. the rest of the objects are smaller objects, such as the five dwarf planets and small solar system bodies.']\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      HammingDiversityLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, current_tokens: torch.LongTensor, beam_group_idx: int) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n",
      "     |              beam search or log softmax for each vocabulary token when using beam search\n",
      "     |          current_tokens (`torch.LongTensor` of shape `(batch_size)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary, corresponding to the tokens selected by the other\n",
      "     |              beam groups in the current generation step.\n",
      "     |          beam_group_idx (`int`):\n",
      "     |              The index of the beam group currently being processed.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`:\n",
      "     |              The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, diversity_penalty: float, num_beams: int, num_beam_groups: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class InfNanRemoveLogitsProcessor(LogitsProcessor)\n",
      "     |  [`LogitsProcessor`] that removes all `nan` and `inf` values to avoid the generation method to fail. Note that using\n",
      "     |  the logits processor should only be used if necessary since it can slow down the generation method.\n",
      "     |  \n",
      "     |  This logits processor has no `generate` example, as there shouldn't be a correct combination of flags that warrants\n",
      "     |  its use.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      InfNanRemoveLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LogitNormalization(LogitsProcessor, LogitsWarper)\n",
      "     |  [`LogitsWarper`] and [`LogitsProcessor`] for normalizing the scores using log-softmax. It's important to normalize\n",
      "     |  the scores during beam search, after applying the logits processors or warpers, since the search algorithm used in\n",
      "     |  this library doesn't do it (it only does it before, but they may need re-normalization) but it still supposes that\n",
      "     |  the scores are normalized when comparing the hypotheses.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  >>> import torch\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2, 3\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # By default, the scores are not normalized -- the sum of their exponentials is NOT a normalized probability\n",
      "     |  >>> # distribution, summing to 1\n",
      "     |  >>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n",
      "     |  >>> print(torch.allclose(torch.sum(torch.exp(outputs.scores[-1])), torch.Tensor((1.000,)), rtol=1e-4))\n",
      "     |  False\n",
      "     |  \n",
      "     |  >>> # Normalizing them may have a positive impact on beam methods, or when using the scores on your application\n",
      "     |  >>> outputs = model.generate(**inputs, renormalize_logits=True, return_dict_in_generate=True, output_scores=True)\n",
      "     |  >>> print(torch.allclose(torch.sum(torch.exp(outputs.scores[-1])), torch.Tensor((1.000,)), rtol=1e-4))\n",
      "     |  True\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogitNormalization\n",
      "     |      LogitsProcessor\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LogitsProcessor(builtins.object)\n",
      "     |  Abstract base class for all logit processors that can be applied during generation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LogitsProcessorList(builtins.list)\n",
      "     |  LogitsProcessorList(iterable=(), /)\n",
      "     |  \n",
      "     |  This class can be used to create a list of [`LogitsProcessor`] or [`LogitsWarper`] to subsequently process a\n",
      "     |  `scores` input tensor. This class inherits from list and adds a specific *__call__* method to apply each\n",
      "     |  [`LogitsProcessor`] or [`LogitsWarper`] to the inputs.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogitsProcessorList\n",
      "     |      builtins.list\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n",
      "     |              beam search or log softmax for each vocabulary token when using beam search\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional kwargs that are specific to a logits processor.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`:\n",
      "     |              The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iadd__(self, value, /)\n",
      "     |      Implement self+=value.\n",
      "     |  \n",
      "     |  __imul__(self, value, /)\n",
      "     |      Implement self*=value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the list.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(self, /)\n",
      "     |      Return the size of the list in memory, in bytes.\n",
      "     |  \n",
      "     |  append(self, object, /)\n",
      "     |      Append object to the end of the list.\n",
      "     |  \n",
      "     |  clear(self, /)\n",
      "     |      Remove all items from list.\n",
      "     |  \n",
      "     |  copy(self, /)\n",
      "     |      Return a shallow copy of the list.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  extend(self, iterable, /)\n",
      "     |      Extend list by appending elements from the iterable.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  insert(self, index, object, /)\n",
      "     |      Insert object before index.\n",
      "     |  \n",
      "     |  pop(self, index=-1, /)\n",
      "     |      Remove and return item at index (default last).\n",
      "     |      \n",
      "     |      Raises IndexError if list is empty or index is out of range.\n",
      "     |  \n",
      "     |  remove(self, value, /)\n",
      "     |      Remove first occurrence of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  reverse(self, /)\n",
      "     |      Reverse *IN PLACE*.\n",
      "     |  \n",
      "     |  sort(self, /, *, key=None, reverse=False)\n",
      "     |      Sort the list in ascending order and return None.\n",
      "     |      \n",
      "     |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n",
      "     |      order of two equal elements is maintained).\n",
      "     |      \n",
      "     |      If a key function is given, apply it once to each list item and sort them,\n",
      "     |      ascending or descending, according to their function values.\n",
      "     |      \n",
      "     |      The reverse flag can be set to sort in descending order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.list:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class LogitsWarper(builtins.object)\n",
      "     |  Abstract base class for all logit warpers that can be applied during generation with multinomial sampling.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MaxLengthCriteria(StoppingCriteria)\n",
      "     |  MaxLengthCriteria(max_length: int, max_position_embeddings: Optional[int] = None)\n",
      "     |  \n",
      "     |  This class can be used to stop generation whenever the full generated number of tokens exceeds `max_length`. Keep\n",
      "     |  in mind for decoder-only type of transformers, this will include the initial prompted tokens.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      max_length (`int`):\n",
      "     |          The maximum length that the output sequence can have in number of tokens.\n",
      "     |      max_position_embeddings (`int`, *optional*):\n",
      "     |          The maximum model length, as defined by the model's `config.max_position_embeddings` attribute.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxLengthCriteria\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  __init__(self, max_length: int, max_position_embeddings: Optional[int] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from StoppingCriteria:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MaxNewTokensCriteria(StoppingCriteria)\n",
      "     |  MaxNewTokensCriteria(start_length: int, max_new_tokens: int)\n",
      "     |  \n",
      "     |  This class can be used to stop generation whenever the generated number of tokens exceeds `max_new_tokens`. Keep in\n",
      "     |  mind for decoder-only type of transformers, this will **not** include the initial prompted tokens. This is very\n",
      "     |  close to `MaxLengthCriteria` but ignores the number of initial tokens.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      start_length (`int`):\n",
      "     |          The number of initial tokens.\n",
      "     |      max_new_tokens (`int`):\n",
      "     |          The maximum number of tokens to generate.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxNewTokensCriteria\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  __init__(self, start_length: int, max_new_tokens: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from StoppingCriteria:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MaxTimeCriteria(StoppingCriteria)\n",
      "     |  MaxTimeCriteria(max_time: float, initial_timestamp: Optional[float] = None)\n",
      "     |  \n",
      "     |  This class can be used to stop generation whenever the full generation exceeds some amount of time. By default, the\n",
      "     |  time will start being counted when you initialize this function. You can override this by passing an\n",
      "     |  `initial_time`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      max_time (`float`):\n",
      "     |          The maximum allowed time in seconds for the generation.\n",
      "     |      initial_time (`float`, *optional*, defaults to `time.time()`):\n",
      "     |          The start of the generation allowed time.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaxTimeCriteria\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  __init__(self, max_time: float, initial_timestamp: Optional[float] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from StoppingCriteria:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MinLengthLogitsProcessor(LogitsProcessor)\n",
      "     |  MinLengthLogitsProcessor(min_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] enforcing a min-length by setting EOS probability to 0. Note that, for decoder-only models\n",
      "     |  like most LLMs, the length includes the prompt.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      min_length (`int`):\n",
      "     |          The minimum length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |      device (`str`, *optional*, defaults to `\"cpu\"`):\n",
      "     |          The device to allocate the tensors.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "     |  \n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A number:\", return_tensors=\"pt\")\n",
      "     |  >>> gen_out = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  A number: one\n",
      "     |  \n",
      "     |  >>> # setting `min_length` to a value smaller than the uncontrolled output length has no impact\n",
      "     |  >>> gen_out = model.generate(**inputs, min_length=3)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  A number: one\n",
      "     |  \n",
      "     |  >>> # setting a larger `min_length` will force the model to generate beyond its natural ending point, which is not\n",
      "     |  >>> # necessarily incorrect\n",
      "     |  >>> gen_out = model.generate(**inputs, min_length=10)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  A number: one thousand, nine hundred and ninety-four\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MinLengthLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, min_length: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MinNewTokensLengthLogitsProcessor(LogitsProcessor)\n",
      "     |  MinNewTokensLengthLogitsProcessor(prompt_length_to_skip: int, min_new_tokens: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] enforcing a min-length of new tokens by setting EOS (End-Of-Sequence) token probability to 0.\n",
      "     |  Contrarily to [`MinLengthLogitsProcessor`], this processor ignores the prompt.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      prompt_length_to_skip (`int`):\n",
      "     |          The input tokens length. Not a valid argument when used with `generate` as it will automatically assign the\n",
      "     |          input length.\n",
      "     |      min_new_tokens (`int`):\n",
      "     |          The minimum *new* tokens length below which the score of `eos_token_id` is set to `-float(\"Inf\")`.\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |      device (`str`, *optional*, defaults to `\"cpu\"`):\n",
      "     |          The device to allocate the tensors.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "     |  \n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer([\"A number:\"], return_tensors=\"pt\")\n",
      "     |  >>> gen_out = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  A number: one\n",
      "     |  \n",
      "     |  >>> # setting `min_new_tokens` will force the model to generate beyond its natural ending point, which is not\n",
      "     |  >>> # necessarily incorrect\n",
      "     |  >>> gen_out = model.generate(**inputs, min_new_tokens=2)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  A number: one thousand\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MinNewTokensLengthLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, prompt_length_to_skip: int, min_new_tokens: int, eos_token_id: Union[int, List[int], torch.Tensor], device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MinPLogitsWarper(LogitsWarper)\n",
      "     |  MinPLogitsWarper(min_p: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs min-p, i.e. keeps all tokens that are above a minimum probability, scaled by the\n",
      "     |  probability of the most likely token. As a result, the filter becomes more agressive in the presence of\n",
      "     |  high-probability tokens, which is a sign of a confident output that we shouldn't deviate from.\n",
      "     |  \n",
      "     |  Often used together with [`TemperatureLogitsWarper`]. Used as an alternative to [`TopPLogitsWarper`] and\n",
      "     |  [`TopKLogitsWarper`].\n",
      "     |  \n",
      "     |  Created by @menhguin and @kalomaze (github handles). Code adapted from [this external PR](https://github.com/oobabooga/text-generation-webui/pull/4449/files)\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      min_p (`float`):\n",
      "     |          Minimum token probability, which will be scaled by the probability of the most likely token. It must be a\n",
      "     |          value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in\n",
      "     |          the 0.99-0.8 range (use the opposite of normal `top_p` values).\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All filtered values will be set to this float value.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Minimum number of tokens that cannot be filtered.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With sampling, the output is unexpected -- sometimes too unexpected.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3 | < 4 (left-hand pointer) ;\n",
      "     |  <BLANKLINE>\n",
      "     |  <BLANKLINE>\n",
      "     |  \n",
      "     |  >>> # With `min_p` sampling, the output gets restricted to high-probability tokens.\n",
      "     |  >>> # Pro tip: In practice, LLMs use `min_p` in the 0.01-0.2 range.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, min_p=0.1)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MinPLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, min_p: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NoBadWordsLogitsProcessor(SequenceBiasLogitsProcessor)\n",
      "     |  NoBadWordsLogitsProcessor(bad_words_ids: List[List[int]], eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that enforces that specified sequences will never be selected.\n",
      "     |  \n",
      "     |  <Tip>\n",
      "     |  \n",
      "     |  In order to get the token ids of the words that should not appear in the generated text, make sure to set\n",
      "     |  `add_prefix_space=True` when initializing the tokenizer, and use `tokenizer(bad_words,\n",
      "     |  add_special_tokens=False).input_ids`. The `add_prefix_space` argument is only supported for some slow tokenizers,\n",
      "     |  as fast tokenizers' prefixing behaviours come from `pre tokenizers`. Read more\n",
      "     |  [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      bad_words_ids (`List[List[int]]`):\n",
      "     |          List of list of token ids that are not allowed to be generated.\n",
      "     |      eos_token_id (`Union[int, List[int], torch.Tensor]`, *optional*):\n",
      "     |          The id(s) of the *end-of-sequence* token.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> inputs = tokenizer([\"In a word, the cake is a\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> output_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=5, pad_token_id=tokenizer.eos_token_id)\n",
      "     |  >>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])\n",
      "     |  In a word, the cake is a bit of a mess.\n",
      "     |  \n",
      "     |  >>> # Now let's take the bad words out. Please note that the tokenizer is initialized differently\n",
      "     |  >>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
      "     |  \n",
      "     |  \n",
      "     |  >>> def get_tokens_as_list(word_list):\n",
      "     |  ...     \"Converts a sequence of words into a list of tokens\"\n",
      "     |  ...     tokens_list = []\n",
      "     |  ...     for word in word_list:\n",
      "     |  ...         tokenized_word = tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]\n",
      "     |  ...         tokens_list.append(tokenized_word)\n",
      "     |  ...     return tokens_list\n",
      "     |  \n",
      "     |  \n",
      "     |  >>> bad_words_ids = get_tokens_as_list(word_list=[\"mess\"])\n",
      "     |  >>> output_ids = model.generate(\n",
      "     |  ...     inputs[\"input_ids\"], max_new_tokens=5, bad_words_ids=bad_words_ids, pad_token_id=tokenizer.eos_token_id\n",
      "     |  ... )\n",
      "     |  >>> print(tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0])\n",
      "     |  In a word, the cake is a bit of a surprise.\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NoBadWordsLogitsProcessor\n",
      "     |      SequenceBiasLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, bad_words_ids: List[List[int]], eos_token_id: Union[int, List[int], torch.Tensor, NoneType] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from SequenceBiasLogitsProcessor:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NoRepeatNGramLogitsProcessor(LogitsProcessor)\n",
      "     |  NoRepeatNGramLogitsProcessor(ngram_size: int)\n",
      "     |  \n",
      "     |  N-grams are groups of \"n\" consecutive words, characters, or tokens taken from a sequence of text. Given the\n",
      "     |  sentence: \"She runs fast\", the bi-grams (n=2) would be (\"she\", \"runs\") and (\"runs\", \"fast\"). In text generation,\n",
      "     |  avoiding repetitions of word sequences provides a more diverse output. This [`LogitsProcessor`] enforces no\n",
      "     |  repetition of n-grams by setting the scores of banned tokens to negative infinity which eliminates those tokens\n",
      "     |  from consideration when further processing the scores. Note that, for decoder-only models like most LLMs, the\n",
      "     |  prompt is also considered to obtain the n-grams.\n",
      "     |  [Fairseq](https://github.com/pytorch/fairseq/blob/a07cb6f40480928c9e0548b737aadd36ee66ac76/fairseq/sequence_generator.py#L345).\n",
      "     |  \n",
      "     |  <Tip>\n",
      "     |  \n",
      "     |  Use n-gram penalties with care. For instance, penalizing 2-grams (bigrams) in an article about the city of New York\n",
      "     |  might lead to undesirable outcomes where the city's name appears only once in the entire text.\n",
      "     |  [Reference](https://huggingface.co/blog/how-to-generate)\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      ngram_size (`int`):\n",
      "     |          All ngrams of size `ngram_size` can only occur once.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```py\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> inputs = tokenizer([\"Today I\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> output = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
      "     |  Today I’m not sure if I’m going to be able to do it.\n",
      "     |  \n",
      "     |  >>> # Now let's add ngram size using `no_repeat_ngram_size`. This stops the repetitions (\"I’m\") in the output.\n",
      "     |  >>> output = model.generate(**inputs, no_repeat_ngram_size=2)\n",
      "     |  >>> print(tokenizer.decode(output[0], skip_special_tokens=True))\n",
      "     |  Today I’m not sure if I can get a better understanding of the nature of this issue\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NoRepeatNGramLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, ngram_size: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PhrasalConstraint(Constraint)\n",
      "     |  PhrasalConstraint(token_ids: List[int])\n",
      "     |  \n",
      "     |  [`Constraint`] enforcing that an ordered sequence of tokens is included in the output.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      token_ids (`List[int]`):\n",
      "     |          The id of the token that must be generated by the output.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PhrasalConstraint\n",
      "     |      Constraint\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, token_ids: List[int])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  advance(self)\n",
      "     |      When called, returns the token that would take this constraint one step closer to being fulfilled.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          token_ids(`torch.tensor`): Must be a tensor of a list of indexable tokens, not some integer.\n",
      "     |  \n",
      "     |  copy(self, stateful=False)\n",
      "     |      Creates a new instance of this constraint.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          stateful(`bool`): Whether to not only copy the constraint for new instance, but also its state.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          constraint(`Constraint`): The same constraint as the one being called from.\n",
      "     |  \n",
      "     |  does_advance(self, token_id: int)\n",
      "     |      Reads in a token and returns whether it creates progress.\n",
      "     |  \n",
      "     |  remaining(self)\n",
      "     |      Returns the number of remaining steps of `advance()` in order to complete this constraint.\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |      Resets the state of this constraint to its initialization. We would call this in cases where the fulfillment of\n",
      "     |      a constraint is abrupted by an unwanted token.\n",
      "     |  \n",
      "     |  update(self, token_id: int)\n",
      "     |      Reads in a token and returns booleans that indicate the progress made by it. This function will update the\n",
      "     |      state of this object unlikes `does_advance(self, token_id: int)`.\n",
      "     |      \n",
      "     |      This isn't to test whether a certain token will advance the progress; it's to update its state as if it has\n",
      "     |      been generated. This becomes important if token_id != desired token (refer to else statement in\n",
      "     |      PhrasalConstraint)\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          token_id(`int`):\n",
      "     |              The id of a newly generated token in the beam search.\n",
      "     |      Return:\n",
      "     |          stepped(`bool`):\n",
      "     |              Whether this constraint has become one step closer to being fulfuilled.\n",
      "     |          completed(`bool`):\n",
      "     |              Whether this constraint has been completely fulfilled by this token being generated.\n",
      "     |          reset (`bool`):\n",
      "     |              Whether this constraint has reset its progress by this token being generated.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Constraint:\n",
      "     |  \n",
      "     |  test(self)\n",
      "     |      Tests whether this constraint has been properly defined.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Constraint:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PrefixConstrainedLogitsProcessor(LogitsProcessor)\n",
      "     |  PrefixConstrainedLogitsProcessor(prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], num_beams: int)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that enforces constrained generation and is useful for prefix-conditioned constrained\n",
      "     |  generation. See [Autoregressive Entity Retrieval](https://arxiv.org/abs/2010.00904) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`):\n",
      "     |          This function constraints the beam search to allowed tokens only at each step. This function takes 2\n",
      "     |          arguments `inputs_ids` and the batch ID `batch_id`. It has to return a list with the allowed tokens for the\n",
      "     |          next generation step conditioned on the previously generated tokens `inputs_ids` and the batch ID\n",
      "     |          `batch_id`.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```py\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"Alice and Bob\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # By default, it continues generating according to the model's logits\n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=5)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  Alice and Bob are friends\n",
      "     |  \n",
      "     |  >>> # We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.\n",
      "     |  >>> # For instance, we can force an entire entity to be generated when its beginning is detected.\n",
      "     |  >>> entity = tokenizer(\" Bob Marley\", return_tensors=\"pt\").input_ids[0]  # 3 tokens\n",
      "     |  >>> def prefix_allowed_tokens_fn(batch_id, input_ids):\n",
      "     |  ...     '''\n",
      "     |  ...     Attempts to generate 'Bob Marley' when 'Bob' is detected.\n",
      "     |  ...     In this case, `batch_id` is not used, but you can set rules for each batch member.\n",
      "     |  ...     '''\n",
      "     |  ...     if input_ids[-1] == entity[0]:\n",
      "     |  ...         return [entity[1].item()]\n",
      "     |  ...     elif input_ids[-2] == entity[0] and input_ids[-1] == entity[1]:\n",
      "     |  ...         return [entity[2].item()]\n",
      "     |  ...     return list(range(tokenizer.vocab_size))  # If no match, allow all tokens\n",
      "     |  \n",
      "     |  >>> outputs = model.generate(**inputs, max_new_tokens=5, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  Alice and Bob Marley\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PrefixConstrainedLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]], num_beams: int)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class PromptLookupCandidateGenerator(CandidateGenerator)\n",
      "     |  PromptLookupCandidateGenerator(num_output_tokens: int = 10, max_matching_ngram_size: int = None, max_length: int = 20)\n",
      "     |  \n",
      "     |  `CandidateGenerator` class to be used for prompt lookup generation. This class generates candidates by looking up\n",
      "     |  likely continuations in the provided prompt (input_ids) itself.\n",
      "     |  Read the following blog post for more information: https://github.com/apoorvumang/prompt-lookup-decoding\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      max_matching_ngram_size (`int`):\n",
      "     |          The maximum ngram size to be considered for matching in the prompt\n",
      "     |      num_output_tokens (`int`):\n",
      "     |          The number of tokens to be output as candidate tokens.\n",
      "     |      max_length (`int`):\n",
      "     |          The number of total maximum tokens that can be generated. For decoder-only models that includes the prompt length.\n",
      "     |          Defaults to 20, which is the max length used as default in generation config.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PromptLookupCandidateGenerator\n",
      "     |      CandidateGenerator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_output_tokens: int = 10, max_matching_ngram_size: int = None, max_length: int = 20)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]\n",
      "     |      Fetches the candidates to be tried for the current input.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.LongTensor` of shape `(num_candidates, candidate_length)`: The candidate sequences to be tried.\n",
      "     |  \n",
      "     |  update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, num_matches: int)\n",
      "     |      Updates the candidate generation strategy based on the outcomes.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, candidate_length, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using\n",
      "     |              beam search or log softmax for each vocabulary token when using beam search\n",
      "     |          num_matches (`int`):\n",
      "     |              The number of matches between the candidate sequences and the model predictions.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from CandidateGenerator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RepetitionPenaltyLogitsProcessor(LogitsProcessor)\n",
      "     |  RepetitionPenaltyLogitsProcessor(penalty: float)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that prevents the repetition of previous tokens through a penalty. This penalty is applied at\n",
      "     |  most once per token. Note that, for decoder-only models like most LLMs, the considered tokens include the prompt.\n",
      "     |  \n",
      "     |  In the original [paper](https://arxiv.org/pdf/1909.05858.pdf), the authors suggest the use of a penalty of around\n",
      "     |  1.2 to achieve a good balance between truthful generation and lack of repetition. To penalize and reduce\n",
      "     |  repetition, use `penalty` values above 1.0, where a higher value penalizes more strongly. To reward and encourage\n",
      "     |  repetition, use `penalty` values between 0.0 and 1.0, where a lower value rewards more strongly.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      penalty (`float`):\n",
      "     |          The parameter for repetition penalty. 1.0 means no penalty. Above 1.0 penalizes previously generated\n",
      "     |          tokens. Between 0.0 and 1.0 rewards previously generated tokens.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```py\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> # Initializing the model and tokenizer for it\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> inputs = tokenizer([\"I'm not going to\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # This shows a normal generate without any specific parameters\n",
      "     |  >>> summary_ids = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n",
      "     |  I'm not going to be able to do that. I'm going to be able to do that\n",
      "     |  \n",
      "     |  >>> # This generates a penalty for repeated tokens\n",
      "     |  >>> penalized_ids = model.generate(**inputs, repetition_penalty=1.1)\n",
      "     |  >>> print(tokenizer.batch_decode(penalized_ids, skip_special_tokens=True)[0])\n",
      "     |  I'm not going to be able to do that. I'll just have to go out and play\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RepetitionPenaltyLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, penalty: float)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    SampleDecoderOnlyOutput = class GenerateDecoderOnlyOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  SampleDecoderOnlyOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of decoder-only generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateDecoderOnlyOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'attentions': typing.Optional[typing.Tuple[typing.T...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'attentions': Field(name='attentions',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'attentions', 'hidd...\n",
      "     |  \n",
      "     |  attentions = None\n",
      "     |  \n",
      "     |  hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    SampleEncoderDecoderOutput = class GenerateEncoderDecoderOutput(transformers.utils.generic.ModelOutput)\n",
      "     |  SampleEncoderDecoderOutput(sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of encoder-decoder generation models, when using non-beam methods.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n",
      "     |          The generated sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n",
      "     |          if all batches finished early due to the `eos_token_id`.\n",
      "     |      scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True` is passed or when `config.output_scores=True`):\n",
      "     |          Processed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True` is passed or when `config.output_logits=True`):\n",
      "     |          Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n",
      "     |          at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n",
      "     |          each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n",
      "     |      encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for each layer of the decoder) of shape `(batch_size, num_heads,\n",
      "     |          sequence_length, sequence_length)`.\n",
      "     |      encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n",
      "     |          shape `(batch_size, sequence_length, hidden_size)`.\n",
      "     |      decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      cross_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or `config.output_attentions=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, num_heads, generated_length, sequence_length)`.\n",
      "     |      decoder_hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n",
      "     |          Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, generated_length, hidden_size)`.\n",
      "     |      past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n",
      "     |          NOTE: some models have a different `past_key_values` format, confirm with the model's documentation.\n",
      "     |          Usually a Tuple (one element for each layer of the decoder) of tuples (two elements, key tensor and value\n",
      "     |          tensor). The first Tuple is of length `config.n_layers`, with each tuple having 2 tensors of shape\n",
      "     |          `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n",
      "     |          `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n",
      "     |          encoder_sequence_length, embed_size_per_head)`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GenerateEncoderDecoderOutput\n",
      "     |      transformers.utils.generic.ModelOutput\n",
      "     |      collections.OrderedDict\n",
      "     |      builtins.dict\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, sequences: torch.LongTensor = None, scores: Optional[Tuple[torch.FloatTensor]] = None, logits: Optional[Tuple[torch.FloatTensor]] = None, encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None, encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None, decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, cross_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, decoder_hidden_states: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, past_key_values: Optional[Tuple[Tuple[Tuple[torch.FloatTensor]]]] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'cross_attentions': typing.Optional[typing.Tuple[ty...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'cross_attentions': Field(name='cross_attentio...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('sequences', 'scores', 'logits', 'encoder_attentions...\n",
      "     |  \n",
      "     |  cross_attentions = None\n",
      "     |  \n",
      "     |  decoder_attentions = None\n",
      "     |  \n",
      "     |  decoder_hidden_states = None\n",
      "     |  \n",
      "     |  encoder_attentions = None\n",
      "     |  \n",
      "     |  encoder_hidden_states = None\n",
      "     |  \n",
      "     |  logits = None\n",
      "     |  \n",
      "     |  past_key_values = None\n",
      "     |  \n",
      "     |  scores = None\n",
      "     |  \n",
      "     |  sequences = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __delitem__(self, *args, **kwargs)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __getitem__(self, k)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __post_init__(self)\n",
      "     |      Check the ModelOutput dataclass.\n",
      "     |      \n",
      "     |      Only occurs if @dataclass decorator has been used.\n",
      "     |  \n",
      "     |  __reduce__(self)\n",
      "     |      Return state information for pickling\n",
      "     |  \n",
      "     |  __setattr__(self, name, value)\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  pop(self, *args, **kwargs)\n",
      "     |      od.pop(key[,default]) -> v, remove specified key and return the corresponding value.\n",
      "     |      \n",
      "     |      If the key is not found, return the default if given; otherwise,\n",
      "     |      raise a KeyError.\n",
      "     |  \n",
      "     |  setdefault(self, *args, **kwargs)\n",
      "     |      Insert key with a value of default if key is not in the dictionary.\n",
      "     |      \n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  to_tuple(self) -> Tuple[Any]\n",
      "     |      Convert self to a tuple containing all the attributes/keys that are not `None`.\n",
      "     |  \n",
      "     |  update(self, *args, **kwargs)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from dict/iterable E and F.\n",
      "     |      If E is present and has a .keys() method, then does:  for k in E: D[k] = E[k]\n",
      "     |      If E is present and lacks a .keys() method, then does:  for k, v in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k in F:  D[k] = F[k]\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from transformers.utils.generic.ModelOutput:\n",
      "     |  \n",
      "     |  __init_subclass__() -> None from builtins.type\n",
      "     |      Register subclasses as pytree nodes.\n",
      "     |      \n",
      "     |      This is necessary to synchronize gradients when using `torch.nn.parallel.DistributedDataParallel` with\n",
      "     |      `static_graph=True` with modules that output `ModelOutput` subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __ior__(self, value, /)\n",
      "     |      Return self|=value.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __reversed__(...)\n",
      "     |      od.__reversed__() <==> reversed(od)\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __sizeof__(...)\n",
      "     |      D.__sizeof__() -> size of D in memory, in bytes\n",
      "     |  \n",
      "     |  clear(...)\n",
      "     |      od.clear() -> None.  Remove all items from od.\n",
      "     |  \n",
      "     |  copy(...)\n",
      "     |      od.copy() -> a shallow copy of od\n",
      "     |  \n",
      "     |  items(...)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(...)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  move_to_end(self, /, key, last=True)\n",
      "     |      Move an existing element to the end (or beginning if last is false).\n",
      "     |      \n",
      "     |      Raise KeyError if the element does not exist.\n",
      "     |  \n",
      "     |  popitem(self, /, last=True)\n",
      "     |      Remove and return a (key, value) pair from the dictionary.\n",
      "     |      \n",
      "     |      Pairs are returned in LIFO order if last is true or FIFO order if false.\n",
      "     |  \n",
      "     |  values(...)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  fromkeys(iterable, value=None) from builtins.type\n",
      "     |      Create a new ordered dictionary with keys from iterable and values set to value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from collections.OrderedDict:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      True if the dictionary has the specified key, else False.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  get(self, key, default=None, /)\n",
      "     |      Return the value for key if key is in the dictionary, else default.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.dict:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class SequenceBiasLogitsProcessor(LogitsProcessor)\n",
      "     |  SequenceBiasLogitsProcessor(sequence_bias: Dict[Tuple[int], float])\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that applies an additive bias on sequences. The bias is applied to the last token of a sequence\n",
      "     |  when the next generated token can complete it. Consequently, to take the most of biasing sequences with more than\n",
      "     |  one token, consider using beam methods (to gracefully work around partially completed sequences that have a\n",
      "     |  negative bias) and applying the bias to their prefixes (to ensure the bias is applied earlier).\n",
      "     |  \n",
      "     |  <Tip>\n",
      "     |  \n",
      "     |  In order to get the token ids of the sequences that you want to bias, make sure to set `add_prefix_space=True` when\n",
      "     |  initializing the tokenizer, and use `tokenizer(bad_words, add_special_tokens=False).input_ids`. The\n",
      "     |  `add_prefix_space` argument is only supported for some slow tokenizers, as fast tokenizers' prefixing behaviours\n",
      "     |  come from `pre tokenizers`. Read more [here](https://huggingface.co/docs/tokenizers/api/pre-tokenizers).\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      sequence_bias (`Dict[Tuple[int], float]`):\n",
      "     |          Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n",
      "     |          sequence being selected, while negative biases do the opposite. If a sequence has a length of 1, its bias\n",
      "     |          will always be applied. Otherwise, the bias will only be applied if the sequence in question is about to be\n",
      "     |          completed (in the token selection step after this processor is applied).\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> inputs = tokenizer([\"The full name of Donald is Donald\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4)\n",
      "     |  >>> print(tokenizer.batch_decode(summary_ids, skip_special_tokens=True)[0])\n",
      "     |  The full name of Donald is Donald J. Trump Jr\n",
      "     |  \n",
      "     |  >>> # Now let's control generation through a bias. Please note that the tokenizer is initialized differently!\n",
      "     |  >>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n",
      "     |  \n",
      "     |  \n",
      "     |  >>> def get_tokens_as_tuple(word):\n",
      "     |  ...     return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])\n",
      "     |  \n",
      "     |  \n",
      "     |  >>> # If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n",
      "     |  >>> sequence_bias = {get_tokens_as_tuple(\"Trump\"): -10.0}\n",
      "     |  >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n",
      "     |  >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n",
      "     |  The full name of Donald is Donald J. Donald,\n",
      "     |  \n",
      "     |  >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
      "     |  >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n",
      "     |  The full name of Donald is Donald Rumsfeld,\n",
      "     |  \n",
      "     |  >>> # We can also add a positive bias to nudge the model towards specific tokens or continuations\n",
      "     |  >>> sequence_bias = {get_tokens_as_tuple(\"Donald Duck\"): 10.0}\n",
      "     |  >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n",
      "     |  >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n",
      "     |  The full name of Donald is Donald Duck.\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SequenceBiasLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, sequence_bias: Dict[Tuple[int], float])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StopStringCriteria(StoppingCriteria)\n",
      "     |  StopStringCriteria(tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, stop_strings: Union[str, List[str]])\n",
      "     |  \n",
      "     |  This class can be used to stop generation whenever specific string sequences are generated. It preprocesses\n",
      "     |  the strings together with the tokenizer vocab to find positions where tokens can validly complete the stop strings.\n",
      "     |  \n",
      "     |  Generation is stopped as soon as a token is generated that completes any of the stop strings.\n",
      "     |  We want to catch any instance in which the stop string would be present in the decoded output, which means\n",
      "     |  we must also catch cases with \"overhangs\" off one or both ends. To make this more concrete, for the stop string\n",
      "     |  \"stop\", any of the following token sequences would trigger the match:\n",
      "     |  \n",
      "     |  - [\"st\", \"op\"]\n",
      "     |  - [\"stop\"]\n",
      "     |  - [\"st\", \"opera\"]\n",
      "     |  - [\"sto\", \"pper\"]\n",
      "     |  - [\"las\", \"topper\"]\n",
      "     |  - [\"s\", \"to\", \"pped\"]\n",
      "     |  \n",
      "     |  Note that a match will only be triggered if the stop string is at the end of the generated sequence. In other\n",
      "     |  words, these sequences will not trigger a match:\n",
      "     |  \n",
      "     |  - [\"stop\", \"at\"]\n",
      "     |  - [\"st\", \"op\", \"at\"]\n",
      "     |  - [\"st\", \"opera\", \"tion\"]\n",
      "     |  \n",
      "     |  The reason these are not a match is that the stop string does not overlap with the final token. If you can remove\n",
      "     |  one or more tokens from the end of the sequence without destroying the stop string, then this criterion will not\n",
      "     |  match that stop string. This is by design; because this check is run after each token is generated, we can't miss a\n",
      "     |  valid stop string if one is generated, but we don't want to halt generation just because the stop string exists\n",
      "     |  somewhere in the past input_ids.\n",
      "     |  \n",
      "     |  How is the match actually performed, though? We do it in quite a confusing way, because we want the entire match\n",
      "     |  process to be compilable with Torch or XLA, which means we cannot use standard string methods. However, it is possible,\n",
      "     |  with some work, to do string matching with pure tensor operations. We'll begin by describing the algorithm we use\n",
      "     |  with standard string operations, and then at the end we'll explain how this is converted to pure tensor operations.\n",
      "     |  \n",
      "     |  The key to the algorithm is an observation: Because the stop string must overlap with the end of the token sequence, we can start at\n",
      "     |  the end of the sequence and work backwards. Specifically, we check that there is an overlap between the start of\n",
      "     |  the final token and the end of the stop_string, or to put it another way, stop_string[-i:] == token[:i] for\n",
      "     |  some i > 0. If you look at the positive examples above, you'll see the last token in all of them fulfills this\n",
      "     |  property:\n",
      "     |  \n",
      "     |  - [\"st\", \"op\"] (overlap is \"op\", overlap length == 2)\n",
      "     |  - [\"stop\"]  (overlap is \"stop\", overlap length == 4)\n",
      "     |  - [\"st\", \"opera\"]  (overlap is \"op\", overlap length == 2)\n",
      "     |  - [\"sto\", \"pper\"]  (overlap is \"p\", overlap length == 1)\n",
      "     |  - [\"las\", \"topper\"]  (overlap is \"top\", overlap length == 3)\n",
      "     |  - [\"s\", \"to\", \"pped\"]  (overlap is \"p\", overlap length == 1)\n",
      "     |  \n",
      "     |  It's impossible to construct a matching sequence that does not have this property (feel free to verify this\n",
      "     |  yourself). However, although this overlap between the start of the final token and the end of the stop string is\n",
      "     |  necessary for a match, it is not sufficient. We also need to check that the rest of the token sequence is\n",
      "     |  consistent with the stop string.\n",
      "     |  \n",
      "     |  How do we do that? Let's use [\"s\", \"to\", \"pped\"] as an example. We know that the final token, \"pped\", has an\n",
      "     |  overlap of 1 with the stop string, \"stop\". We then go back to the previous token, \"to\". Since we have already\n",
      "     |  matched 1 character from the stop string, the remainder to check is \"sto\". We check that the next token \"to\"\n",
      "     |  matches the end of the remainder, which it does. We have now matched 3 characters from the stop string, and the\n",
      "     |  remainder to match is \"s\". We go back to the previous token again, which is also \"s\". This is a match, and so\n",
      "     |  we have matched the entire stop string.\n",
      "     |  \n",
      "     |  How does it work when the tokens run off the start of the stop string, though? Let's consider the example of\n",
      "     |  [\"las\", \"topper\"]. The final token, \"topper\", has an overlap of 3 with the stop string, \"stop\". Therefore,\n",
      "     |  the remaining stop string to match is \"s\". We go back to the previous token, \"las\". Because the remainder to\n",
      "     |  match is just \"s\", with length 1, we consider only the final 1 character from the token, which is \"s\". This\n",
      "     |  matches the stop string, and so the entire string is matched.\n",
      "     |  \n",
      "     |  How do we compute these matches with tensor operations, though? Simply: we efficiently precompute the necessary\n",
      "     |  information for all tokens! For every token, we compute:\n",
      "     |  - Its overlap with the end of the stop string, if any\n",
      "     |  - The positions inside the stop string where the token matches, including matches that run off the start.\n",
      "     |  - The total length of the token\n",
      "     |  \n",
      "     |  For example, for the token \"pped\", we would compute an end overlap of 1, no internal matching positions,\n",
      "     |  and a length of 4. For the token \"to\", we would compute no end overlap, a single internal matching position\n",
      "     |  of 1 (counting from the end), and a length of 2. For the token \"s\", we would compute no end overlap,\n",
      "     |  a single internal matching position of 3 (again counting from the end) and a length of 1.\n",
      "     |  \n",
      "     |  As long as we have this information, we can execute the algorithm above without any string comparison\n",
      "     |  operations. We simply perform the following steps:\n",
      "     |  - Check if the final token has an end-overlap with the start string\n",
      "     |  - Continue backwards, keeping track of how much of the stop string we've matched so far\n",
      "     |  - At each point, check if the next token has the current position as one of its valid positions\n",
      "     |  - Continue until either a match fails, or we completely match the whole stop string\n",
      "     |  \n",
      "     |  Again, consider [\"s\", \"to\", \"pped\"] as an example. \"pped\" has an end overlap of 1, so we can begin a match.\n",
      "     |  We have matched 1 character so far, so we check that the next token \"to\", has 1 as a valid position (again,\n",
      "     |  counting from the end). It does, so we add the length of \"to\" to our position tracker. We have now matched\n",
      "     |  3 characters, so we check that the next token \"s\" has 3 as a valid position. It does, so we add its length\n",
      "     |  to the position tracker. The position tracker is now 4, which is the length of the stop string. We have matched the\n",
      "     |  entire stop string.\n",
      "     |  \n",
      "     |  In the second case, [\"las\", \"topper\"], \"topper\" has an end overlap of 3, so we can begin a match. We have\n",
      "     |  matched 3 characters so far, so we check that the next token \"las\" has 3 as a valid position. It does, because we\n",
      "     |  allow tokens to match positions that run off the start of the stop string. We add its length to the position\n",
      "     |  tracker. The position tracker is now 6, which is greater than the length of the stop string! Don't panic, though -\n",
      "     |  this also counts as a match of the stop string. We have matched the entire stop string.\n",
      "     |  \n",
      "     |  \n",
      "     |  Args:\n",
      "     |      tokenizer (`PreTrainedTokenizer`):\n",
      "     |          The model's associated tokenizer (necessary to extract vocab and tokenize the termination sequences)\n",
      "     |      stop_strings (`Union[str, List[str]]`):\n",
      "     |          A list of strings that should end generation. If a string is passed, it will be treated like a\n",
      "     |          list with a single element.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n",
      "     |  \n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\")\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\")\n",
      "     |  >>> inputs = tokenizer(\"The biggest states in the USA by land area:\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> gen_out = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  The biggest states in the USA by land area:\n",
      "     |  - Alaska\n",
      "     |  - Texas\n",
      "     |  - California\n",
      "     |  \n",
      "     |  >>> # Passing one or more stop strings will halt generation after those strings are emitted\n",
      "     |  >>> # Note that generating with stop strings requires you to pass the tokenizer too\n",
      "     |  >>> gen_out = model.generate(**inputs, stop_strings=[\"Texas\"], tokenizer=tokenizer)\n",
      "     |  >>> print(tokenizer.batch_decode(gen_out, skip_special_tokens=True)[0])\n",
      "     |  The biggest states in the USA by land area:\n",
      "     |  - Alaska\n",
      "     |  - Texas\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StopStringCriteria\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.Tensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  __init__(self, tokenizer: transformers.tokenization_utils_base.PreTrainedTokenizerBase, stop_strings: Union[str, List[str]])\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  clean_and_embed_tokens_with_cache(self, token_list, token_indices, stop_strings, tokenizer)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  clean_tokenizer_vocab(tokenizer, static_prefix='abcdef')\n",
      "     |      This method turns a tokenizer vocab into a \"clean\" vocab where each token represents the actual string\n",
      "     |      it will yield, without any special prefixes like \"##\" or \"Ġ\". This is trickier than it looks - the method\n",
      "     |      tokenizer.convert_tokens_to_string() does not always return the correct string because of issues with prefix\n",
      "     |      space addition/removal. To work around this, we add a static prefix to the start of the token, then remove\n",
      "     |      it (and any prefix that may have been introduced with it) after calling convert_tokens_to_string().\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from StoppingCriteria:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StoppingCriteria(abc.ABC)\n",
      "     |  Abstract base class for all stopping criteria that can be applied during generation.\n",
      "     |  \n",
      "     |  If your stopping criteria depends on the `scores` input, make sure you pass `return_dict_in_generate=True,\n",
      "     |  output_scores=True` to `generate`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StoppingCriteria\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "    \n",
      "    class StoppingCriteriaList(builtins.list)\n",
      "     |  StoppingCriteriaList(iterable=(), /)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StoppingCriteriaList\n",
      "     |      builtins.list\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> torch.BoolTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary.\n",
      "     |      \n",
      "     |              Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n",
      "     |              [`PreTrainedTokenizer.__call__`] for details.\n",
      "     |      \n",
      "     |              [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be scores for each vocabulary token before SoftMax\n",
      "     |              or scores for each vocabulary token after SoftMax. If this stopping criteria depends on the `scores` input,\n",
      "     |              make sure you pass `return_dict_in_generate=True, output_scores=True` to `generate`.\n",
      "     |          kwargs (`Dict[str, Any]`, *optional*):\n",
      "     |              Additional stopping criteria specific kwargs.\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.BoolTensor`. (`torch.BoolTensor` of shape `(batch_size, 1)`), where `True` indicates we stop generation\n",
      "     |              for a particular row, `True` indicates we should continue.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  max_length\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __contains__(self, key, /)\n",
      "     |      Return key in self.\n",
      "     |  \n",
      "     |  __delitem__(self, key, /)\n",
      "     |      Delete self[key].\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getitem__(...)\n",
      "     |      x.__getitem__(y) <==> x[y]\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __iadd__(self, value, /)\n",
      "     |      Implement self+=value.\n",
      "     |  \n",
      "     |  __imul__(self, value, /)\n",
      "     |      Implement self*=value.\n",
      "     |  \n",
      "     |  __init__(self, /, *args, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self, /)\n",
      "     |      Implement iter(self).\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __len__(self, /)\n",
      "     |      Return len(self).\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __reversed__(self, /)\n",
      "     |      Return a reverse iterator over the list.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __setitem__(self, key, value, /)\n",
      "     |      Set self[key] to value.\n",
      "     |  \n",
      "     |  __sizeof__(self, /)\n",
      "     |      Return the size of the list in memory, in bytes.\n",
      "     |  \n",
      "     |  append(self, object, /)\n",
      "     |      Append object to the end of the list.\n",
      "     |  \n",
      "     |  clear(self, /)\n",
      "     |      Remove all items from list.\n",
      "     |  \n",
      "     |  copy(self, /)\n",
      "     |      Return a shallow copy of the list.\n",
      "     |  \n",
      "     |  count(self, value, /)\n",
      "     |      Return number of occurrences of value.\n",
      "     |  \n",
      "     |  extend(self, iterable, /)\n",
      "     |      Extend list by appending elements from the iterable.\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=9223372036854775807, /)\n",
      "     |      Return first index of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  insert(self, index, object, /)\n",
      "     |      Insert object before index.\n",
      "     |  \n",
      "     |  pop(self, index=-1, /)\n",
      "     |      Remove and return item at index (default last).\n",
      "     |      \n",
      "     |      Raises IndexError if list is empty or index is out of range.\n",
      "     |  \n",
      "     |  remove(self, value, /)\n",
      "     |      Remove first occurrence of value.\n",
      "     |      \n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |  \n",
      "     |  reverse(self, /)\n",
      "     |      Reverse *IN PLACE*.\n",
      "     |  \n",
      "     |  sort(self, /, *, key=None, reverse=False)\n",
      "     |      Sort the list in ascending order and return None.\n",
      "     |      \n",
      "     |      The sort is in-place (i.e. the list itself is modified) and stable (i.e. the\n",
      "     |      order of two equal elements is maintained).\n",
      "     |      \n",
      "     |      If a key function is given, apply it once to each list item and sort them,\n",
      "     |      ascending or descending, according to their function values.\n",
      "     |      \n",
      "     |      The reverse flag can be set to sort in descending order.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __class_getitem__(...) from builtins.type\n",
      "     |      See PEP 585\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from builtins.list:\n",
      "     |  \n",
      "     |  __new__(*args, **kwargs) from builtins.type\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from builtins.list:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "    \n",
      "    class SuppressTokensAtBeginLogitsProcessor(LogitsProcessor)\n",
      "     |  SuppressTokensAtBeginLogitsProcessor(begin_suppress_tokens, begin_index, device: str = 'cpu')\n",
      "     |  \n",
      "     |  [`SuppressTokensAtBeginLogitsProcessor`] supresses a list of tokens as soon as the `generate` function starts\n",
      "     |  generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are\n",
      "     |  not generated at the begining. Originally created for\n",
      "     |  [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
      "     |  >>> from datasets import load_dataset\n",
      "     |  \n",
      "     |  >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
      "     |  >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # Whisper has `begin_suppress_tokens` set by default (= `[220, 50256]`). 50256 is the EOS token, so this means\n",
      "     |  >>> # it can't generate and EOS token in the first iteration, but it can in the others.\n",
      "     |  >>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n",
      "     |  >>> print(outputs.scores[0][0, 50256])\n",
      "     |  tensor(-inf)\n",
      "     |  >>> print(outputs.scores[-1][0, 50256])  # in other places we can see some probability mass for EOS\n",
      "     |  tensor(29.9010)\n",
      "     |  \n",
      "     |  >>> # If we disable `begin_suppress_tokens`, we can generate EOS in the first iteration.\n",
      "     |  >>> outputs = model.generate(\n",
      "     |  ...     **inputs, return_dict_in_generate=True, output_scores=True, begin_suppress_tokens=None\n",
      "     |  ... )\n",
      "     |  >>> print(outputs.scores[0][0, 50256])\n",
      "     |  tensor(11.2027)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SuppressTokensAtBeginLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, begin_suppress_tokens, begin_index, device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_begin_index(self, begin_index)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SuppressTokensLogitsProcessor(LogitsProcessor)\n",
      "     |  SuppressTokensLogitsProcessor(suppress_tokens, device: str = 'cpu')\n",
      "     |  \n",
      "     |  This processor can be used to suppress a list of tokens. The processor will set their log probs to `-inf` so\n",
      "     |  that they are not generated. Originally created for\n",
      "     |  [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoProcessor, WhisperForConditionalGeneration\n",
      "     |  >>> from datasets import load_dataset\n",
      "     |  \n",
      "     |  >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
      "     |  >>> inputs = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # Whisper has a long list of suppressed tokens. For instance, in this case, the token 1 is suppressed by default.\n",
      "     |  >>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True)\n",
      "     |  >>> print(outputs.scores[1][0, 1])  # 1 (and not 0) is the first freely generated token\n",
      "     |  tensor(-inf)\n",
      "     |  \n",
      "     |  >>> # If we disable `suppress_tokens`, we can generate it.\n",
      "     |  >>> outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True, suppress_tokens=None)\n",
      "     |  >>> print(outputs.scores[1][0, 1])\n",
      "     |  tensor(6.0678)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SuppressTokensLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, suppress_tokens, device: str = 'cpu')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TemperatureLogitsWarper(LogitsWarper)\n",
      "     |  TemperatureLogitsWarper(temperature: float)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] for temperature (exponential scaling output probability distribution), which effectively means\n",
      "     |  that it can control the randomness of the predicted tokens. Often used together with [`TopPLogitsWarper`] and\n",
      "     |  [`TopKLogitsWarper`].\n",
      "     |  \n",
      "     |  <Tip>\n",
      "     |  \n",
      "     |  Make sure that `do_sample=True` is included in the `generate` arguments otherwise the temperature value won't have\n",
      "     |  any effect.\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      temperature (`float`):\n",
      "     |          Strictly positive float value used to modulate the logits distribution. A value smaller than `1` decreases\n",
      "     |          randomness (and vice versa), with `0` being equivalent to shifting all probability mass to the most likely\n",
      "     |          token.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> import torch\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(0)  # for reproducibility\n",
      "     |  \n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> model.config.pad_token_id = model.config.eos_token_id\n",
      "     |  >>> inputs = tokenizer([\"Hugging Face Company is\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With temperature=1.0, the default, we consistently get random outputs due to random sampling.\n",
      "     |  >>> generate_kwargs = {\"max_new_tokens\": 10, \"do_sample\": True, \"temperature\": 1.0, \"num_return_sequences\": 2}\n",
      "     |  >>> outputs = model.generate(**inputs, **generate_kwargs)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "     |  ['Hugging Face Company is one of these companies that is going to take a',\n",
      "     |  \"Hugging Face Company is a brand created by Brian A. O'Neil\"]\n",
      "     |  \n",
      "     |  >>> # However, with temperature close to 0, it approximates greedy decoding strategies (invariant)\n",
      "     |  >>> generate_kwargs[\"temperature\"] = 0.0001\n",
      "     |  >>> outputs = model.generate(**inputs, **generate_kwargs)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
      "     |  ['Hugging Face Company is a company that has been around for over 20 years',\n",
      "     |  'Hugging Face Company is a company that has been around for over 20 years']\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TemperatureLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, temperature: float)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TextIteratorStreamer(TextStreamer)\n",
      "     |  TextIteratorStreamer(tokenizer: 'AutoTokenizer', skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs)\n",
      "     |  \n",
      "     |  Streamer that stores print-ready text in a queue, to be used by a downstream application as an iterator. This is\n",
      "     |  useful for applications that benefit from acessing the generated text in a non-blocking way (e.g. in an interactive\n",
      "     |  Gradio demo).\n",
      "     |  \n",
      "     |  <Tip warning={true}>\n",
      "     |  \n",
      "     |  The API for the streamer classes is still under development and may change in the future.\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Parameters:\n",
      "     |      tokenizer (`AutoTokenizer`):\n",
      "     |          The tokenized used to decode the tokens.\n",
      "     |      skip_prompt (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.\n",
      "     |      timeout (`float`, *optional*):\n",
      "     |          The timeout for the text queue. If `None`, the queue will block indefinitely. Useful to handle exceptions\n",
      "     |          in `.generate()`, when it is called in a separate thread.\n",
      "     |      decode_kwargs (`dict`, *optional*):\n",
      "     |          Additional keyword arguments to pass to the tokenizer's `decode` method.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      ```python\n",
      "     |      >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer\n",
      "     |      >>> from threading import Thread\n",
      "     |  \n",
      "     |      >>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      >>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
      "     |      >>> streamer = TextIteratorStreamer(tok)\n",
      "     |  \n",
      "     |      >>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n",
      "     |      >>> generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n",
      "     |      >>> thread = Thread(target=model.generate, kwargs=generation_kwargs)\n",
      "     |      >>> thread.start()\n",
      "     |      >>> generated_text = \"\"\n",
      "     |      >>> for new_text in streamer:\n",
      "     |      ...     generated_text += new_text\n",
      "     |      >>> generated_text\n",
      "     |      'An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,'\n",
      "     |      ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TextIteratorStreamer\n",
      "     |      TextStreamer\n",
      "     |      BaseStreamer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tokenizer: 'AutoTokenizer', skip_prompt: bool = False, timeout: Optional[float] = None, **decode_kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |  \n",
      "     |  on_finalized_text(self, text: str, stream_end: bool = False)\n",
      "     |      Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from TextStreamer:\n",
      "     |  \n",
      "     |  end(self)\n",
      "     |      Flushes any remaining cache and prints a newline to stdout.\n",
      "     |  \n",
      "     |  put(self, value)\n",
      "     |      Receives tokens, decodes them, and prints them to stdout as soon as they form entire words.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseStreamer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TextStreamer(BaseStreamer)\n",
      "     |  TextStreamer(tokenizer: 'AutoTokenizer', skip_prompt: bool = False, **decode_kwargs)\n",
      "     |  \n",
      "     |  Simple text streamer that prints the token(s) to stdout as soon as entire words are formed.\n",
      "     |  \n",
      "     |  <Tip warning={true}>\n",
      "     |  \n",
      "     |  The API for the streamer classes is still under development and may change in the future.\n",
      "     |  \n",
      "     |  </Tip>\n",
      "     |  \n",
      "     |  Parameters:\n",
      "     |      tokenizer (`AutoTokenizer`):\n",
      "     |          The tokenized used to decode the tokens.\n",
      "     |      skip_prompt (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.\n",
      "     |      decode_kwargs (`dict`, *optional*):\n",
      "     |          Additional keyword arguments to pass to the tokenizer's `decode` method.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      ```python\n",
      "     |      >>> from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
      "     |  \n",
      "     |      >>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |      >>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
      "     |      >>> streamer = TextStreamer(tok)\n",
      "     |  \n",
      "     |      >>> # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
      "     |      >>> _ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)\n",
      "     |      An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n",
      "     |      ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TextStreamer\n",
      "     |      BaseStreamer\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, tokenizer: 'AutoTokenizer', skip_prompt: bool = False, **decode_kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  end(self)\n",
      "     |      Flushes any remaining cache and prints a newline to stdout.\n",
      "     |  \n",
      "     |  on_finalized_text(self, text: str, stream_end: bool = False)\n",
      "     |      Prints the new text to stdout. If the stream is ending, also prints a newline.\n",
      "     |  \n",
      "     |  put(self, value)\n",
      "     |      Receives tokens, decodes them, and prints them to stdout as soon as they form entire words.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from BaseStreamer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TopKLogitsWarper(LogitsWarper)\n",
      "     |  TopKLogitsWarper(top_k: int, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements. Often used together\n",
      "     |  with [`TemperatureLogitsWarper`] and [`TopPLogitsWarper`].\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      top_k (`int`):\n",
      "     |          The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All filtered values will be set to this float value.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Minimum number of tokens that cannot be filtered.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: A, B, C, D\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With sampling, the output is unexpected -- sometimes too unexpected.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: A, B, C, D, E — S — O, P — R\n",
      "     |  \n",
      "     |  >>> # With `top_k` sampling, the output gets restricted the k most likely tokens.\n",
      "     |  >>> # Pro tip: In practice, LLMs use `top_k` in the 5-50 range.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, top_k=2)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: A, B, C, D, E, F, G, H, I\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TopKLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, top_k: int, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TopPLogitsWarper(LogitsWarper)\n",
      "     |  TopPLogitsWarper(top_p: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs top-p, i.e. restricting to top tokens summing to prob_cut_off <= prob_cut_off. Often\n",
      "     |  used together with [`TemperatureLogitsWarper`] and [`TopKLogitsWarper`].\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      top_p (`float`):\n",
      "     |          If set to < 1, only the smallest set of most probable tokens with probabilities that add up to `top_p` or\n",
      "     |          higher are kept for generation.\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All filtered values will be set to this float value.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Minimum number of tokens that cannot be filtered.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> set_seed(1)\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # With sampling, the output is unexpected -- sometimes too unexpected.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3 | < 4 (left-hand pointer) ;\n",
      "     |  <BLANKLINE>\n",
      "     |  <BLANKLINE>\n",
      "     |  \n",
      "     |  >>> # With `top_p` sampling, the output gets restricted to high-probability tokens.\n",
      "     |  >>> # Pro tip: In practice, LLMs use `top_p` in the 0.9-0.95 range.\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True, top_p=0.1)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TopPLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, top_p: float, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TypicalLogitsWarper(LogitsWarper)\n",
      "     |  TypicalLogitsWarper(mass: float = 0.9, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |  \n",
      "     |  [`LogitsWarper`] that performs typical decoding. Inspired on how humans use language, it prioritizes tokens whose\n",
      "     |  log probability is close to the entropy of the token probability distribution. This means that the most likely\n",
      "     |  tokens may be discarded in the process.\n",
      "     |  \n",
      "     |  See [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      mass (`float`, *optional*, defaults to 0.9):\n",
      "     |          Value of typical_p between 0 and 1 inclusive, defaults to 0.9.\n",
      "     |      filter_value (`float`, *optional*, defaults to -inf):\n",
      "     |          All filtered values will be set to this float value.\n",
      "     |      min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
      "     |          Minimum number of tokens that cannot be filtered.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"bigscience/bloomz-560m\")\n",
      "     |  \n",
      "     |  >>> inputs = tokenizer(\"1, 2, 3\", return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # We can see that greedy decoding produces a sequence of numbers\n",
      "     |  >>> outputs = model.generate(**inputs)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
      "     |  \n",
      "     |  >>> # For this particular seed, we can see that sampling produces nearly the same low-information (= low entropy)\n",
      "     |  >>> # sequence\n",
      "     |  >>> set_seed(18)\n",
      "     |  >>> outputs = model.generate(**inputs, do_sample=True)\n",
      "     |  >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
      "     |  1, 2, 3, 4, 5, 6, 7, 8, 9 and 10\n",
      "     |  \n",
      "     |  >>> # With `typical_p` set, the most obvious sequence is no longer produced, which may be good for your problem\n",
      "     |  >>> set_seed(18)\n",
      "     |  >>> outputs = model.generate(\n",
      "     |  ...     **inputs, do_sample=True, typical_p=0.1, return_dict_in_generate=True, output_scores=True\n",
      "     |  ... )\n",
      "     |  >>> print(tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)[0])\n",
      "     |  1, 2, 3 and 5\n",
      "     |  \n",
      "     |  >>> # We can see that the token corresponding to \"4\" (token 934) in the second position, the most likely token\n",
      "     |  >>> # as seen with greedy decoding, was entirely blocked out\n",
      "     |  >>> print(outputs.scores[1][0, 934])\n",
      "     |  tensor(-inf)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TypicalLogitsWarper\n",
      "     |      LogitsWarper\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, mass: float = 0.9, filter_value: float = -inf, min_tokens_to_keep: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsWarper:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class UnbatchedClassifierFreeGuidanceLogitsProcessor(LogitsProcessor)\n",
      "     |  UnbatchedClassifierFreeGuidanceLogitsProcessor(guidance_scale: float, model, unconditional_ids: Optional[torch.LongTensor] = None, unconditional_attention_mask: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = True)\n",
      "     |  \n",
      "     |  Logits processor for Classifier-Free Guidance (CFG). The processors computes a weighted average across scores\n",
      "     |  from prompt conditional and prompt unconditional (or negative) logits, parameterized by the `guidance_scale`.\n",
      "     |  The unconditional scores are computed internally by prompting `model` with the `unconditional_ids` branch.\n",
      "     |  \n",
      "     |  See [the paper](https://arxiv.org/abs/2306.17806) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      guidance_scale (`float`):\n",
      "     |          The guidance scale for classifier free guidance (CFG). CFG is enabled by setting `guidance_scale != 1`.\n",
      "     |          Higher guidance scale encourages the model to generate samples that are more closely linked to the input\n",
      "     |          prompt, usually at the expense of poorer quality. A value smaller than 1 has the opposite effect, while\n",
      "     |          making the negative prompt provided with negative_prompt_ids (if any) act as a positive prompt.\n",
      "     |      model (`PreTrainedModel`):\n",
      "     |          The model computing the unconditional scores. Supposedly the same as the one computing the conditional\n",
      "     |          scores. Both models must use the same tokenizer.\n",
      "     |      unconditional_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "     |          Indices of input sequence tokens in the vocabulary for the unconditional branch. If unset, will default to\n",
      "     |          the last token of the prompt.\n",
      "     |      unconditional_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
      "     |          Attention mask for unconditional_ids.\n",
      "     |      use_cache (`bool`, *optional*, defaults to `True`):\n",
      "     |          Whether to cache key/values during the negative prompt forward pass.\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> inputs = tokenizer([\"Today, a dragon flew over Paris, France,\"], return_tensors=\"pt\")\n",
      "     |  >>> out = model.generate(inputs[\"input_ids\"], guidance_scale=1.5)\n",
      "     |  >>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
      "     |  'Today, a dragon flew over Paris, France, killing at least 50 people and injuring more than 100'\n",
      "     |  \n",
      "     |  >>> # with a negative prompt\n",
      "     |  >>> neg_inputs = tokenizer([\"A very happy event happened,\"], return_tensors=\"pt\")\n",
      "     |  >>> out = model.generate(inputs[\"input_ids\"], guidance_scale=2, negative_prompt_ids=neg_inputs[\"input_ids\"])\n",
      "     |  >>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
      "     |  'Today, a dragon flew over Paris, France, killing at least 130 people. French media reported that'\n",
      "     |  \n",
      "     |  >>> # with a positive prompt\n",
      "     |  >>> neg_inputs = tokenizer([\"A very happy event happened,\"], return_tensors=\"pt\")\n",
      "     |  >>> out = model.generate(inputs[\"input_ids\"], guidance_scale=0, negative_prompt_ids=neg_inputs[\"input_ids\"])\n",
      "     |  >>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
      "     |  \"Today, a dragon flew over Paris, France, and I'm very happy to be here. I\"\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UnbatchedClassifierFreeGuidanceLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids, scores)\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, guidance_scale: float, model, unconditional_ids: Optional[torch.LongTensor] = None, unconditional_attention_mask: Optional[torch.LongTensor] = None, use_cache: Optional[bool] = True)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_unconditional_logits(self, input_ids)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WatermarkDetector(builtins.object)\n",
      "     |  WatermarkDetector(model_config: transformers.configuration_utils.PretrainedConfig, device: str, watermarking_config: Union[transformers.generation.configuration_utils.WatermarkingConfig, Dict], ignore_repeated_ngrams: bool = False, max_cache_size: int = 128)\n",
      "     |  \n",
      "     |  Detector for detection of watermark generated text. The detector needs to be given the exact same settings that were\n",
      "     |  given during text generation to replicate the watermark greenlist generation and so detect the watermark. This includes\n",
      "     |  the correct device that was used during text generation, the correct watermarking arguments and the correct tokenizer vocab size.\n",
      "     |  The code was based on the [original repo](https://github.com/jwkirchenbauer/lm-watermarking/tree/main).\n",
      "     |  \n",
      "     |  See [the paper](https://arxiv.org/abs/2306.04634) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      model_config (`PretrainedConfig`):\n",
      "     |          The model config that will be used to get model specific arguments used when generating.\n",
      "     |      device (`str`):\n",
      "     |          The device which was used during watermarked text generation.\n",
      "     |      watermarking_config (Union[`WatermarkingConfig`, `Dict`]):\n",
      "     |          The exact same watermarking config and arguments used when generating text.\n",
      "     |      ignore_repeated_ngrams (`bool`, *optional*, defaults to `False`):\n",
      "     |          Whether to count every unique ngram only once or not.\n",
      "     |      max_cache_size (`int`, *optional*, defaults to 128):\n",
      "     |          The max size to be used for LRU caching of seeding/sampling algorithms called for every token.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig\n",
      "     |  \n",
      "     |  >>> model_id = \"openai-community/gpt2\"\n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(model_id)\n",
      "     |  >>> tok = AutoTokenizer.from_pretrained(model_id)\n",
      "     |  >>> tok.pad_token_id = tok.eos_token_id\n",
      "     |  >>> tok.padding_side = \"left\"\n",
      "     |  \n",
      "     |  >>> inputs = tok([\"This is the beginning of a long story\", \"Alice and Bob are\"], padding=True, return_tensors=\"pt\")\n",
      "     |  >>> input_len = inputs[\"input_ids\"].shape[-1]\n",
      "     |  \n",
      "     |  >>> # first generate text with watermark and without\n",
      "     |  >>> watermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")\n",
      "     |  >>> out_watermarked = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n",
      "     |  >>> out = model.generate(**inputs, do_sample=False, max_length=20)\n",
      "     |  \n",
      "     |  >>> # now we can instantiate the detector and check the generated text\n",
      "     |  >>> detector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config=watermarking_config)\n",
      "     |  >>> detection_out_watermarked = detector(out_watermarked, return_dict=True)\n",
      "     |  >>> detection_out = detector(out, return_dict=True)\n",
      "     |  >>> detection_out_watermarked.prediction\n",
      "     |  array([ True,  True])\n",
      "     |  \n",
      "     |  >>> detection_out.prediction\n",
      "     |  array([False,  False])\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, z_threshold: float = 3.0, return_dict: bool = False) -> Union[transformers.generation.watermarking.WatermarkDetectorOutput, <built-in function array>]\n",
      "     |              Args:\n",
      "     |              input_ids (`torch.LongTensor`):\n",
      "     |                  The watermark generated text. It is advised to remove the prompt, which can affect the detection.\n",
      "     |              z_threshold (`Dict`, *optional*, defaults to `3.0`):\n",
      "     |                  Changing this threshold will change the sensitivity of the detector. Higher z threshold gives less\n",
      "     |                  sensitivity and vice versa for lower z threshold.\n",
      "     |              return_dict (`bool`,  *optional*, defaults to `False`):\n",
      "     |                  Whether to return `~generation.WatermarkDetectorOutput` or not. If not it will return boolean predictions,\n",
      "     |      ma\n",
      "     |              Return:\n",
      "     |                  [`~generation.WatermarkDetectorOutput`] or `np.array`: A [`~generation.WatermarkDetectorOutput`]\n",
      "     |                  if `return_dict=True` otherwise a `np.array`.\n",
      "     |  \n",
      "     |  __init__(self, model_config: transformers.configuration_utils.PretrainedConfig, device: str, watermarking_config: Union[transformers.generation.configuration_utils.WatermarkingConfig, Dict], ignore_repeated_ngrams: bool = False, max_cache_size: int = 128)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WatermarkDetectorOutput(builtins.object)\n",
      "     |  WatermarkDetectorOutput(num_tokens_scored: <built-in function array> = None, num_green_tokens: <built-in function array> = None, green_fraction: <built-in function array> = None, z_score: <built-in function array> = None, p_value: <built-in function array> = None, prediction: Optional[<built-in function array>] = None, confidence: Optional[<built-in function array>] = None) -> None\n",
      "     |  \n",
      "     |  Outputs of a watermark detector.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      num_tokens_scored (np.array of shape (batch_size)):\n",
      "     |          Array containing the number of tokens scored for each element in the batch.\n",
      "     |      num_green_tokens (np.array of shape (batch_size)):\n",
      "     |          Array containing the number of green tokens for each element in the batch.\n",
      "     |      green_fraction (np.array of shape (batch_size)):\n",
      "     |          Array containing the fraction of green tokens for each element in the batch.\n",
      "     |      z_score (np.array of shape (batch_size)):\n",
      "     |          Array containing the z-score for each element in the batch. Z-score here shows\n",
      "     |          how many standard deviations away is the green token count in the input text\n",
      "     |          from the expected green token count for machine-generated text.\n",
      "     |      p_value (np.array of shape (batch_size)):\n",
      "     |          Array containing the p-value for each batch obtained from z-scores.\n",
      "     |      prediction (np.array of shape (batch_size)), *optional*:\n",
      "     |          Array containing boolean predictions whether a text is machine-generated for each element in the batch.\n",
      "     |      confidence (np.array of shape (batch_size)), *optional*:\n",
      "     |          Array containing confidence scores of a text being machine-generated for each element in the batch.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, num_tokens_scored: <built-in function array> = None, num_green_tokens: <built-in function array> = None, green_fraction: <built-in function array> = None, z_score: <built-in function array> = None, p_value: <built-in function array> = None, prediction: Optional[<built-in function array>] = None, confidence: Optional[<built-in function array>] = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'confidence': typing.Optional[<built-in function ar...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'confidence': Field(name='confidence',type=typ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('num_tokens_scored', 'num_green_tokens', 'green_frac...\n",
      "     |  \n",
      "     |  confidence = None\n",
      "     |  \n",
      "     |  green_fraction = None\n",
      "     |  \n",
      "     |  num_green_tokens = None\n",
      "     |  \n",
      "     |  num_tokens_scored = None\n",
      "     |  \n",
      "     |  p_value = None\n",
      "     |  \n",
      "     |  prediction = None\n",
      "     |  \n",
      "     |  z_score = None\n",
      "    \n",
      "    class WatermarkLogitsProcessor(LogitsProcessor)\n",
      "     |  WatermarkLogitsProcessor(vocab_size, device, greenlist_ratio: float = 0.25, bias: float = 2.0, hashing_key: int = 15485863, seeding_scheme: str = 'lefthash', context_width: int = 1)\n",
      "     |  \n",
      "     |  Logits processor for watermarking generated text. The processor modifies model output scores by adding a small bias to\n",
      "     |  randomized set of \"green\" tokens before generating the next token. \"Green\" tokens selection process depends on the\n",
      "     |  `seeding_scheme` used. The code was based on the [original repo](https://github.com/jwkirchenbauer/lm-watermarking/tree/main).\n",
      "     |  \n",
      "     |  The text generated by this `LogitsProcessor` can be detected using `WatermarkDetector`. See [`~WatermarkDetector.__call__`] for details,\n",
      "     |  \n",
      "     |  See [the paper](https://arxiv.org/abs/2306.04634) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      vocab_size (`int`):\n",
      "     |          The model tokenizer's vocab_size. Used to calculate \"green\" tokens ratio.\n",
      "     |      device (`str`):\n",
      "     |          The device where model is allocated.\n",
      "     |      greenlist_ratio (`float`, optional, *optional*, defaults to 0.25):\n",
      "     |          The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n",
      "     |      bias (`float`, optional, *optional*, defaults to 2.0):\n",
      "     |          The bias added to the selected \"green\" tokens' logits. Consider lowering the\n",
      "     |          `bias` if the text generation quality degrades. Recommended values are in the\n",
      "     |          range of [0.5, 2.0]. Defaults to 2.0.\n",
      "     |      hashing_key (`int`, optional, *optional*, defaults to 15485863):\n",
      "     |          Key used for hashing. If you deploy this watermark, we advise using another private key.\n",
      "     |          Defaults to 15485863 (the millionth prime).\n",
      "     |      seeding_scheme (`str`, optional, *optional*, defaults to `\"lefthash\"`):\n",
      "     |          The seeding scheme used for selecting \"green\" tokens. Accepts values:\n",
      "     |              - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from paper)\n",
      "     |              - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from paper)\n",
      "     |                  The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n",
      "     |          The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n",
      "     |      context_width (`int`, *optional*, defaults to 1):\n",
      "     |          The number of previous tokens to use when setting the seed.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  >>> from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkingConfig\n",
      "     |  \n",
      "     |  >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
      "     |  >>> inputs = tokenizer([\"Alice and Bob are\"], return_tensors=\"pt\")\n",
      "     |  \n",
      "     |  >>> # normal generation\n",
      "     |  >>> out = model.generate(inputs[\"input_ids\"], max_length=20, do_sample=False)\n",
      "     |  >>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
      "     |  'Alice and Bob are both in the same room.\\n\\n\"I\\'m not sure if you\\'re'\n",
      "     |  \n",
      "     |  >>> # watermarked generation\n",
      "     |  >>> watermarking_config = WatermarkingConfig(bias=2.5, context_width=2, seeding_scheme=\"selfhash\")\n",
      "     |  >>> out = model.generate(inputs[\"input_ids\"], watermarking_config=watermarking_config, max_length=20, do_sample=False)\n",
      "     |  >>> tokenizer.batch_decode(out, skip_special_tokens=True)[0]\n",
      "     |  'Alice and Bob are both still alive and well and the story is pretty much a one-hour adventure'\n",
      "     |  \n",
      "     |  >>> # to detect watermarked text use the WatermarkDetector class\n",
      "     |  >>> from transformers import WatermarkDetector\n",
      "     |  >>> detector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config= watermarking_config)\n",
      "     |  >>> detection_preds = detector(out)\n",
      "     |  >>> detection_preds\n",
      "     |  array([ True])\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WatermarkLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, device, greenlist_ratio: float = 0.25, bias: float = 2.0, hashing_key: int = 15485863, seeding_scheme: str = 'lefthash', context_width: int = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_seed(self, input_seq: torch.LongTensor)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class WatermarkingConfig(builtins.object)\n",
      "     |  WatermarkingConfig(greenlist_ratio: Optional[float] = 0.25, bias: Optional[float] = 2.0, hashing_key: Optional[int] = 15485863, seeding_scheme: Optional[str] = 'lefthash', context_width: Optional[int] = 1)\n",
      "     |  \n",
      "     |  Class that holds arguments for watermark generation and should be passed into `GenerationConfig` during `generate`.\n",
      "     |  See [this paper](https://arxiv.org/abs/2306.04634) for more details on the arguments.\n",
      "     |  \n",
      "     |  Accepts the following keys:\n",
      "     |      - greenlist_ratio (`float`):\n",
      "     |          Used for watermarking. The ratio of \"green\" tokens used to the vocabulary size. Defaults to 0.25.\n",
      "     |      - bias (`float`):\n",
      "     |          Used with watermarking. The bias added to the selected \"green\" tokens' logits. Defaults to 2.0.\n",
      "     |      - hashing_key (`int`):\n",
      "     |          Hashing key used for watermarking. Defaults to 15485863 (the millionth prime).\n",
      "     |      - seeding_scheme (`str`):\n",
      "     |          Algorithm to use for watermarking. Accepts values:\n",
      "     |              - \"lefthash\" (default): \"green\" tokens selection depend on the last token (Algorithm 2 from the paper)\n",
      "     |              - \"selfhash\": \"green\" tokens selection depends on the current token itself (Algorithm 3 from the paper)\n",
      "     |                  The downside of this scheme is that it considers all possible next tokens and can be slower than \"lefthash\".\n",
      "     |      - context_width(`int`):\n",
      "     |          The context length of previous tokens to use in seeding. Higher context length makes watermarking more robust.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, greenlist_ratio: Optional[float] = 0.25, bias: Optional[float] = 2.0, hashing_key: Optional[int] = 15485863, seeding_scheme: Optional[str] = 'lefthash', context_width: Optional[int] = 1)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  to_dict(self) -> Dict[str, Any]\n",
      "     |      Serializes this instance to a Python dictionary.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Dict[str, Any]: Dictionary of all the attributes that make up this configuration instance.\n",
      "     |  \n",
      "     |  to_json_file(self, json_file_path: Union[str, os.PathLike])\n",
      "     |      Save this instance to a JSON file.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          json_file_path (Union[str, os.PathLike]): Path to the JSON file in which this configuration instance's parameters will be saved.\n",
      "     |  \n",
      "     |  to_json_string(self)\n",
      "     |      Serializes this instance to a JSON formatted string.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          str: JSON formatted string representing the configuration instance.\n",
      "     |  \n",
      "     |  update(self, **kwargs)\n",
      "     |      Update the configuration attributes with new values.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          **kwargs: Keyword arguments representing configuration attributes and their new values.\n",
      "     |  \n",
      "     |  validate(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_dict(config_dict, **kwargs) from builtins.type\n",
      "     |      Constructs a WatermarkingConfig instance from a dictionary of parameters.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config_dict (Dict[str, Any]): Dictionary containing configuration parameters.\n",
      "     |          **kwargs: Additional keyword arguments to override dictionary values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          WatermarkingConfig: Instance of WatermarkingConfig constructed from the dictionary.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {}\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ()\n",
      "    \n",
      "    class WhisperTimeStampLogitsProcessor(LogitsProcessor)\n",
      "     |  WhisperTimeStampLogitsProcessor(generate_config, begin_index: Optional[int] = None, _detect_timestamp_from_logprob: Optional[bool] = None)\n",
      "     |  \n",
      "     |  [`LogitsProcessor`] that modifies the logits for the generation of timestamps in the transcription. When the input\n",
      "     |  tokens are at a specific threshold, the processor sets the scores to negative infinity. The processor makes sure\n",
      "     |  that timestamp tokens appear in pairs, by masking out the logits that would break this pairing pattern. This is\n",
      "     |  done to maintain the consistency and structure of generated timestamps. It also ensures that when the predicted\n",
      "     |  probability of sampling any of the timestamp token is greater than any individual non-timestamp token, those\n",
      "     |  non-timestamp logits are set to negative infinity. This is done to ensure the generation of timestamps over other\n",
      "     |  potential tokens.\n",
      "     |  \n",
      "     |  \n",
      "     |  See [the paper](https://arxiv.org/abs/2212.04356) for more information.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      generate_config (`GenerateConfig`):\n",
      "     |          The generate config used to generate the output. The following parameters are required:\n",
      "     |              eos_token_id (`int`, *optional*, defaults to 50257):\n",
      "     |                  The id of the *end-of-sequence* token.\n",
      "     |              no_timestamps_token_id (`int`, *optional*, defaults to 50363):\n",
      "     |                  The id of the `\"<|notimestamps|>\"` token.\n",
      "     |              max_initial_timestamp_index (`int`, *optional*, defaults to 1):\n",
      "     |                  Used to set the maximum value of the initial timestamp. This is used to prevent the model from\n",
      "     |                  predicting timestamps that are too far in the future.\n",
      "     |      begin_index (`Optional`, *optional*): Token index of the first token that is generated by the model.\n",
      "     |      _detect_timestamp_from_logprob (`bool`, *optional*): Whether timestamps can be predicted from logprobs over all timestamps.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  ``` python\n",
      "     |  >>> import torch\n",
      "     |  >>> from transformers import AutoProcessor, WhisperForConditionalGeneration, GenerationConfig\n",
      "     |  >>> from datasets import load_dataset\n",
      "     |  \n",
      "     |  >>> processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n",
      "     |  >>> ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\", trust_remote_code=True)\n",
      "     |  >>> inputs = processor(ds[3][\"audio\"][\"array\"], return_tensors=\"pt\")\n",
      "     |  >>> input_features = inputs.input_features\n",
      "     |  \n",
      "     |  >>> #Displaying timestamps\n",
      "     |  >>> generated_ids = model.generate(inputs=input_features, return_timestamps=True)\n",
      "     |  >>> transcription = processor.batch_decode(generated_ids, decode_with_timestamps=True)[0]\n",
      "     |  >>> print(\"Transcription:\", transcription)\n",
      "     |  Transcription: <|startoftranscript|><|0.00|> He has grave doubts whether Sir Frederick Layton's work is really Greek after all, and can<|6.44|><|6.44|> discover in it but little of rocky Ithaca.<|9.44|><|endoftext|>\n",
      "     |  \n",
      "     |  \n",
      "     |  >>> #No timestamps & change EOS:\n",
      "     |  >>> #This allows the user to select a specific token to terminate the sequence on, in this case it's the word \"can\"(460)\n",
      "     |  >>> model.generation_config.eos_token_id = 460\n",
      "     |  >>> generated_ids = model.generate(inputs=input_features,return_timestamps=False)\n",
      "     |  >>> transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
      "     |  >>> print(\"Transcription:\", transcription)\n",
      "     |  Transcription:  He has grave doubts whether Sir Frederick Layton's work is really Greek after all and can\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      WhisperTimeStampLogitsProcessor\n",
      "     |      LogitsProcessor\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor\n",
      "     |      Args:\n",
      "     |          input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n",
      "     |              Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n",
      "     |          scores (`torch.FloatTensor` of shape `(batch_size, config.vocab_size)`):\n",
      "     |              Prediction scores of a language modeling head. These can be logits for each vocabulary when not using beam\n",
      "     |              search or log softmax for each vocabulary token when using beam search\n",
      "     |      \n",
      "     |      Return:\n",
      "     |          `torch.FloatTensor` of shape `(batch_size, config.vocab_size)`: The processed prediction scores.\n",
      "     |  \n",
      "     |  __init__(self, generate_config, begin_index: Optional[int] = None, _detect_timestamp_from_logprob: Optional[bool] = None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  set_begin_index(self, begin_index)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from LogitsProcessor:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    validate_stopping_criteria(stopping_criteria: transformers.generation.stopping_criteria.StoppingCriteriaList, max_length: int) -> transformers.generation.stopping_criteria.StoppingCriteriaList\n",
      "\n",
      "DATA\n",
      "    __all__ = ['configuration_utils', 'streamers', 'beam_constraints', 'be...\n",
      "\n",
      "FILE\n",
      "    /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/generation/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This script allows display the documentation of subpackage as shown above.\n",
    "\n",
    "import transformers\n",
    "import importlib\n",
    "\n",
    "def display_subpackage(subpackage_name):\n",
    "    module_name = f'transformers.{subpackage_name}'\n",
    "    if importlib.util.find_spec(module_name) is not None:\n",
    "        # Dynamically import the subpackage\n",
    "        subpackage = importlib.import_module(module_name)\n",
    "        # Print the documentation\n",
    "        help(subpackage)\n",
    "    else:\n",
    "        print(f\"Error: The subpackage '{subpackage_name}' does not exist in the transformers library.\")\n",
    "\n",
    "display_subpackage(\"generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package transformers.quantizers in transformers:\n",
      "\n",
      "NAME\n",
      "    transformers.quantizers\n",
      "\n",
      "DESCRIPTION\n",
      "    # Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n",
      "    #\n",
      "    # Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "    # you may not use this file except in compliance with the License.\n",
      "    # You may obtain a copy of the License at\n",
      "    #\n",
      "    #     http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    auto\n",
      "    base\n",
      "    quantizer_aqlm\n",
      "    quantizer_awq\n",
      "    quantizer_bnb_4bit\n",
      "    quantizer_bnb_8bit\n",
      "    quantizer_eetq\n",
      "    quantizer_gptq\n",
      "    quantizer_hqq\n",
      "    quantizer_quanto\n",
      "    quantizers_utils\n",
      "\n",
      "FILE\n",
      "    /home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/quantizers/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#This script allows to import any attribute and show its documentation if the above two don't work.\n",
    "\n",
    "from transformers import quantizers\n",
    "help(quantizers) \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
